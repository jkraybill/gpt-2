{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GPT FB-MSG",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkraybill/gpt-2/blob/finetuning/GPT2-finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHiwt3Ww7FF6",
        "colab_type": "text"
      },
      "source": [
        "To try out GPT-2, do this:\n",
        "\n",
        "- go to the \"Runtime\" menu and click \"Change runtime type\" and make sure this is a Python 3 notebook, running with GPU hardware acceleration.\n",
        "- use the \"Files\" section to the left to upload a text file called \"corpus.txt\" which contains all the text you want to train on.\n",
        "- run the steps below in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE_fFgQ8a_Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etM-i8RwbcTH",
        "colab_type": "code",
        "outputId": "3cb2d095-3030-4ce7-bc3d-3f40e8e01379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "!git clone https://github.com/jkraybill/gpt-2.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 160 (delta 0), reused 0 (delta 0), pack-reused 157\u001b[K\n",
            "Receiving objects: 100% (160/160), 4.35 MiB | 1.81 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVTyGrhsbdep",
        "colab_type": "code",
        "outputId": "66b2e791-7fc5-44b4-bb88-a9ead1855891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azu6KCOHbhIy",
        "colab_type": "code",
        "outputId": "46e0f58c-d834-407a-fa4f-26b6b0e6ed53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 28.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.0.18 has requirement regex==2018.01.10, but you'll have regex 2017.4.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex\n",
            "  Found existing installation: regex 2018.1.10\n",
            "    Uninstalling regex-2018.1.10:\n",
            "      Successfully uninstalled regex-2018.1.10\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecmuIWxFgFBz",
        "colab_type": "code",
        "outputId": "8b8b54a2-b53c-4052-a455-5ee748642031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "source": [
        "!sh download_model.sh 117M"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching 117M/checkpoint\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    77  100    77    0     0   3850      0 --:--:-- --:--:-- --:--:--  3850\n",
            "Fetching 117M/encoder.json\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1017k  100 1017k    0     0  33.1M      0 --:--:-- --:--:-- --:--:-- 33.1M\n",
            "Fetching 117M/hparams.json\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    90  100    90    0     0   5625      0 --:--:-- --:--:-- --:--:--  5625\n",
            "Fetching 117M/model.ckpt.data-00000-of-00001\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  474M  100  474M    0     0  79.3M      0  0:00:05  0:00:05 --:--:-- 98.1M\n",
            "Fetching 117M/model.ckpt.index\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5215  100  5215    0     0   318k      0 --:--:-- --:--:-- --:--:--  318k\n",
            "Fetching 117M/model.ckpt.meta\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  460k  100  460k    0     0  22.4M      0 --:--:-- --:--:-- --:--:-- 23.6M\n",
            "Fetching 117M/vocab.bpe\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  445k  100  445k    0     0  21.7M      0 --:--:-- --:--:-- --:--:-- 21.7M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OhJ6ka_DD94",
        "colab_type": "text"
      },
      "source": [
        "The below step encodes your corpus into \"NPZ\" tokenized format for GPT-2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHsNeMNenxR",
        "colab_type": "code",
        "outputId": "5f4bf031-37d2-41d9-bf0c-3269cb3f4348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!PYTHONPATH=src ./encode.py --in-text ../corpus.txt --out-npz corpus.txt.npz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "100% 1/1 [00:00<00:00,  1.59it/s]\n",
            "Writing corpus.txt.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94oLzOq23474",
        "colab_type": "text"
      },
      "source": [
        "Training is below. I usually get usable results with \"stop_after\" anywhere from 800 to 3000, but you can try going even higher. 800 steps takes only a few minutes.\n",
        "\n",
        "\"sample_every\" controls how often you get sample output from the trained model.\n",
        "\n",
        "\"save_every\" controls how often the model is saved.\n",
        "\n",
        "\"learning_rate\" is the AI learning rate. 0.00005 is the rate I've gotten the best results with, but I think most people are running with significantly higher rates, so you could try adjusting it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbBJ6JoufBGQ",
        "colab_type": "code",
        "outputId": "b30f7408-5457-48b8-9bdf-107bd07009b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21554
        }
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset corpus.txt.npz --sample_every=200 --save_every=250 --learning_rate=0.00005 --stop_after=800\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-04 08:10:31.678603: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-04 08:10:31.679072: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x207a520 executing computations on platform Host. Devices:\n",
            "2019-05-04 08:10:31.679114: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-04 08:10:31.959328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-04 08:10:31.959938: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2079e40 executing computations on platform CUDA. Devices:\n",
            "2019-05-04 08:10:31.959970: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-04 08:10:31.960381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-04 08:10:31.960408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-04 08:10:33.367775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-04 08:10:33.367866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-04 08:10:33.367890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-04 08:10:33.368296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "Reading corpus.txt.npz\n",
            "dataset has 87359 tokens\n",
            "Training...\n",
            "2019-05-04 08:11:00.067442: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "[1 | 6.67] loss=2.37 avg=2.37\n",
            "[2 | 7.12] loss=1.77 avg=2.07\n",
            "[3 | 7.59] loss=2.00 avg=2.05\n",
            "[4 | 8.05] loss=1.89 avg=2.01\n",
            "[5 | 8.51] loss=1.84 avg=1.97\n",
            "[6 | 8.96] loss=1.80 avg=1.94\n",
            "[7 | 9.43] loss=1.64 avg=1.90\n",
            "[8 | 9.89] loss=1.78 avg=1.88\n",
            "[9 | 10.35] loss=1.70 avg=1.86\n",
            "[10 | 10.81] loss=1.58 avg=1.83\n",
            "[11 | 11.27] loss=1.70 avg=1.82\n",
            "[12 | 11.73] loss=1.70 avg=1.81\n",
            "[13 | 12.19] loss=1.73 avg=1.80\n",
            "[14 | 12.65] loss=1.76 avg=1.80\n",
            "[15 | 13.11] loss=1.87 avg=1.81\n",
            "[16 | 13.58] loss=1.58 avg=1.79\n",
            "[17 | 14.04] loss=1.63 avg=1.78\n",
            "[18 | 14.50] loss=1.70 avg=1.78\n",
            "[19 | 14.96] loss=1.60 avg=1.77\n",
            "[20 | 15.43] loss=1.56 avg=1.75\n",
            "[21 | 15.89] loss=1.67 avg=1.75\n",
            "[22 | 16.35] loss=1.99 avg=1.76\n",
            "[23 | 16.82] loss=1.70 avg=1.76\n",
            "[24 | 17.29] loss=1.72 avg=1.76\n",
            "[25 | 17.75] loss=1.94 avg=1.77\n",
            "[26 | 18.22] loss=1.64 avg=1.76\n",
            "[27 | 18.68] loss=1.66 avg=1.76\n",
            "[28 | 19.15] loss=1.69 avg=1.75\n",
            "[29 | 19.61] loss=1.65 avg=1.75\n",
            "[30 | 20.07] loss=1.56 avg=1.74\n",
            "[31 | 20.54] loss=1.68 avg=1.74\n",
            "[32 | 21.01] loss=1.77 avg=1.74\n",
            "[33 | 21.47] loss=1.59 avg=1.74\n",
            "[34 | 21.94] loss=1.44 avg=1.72\n",
            "[35 | 22.40] loss=1.68 avg=1.72\n",
            "[36 | 22.87] loss=1.63 avg=1.72\n",
            "[37 | 23.33] loss=1.38 avg=1.71\n",
            "[38 | 23.80] loss=1.58 avg=1.71\n",
            "[39 | 24.27] loss=1.53 avg=1.70\n",
            "[40 | 24.75] loss=1.51 avg=1.69\n",
            "[41 | 25.22] loss=1.57 avg=1.69\n",
            "[42 | 25.69] loss=1.69 avg=1.69\n",
            "[43 | 26.16] loss=1.75 avg=1.69\n",
            "[44 | 26.63] loss=1.83 avg=1.70\n",
            "[45 | 27.10] loss=1.51 avg=1.69\n",
            "[46 | 27.57] loss=1.66 avg=1.69\n",
            "[47 | 28.04] loss=1.68 avg=1.69\n",
            "[48 | 28.51] loss=1.52 avg=1.69\n",
            "[49 | 28.99] loss=1.64 avg=1.68\n",
            "[50 | 29.46] loss=1.80 avg=1.69\n",
            "[51 | 29.93] loss=1.33 avg=1.68\n",
            "[52 | 30.40] loss=1.50 avg=1.67\n",
            "[53 | 30.87] loss=1.66 avg=1.67\n",
            "[54 | 31.34] loss=1.57 avg=1.67\n",
            "[55 | 31.81] loss=1.67 avg=1.67\n",
            "[56 | 32.28] loss=1.60 avg=1.67\n",
            "[57 | 32.75] loss=1.36 avg=1.66\n",
            "[58 | 33.22] loss=1.59 avg=1.66\n",
            "[59 | 33.69] loss=1.54 avg=1.66\n",
            "[60 | 34.16] loss=1.50 avg=1.65\n",
            "[61 | 34.63] loss=1.33 avg=1.65\n",
            "[62 | 35.11] loss=1.49 avg=1.64\n",
            "[63 | 35.58] loss=1.64 avg=1.64\n",
            "[64 | 36.05] loss=1.57 avg=1.64\n",
            "[65 | 36.53] loss=1.44 avg=1.64\n",
            "[66 | 37.00] loss=1.53 avg=1.64\n",
            "[67 | 37.48] loss=1.20 avg=1.63\n",
            "[68 | 37.95] loss=1.52 avg=1.62\n",
            "[69 | 38.43] loss=1.53 avg=1.62\n",
            "[70 | 38.90] loss=1.57 avg=1.62\n",
            "[71 | 39.37] loss=1.67 avg=1.62\n",
            "[72 | 39.84] loss=1.45 avg=1.62\n",
            "[73 | 40.32] loss=1.59 avg=1.62\n",
            "[74 | 40.79] loss=1.63 avg=1.62\n",
            "[75 | 41.27] loss=1.42 avg=1.62\n",
            "[76 | 41.74] loss=1.63 avg=1.62\n",
            "[77 | 42.22] loss=1.69 avg=1.62\n",
            "[78 | 42.70] loss=1.64 avg=1.62\n",
            "[79 | 43.18] loss=1.42 avg=1.61\n",
            "[80 | 43.65] loss=1.55 avg=1.61\n",
            "[81 | 44.13] loss=1.35 avg=1.61\n",
            "[82 | 44.60] loss=1.59 avg=1.61\n",
            "[83 | 45.08] loss=1.55 avg=1.61\n",
            "[84 | 45.56] loss=1.55 avg=1.61\n",
            "[85 | 46.04] loss=1.45 avg=1.60\n",
            "[86 | 46.51] loss=1.31 avg=1.60\n",
            "[87 | 46.99] loss=1.51 avg=1.60\n",
            "[88 | 47.47] loss=1.46 avg=1.59\n",
            "[89 | 47.95] loss=1.53 avg=1.59\n",
            "[90 | 48.44] loss=1.56 avg=1.59\n",
            "[91 | 48.91] loss=1.46 avg=1.59\n",
            "[92 | 49.39] loss=1.49 avg=1.59\n",
            "[93 | 49.88] loss=1.56 avg=1.59\n",
            "[94 | 50.36] loss=1.36 avg=1.58\n",
            "[95 | 50.84] loss=1.68 avg=1.59\n",
            "[96 | 51.32] loss=1.42 avg=1.58\n",
            "[97 | 51.80] loss=1.39 avg=1.58\n",
            "[98 | 52.28] loss=1.54 avg=1.58\n",
            "[99 | 52.76] loss=1.26 avg=1.57\n",
            "[100 | 53.24] loss=1.32 avg=1.57\n",
            "[101 | 53.73] loss=1.59 avg=1.57\n",
            "[102 | 54.21] loss=1.37 avg=1.57\n",
            "[103 | 54.70] loss=1.33 avg=1.56\n",
            "[104 | 55.19] loss=1.47 avg=1.56\n",
            "[105 | 55.67] loss=1.27 avg=1.56\n",
            "[106 | 56.15] loss=1.28 avg=1.55\n",
            "[107 | 56.64] loss=1.50 avg=1.55\n",
            "[108 | 57.13] loss=1.42 avg=1.55\n",
            "[109 | 57.61] loss=1.23 avg=1.55\n",
            "[110 | 58.10] loss=1.40 avg=1.54\n",
            "[111 | 58.59] loss=1.33 avg=1.54\n",
            "[112 | 59.09] loss=1.51 avg=1.54\n",
            "[113 | 59.58] loss=1.64 avg=1.54\n",
            "[114 | 60.06] loss=1.41 avg=1.54\n",
            "[115 | 60.56] loss=1.51 avg=1.54\n",
            "[116 | 61.05] loss=1.57 avg=1.54\n",
            "[117 | 61.54] loss=1.60 avg=1.54\n",
            "[118 | 62.03] loss=1.50 avg=1.54\n",
            "[119 | 62.52] loss=1.32 avg=1.54\n",
            "[120 | 63.00] loss=1.57 avg=1.54\n",
            "[121 | 63.50] loss=1.61 avg=1.54\n",
            "[122 | 63.98] loss=1.50 avg=1.54\n",
            "[123 | 64.47] loss=1.49 avg=1.54\n",
            "[124 | 64.96] loss=1.22 avg=1.53\n",
            "[125 | 65.45] loss=1.28 avg=1.53\n",
            "[126 | 65.94] loss=1.35 avg=1.53\n",
            "[127 | 66.43] loss=1.43 avg=1.53\n",
            "[128 | 66.92] loss=1.30 avg=1.52\n",
            "[129 | 67.42] loss=1.67 avg=1.52\n",
            "[130 | 67.92] loss=1.47 avg=1.52\n",
            "[131 | 68.41] loss=1.41 avg=1.52\n",
            "[132 | 68.90] loss=1.71 avg=1.52\n",
            "[133 | 69.40] loss=1.57 avg=1.52\n",
            "[134 | 69.89] loss=1.48 avg=1.52\n",
            "[135 | 70.39] loss=1.24 avg=1.52\n",
            "[136 | 70.88] loss=1.37 avg=1.52\n",
            "[137 | 71.38] loss=1.64 avg=1.52\n",
            "[138 | 71.88] loss=1.89 avg=1.53\n",
            "[139 | 72.38] loss=1.45 avg=1.52\n",
            "[140 | 72.87] loss=1.31 avg=1.52\n",
            "[141 | 73.38] loss=1.33 avg=1.52\n",
            "[142 | 73.88] loss=1.44 avg=1.52\n",
            "[143 | 74.38] loss=1.43 avg=1.52\n",
            "[144 | 74.88] loss=1.81 avg=1.52\n",
            "[145 | 75.38] loss=1.55 avg=1.52\n",
            "[146 | 75.87] loss=1.37 avg=1.52\n",
            "[147 | 76.36] loss=1.37 avg=1.52\n",
            "[148 | 76.85] loss=1.60 avg=1.52\n",
            "[149 | 77.35] loss=1.38 avg=1.52\n",
            "[150 | 77.84] loss=1.41 avg=1.51\n",
            "[151 | 78.34] loss=1.54 avg=1.52\n",
            "[152 | 78.83] loss=1.59 avg=1.52\n",
            "[153 | 79.32] loss=1.38 avg=1.51\n",
            "[154 | 79.82] loss=1.28 avg=1.51\n",
            "[155 | 80.31] loss=1.45 avg=1.51\n",
            "[156 | 80.81] loss=1.37 avg=1.51\n",
            "[157 | 81.31] loss=1.54 avg=1.51\n",
            "[158 | 81.80] loss=1.40 avg=1.51\n",
            "[159 | 82.30] loss=1.36 avg=1.51\n",
            "[160 | 82.80] loss=1.54 avg=1.51\n",
            "[161 | 83.30] loss=1.28 avg=1.50\n",
            "[162 | 83.81] loss=1.40 avg=1.50\n",
            "[163 | 84.30] loss=1.28 avg=1.50\n",
            "[164 | 84.81] loss=1.59 avg=1.50\n",
            "[165 | 85.31] loss=1.31 avg=1.50\n",
            "[166 | 85.81] loss=1.50 avg=1.50\n",
            "[167 | 86.31] loss=1.41 avg=1.50\n",
            "[168 | 86.81] loss=1.29 avg=1.49\n",
            "[169 | 87.31] loss=1.45 avg=1.49\n",
            "[170 | 87.82] loss=1.38 avg=1.49\n",
            "[171 | 88.31] loss=1.27 avg=1.49\n",
            "[172 | 88.83] loss=1.48 avg=1.49\n",
            "[173 | 89.33] loss=1.22 avg=1.49\n",
            "[174 | 89.83] loss=1.21 avg=1.48\n",
            "[175 | 90.34] loss=1.20 avg=1.48\n",
            "[176 | 90.84] loss=1.52 avg=1.48\n",
            "[177 | 91.34] loss=1.73 avg=1.48\n",
            "[178 | 91.85] loss=1.40 avg=1.48\n",
            "[179 | 92.36] loss=1.72 avg=1.49\n",
            "[180 | 92.87] loss=1.32 avg=1.48\n",
            "[181 | 93.38] loss=1.13 avg=1.48\n",
            "[182 | 93.88] loss=1.14 avg=1.47\n",
            "[183 | 94.39] loss=1.50 avg=1.48\n",
            "[184 | 94.90] loss=1.34 avg=1.47\n",
            "[185 | 95.41] loss=1.34 avg=1.47\n",
            "[186 | 95.92] loss=1.25 avg=1.47\n",
            "[187 | 96.43] loss=1.48 avg=1.47\n",
            "[188 | 96.94] loss=1.41 avg=1.47\n",
            "[189 | 97.45] loss=1.52 avg=1.47\n",
            "[190 | 97.96] loss=1.37 avg=1.47\n",
            "[191 | 98.47] loss=1.33 avg=1.47\n",
            "[192 | 98.98] loss=1.46 avg=1.47\n",
            "[193 | 99.49] loss=1.34 avg=1.46\n",
            "[194 | 100.00] loss=1.56 avg=1.47\n",
            "[195 | 100.52] loss=1.28 avg=1.46\n",
            "[196 | 101.03] loss=1.43 avg=1.46\n",
            "[197 | 101.54] loss=1.23 avg=1.46\n",
            "[198 | 102.05] loss=1.16 avg=1.46\n",
            "[199 | 102.56] loss=1.47 avg=1.46\n",
            "======== SAMPLE 1 ========\n",
            ":23 PM] David Gallant: But we never had to.\n",
            "\n",
            "09:23:28,231 --> 00:23:26,981\n",
            "We had one\n",
            "\n",
            "09:23:34,981 --> 00:23:37,639\n",
            "for the whole thing\n",
            "\n",
            "09:23:41,616 --> 00:23:45,891\n",
            "the way you do things\n",
            "\n",
            "09:23:45,891 --> 00:23:49,058\n",
            "is that there's... something in me\n",
            "that I like. It's...\n",
            "\n",
            "09:23:49,058 --> 00:23:52,835\n",
            "...in the way\n",
            "I like things.\n",
            "\n",
            "09:23:52,835 --> 00:23:56,911\n",
            "...but it can't be my favorite.\n",
            "\n",
            "09:23:57,966 --> 00:24:02,829\n",
            "I'd never be able to do something like that,\n",
            "\n",
            "09:24:02,829 --> 00:24:06,963\n",
            "because it's not my thing.\n",
            "\n",
            "09:24:06,963 --> 00:24:10,969\n",
            "That's not why I like things\n",
            "\n",
            "09:24:10,969 --> 00:24:15,087\n",
            "so much.\n",
            "\n",
            "09:24:15,089 --> 00:24:19,080\n",
            "I like the way things are.\n",
            "\n",
            "09:24:19,080 --> 00:24:22,853\n",
            "I like the way they're organized.\n",
            "\n",
            "09:24:22,853 --> 00:24:25,878\n",
            "[laughter]\n",
            "\n",
            "09:24:29,749 --> 00:24:32,113\n",
            "It is unusual for something that other people think\n",
            "is weird to have something so...\n",
            "\n",
            "09:24:32,113 --> 00:24:35,091\n",
            "that it's the opposite\n",
            "or less unusual.\n",
            "\n",
            "09:24:35,091 --> 00:24:38,764\n",
            "And also, it's weird that some people\n",
            "can see through my eyes\n",
            "\n",
            "09:24:38,764 --> 00:24:42,869\n",
            "and say, \"Thank you, my god.\"\n",
            "\n",
            "09:24:43,869 --> 00:24:46,869\n",
            "[laughter]\n",
            "\n",
            "09:24:48,869 --> 00:24:51,083\n",
            "\"You're so kind.\"\n",
            "\n",
            "09:24:52,869 --> 00:24:55,084\n",
            "[laughter]\n",
            "\n",
            "09:24:55,084 --> 00:24:57,855\n",
            "\"Come on, man!\"\n",
            "\n",
            "09:24:57,855 --> 00:25:00,083\n",
            "[cheering]\n",
            "\n",
            "09:25:00,071 --> 00:25:01,764\n",
            "\"You know what could be more strange?\"\n",
            "\"Uh, you know what I mean?\"\n",
            "\n",
            "09:25:01,764 --> 00:25:04,849\n",
            "\"What happened to the sky?\"\n",
            "\n",
            "09:25:04,849 --> 00:25:06,865\n",
            "\"Oh, shit, I don't get it. They couldn't afford the supplies.\n",
            "\n",
            "09:25:06,865 --> 00:25:08,883\n",
            "[laughter]\n",
            "\n",
            "09:25:08,885 --> 00:25:11,967\n",
            "...but it was like...\n",
            "\n",
            "09:25:11,967 --> 00:25:14,049\n",
            "\"It's going to be a tough race, man.\"\n",
            "\n",
            "09:25:14,049 --> 00:25:16,044\n",
            "[laughter]\n",
            "\n",
            "09:25:16,044 --> 00:25:18,083\n",
            "Because at some point, you know?\n",
            "\n",
            "09:25:18,083 --> 00:25:19,143\n",
            "When...\n",
            "\n",
            "09:25:19,143 --> 00:25:22,869\n",
            "Then you can't even look\n",
            "\n",
            "09:25:22,869 --> 00:25:24,966\n",
            "through my eyes and try to...\n",
            "\n",
            "09:25:26,866 --> 00:25:28,059\n",
            "[laughter, applause]\n",
            "\n",
            "09:25:29,633 --> 00:25:32,065\n",
            "[cheering, applause]\n",
            "\n",
            "09:25:33,065 --> 00:25:37,039\n",
            "I'd like to hear some great stories\n",
            "\n",
            "09:25:37,069 --> 00:25:40,979\n",
            "about sports,\n",
            "\n",
            "09:25:40,979 --> 00:\n",
            "\n",
            "[200 | 117.61] loss=1.43 avg=1.46\n",
            "[201 | 118.13] loss=1.25 avg=1.45\n",
            "[202 | 118.66] loss=1.64 avg=1.46\n",
            "[203 | 119.17] loss=1.50 avg=1.46\n",
            "[204 | 119.69] loss=1.48 avg=1.46\n",
            "[205 | 120.21] loss=1.45 avg=1.46\n",
            "[206 | 120.74] loss=1.33 avg=1.46\n",
            "[207 | 121.25] loss=1.57 avg=1.46\n",
            "[208 | 121.78] loss=1.06 avg=1.45\n",
            "[209 | 122.30] loss=1.29 avg=1.45\n",
            "[210 | 122.82] loss=1.40 avg=1.45\n",
            "[211 | 123.35] loss=1.36 avg=1.45\n",
            "[212 | 123.88] loss=1.27 avg=1.45\n",
            "[213 | 124.39] loss=1.16 avg=1.44\n",
            "[214 | 124.92] loss=1.24 avg=1.44\n",
            "[215 | 125.44] loss=1.09 avg=1.44\n",
            "[216 | 125.97] loss=1.48 avg=1.44\n",
            "[217 | 126.48] loss=1.22 avg=1.44\n",
            "[218 | 127.01] loss=1.39 avg=1.44\n",
            "[219 | 127.52] loss=1.40 avg=1.44\n",
            "[220 | 128.05] loss=1.12 avg=1.43\n",
            "[221 | 128.57] loss=1.17 avg=1.43\n",
            "[222 | 129.09] loss=1.18 avg=1.43\n",
            "[223 | 129.61] loss=1.34 avg=1.43\n",
            "[224 | 130.14] loss=1.27 avg=1.42\n",
            "[225 | 130.66] loss=1.20 avg=1.42\n",
            "[226 | 131.19] loss=1.51 avg=1.42\n",
            "[227 | 131.71] loss=1.51 avg=1.42\n",
            "[228 | 132.23] loss=1.14 avg=1.42\n",
            "[229 | 132.75] loss=1.41 avg=1.42\n",
            "[230 | 133.27] loss=1.09 avg=1.42\n",
            "[231 | 133.80] loss=1.44 avg=1.42\n",
            "[232 | 134.32] loss=1.29 avg=1.41\n",
            "[233 | 134.84] loss=1.32 avg=1.41\n",
            "[234 | 135.36] loss=1.07 avg=1.41\n",
            "[235 | 135.89] loss=1.15 avg=1.41\n",
            "[236 | 136.41] loss=1.24 avg=1.41\n",
            "[237 | 136.92] loss=1.40 avg=1.41\n",
            "[238 | 137.45] loss=1.21 avg=1.40\n",
            "[239 | 137.97] loss=1.25 avg=1.40\n",
            "[240 | 138.48] loss=1.34 avg=1.40\n",
            "[241 | 139.01] loss=1.37 avg=1.40\n",
            "[242 | 139.52] loss=1.19 avg=1.40\n",
            "[243 | 140.04] loss=1.21 avg=1.40\n",
            "[244 | 140.56] loss=1.20 avg=1.39\n",
            "[245 | 141.07] loss=1.34 avg=1.39\n",
            "[246 | 141.60] loss=1.08 avg=1.39\n",
            "[247 | 142.11] loss=1.21 avg=1.39\n",
            "[248 | 142.62] loss=1.09 avg=1.38\n",
            "[249 | 143.14] loss=1.24 avg=1.38\n",
            "Saving checkpoint/run1/model-250\n",
            "[250 | 148.62] loss=1.12 avg=1.38\n",
            "[251 | 149.11] loss=1.26 avg=1.38\n",
            "[252 | 149.64] loss=0.85 avg=1.37\n",
            "[253 | 150.13] loss=1.50 avg=1.37\n",
            "[254 | 150.66] loss=1.25 avg=1.37\n",
            "[255 | 151.16] loss=1.07 avg=1.37\n",
            "[256 | 151.68] loss=1.07 avg=1.37\n",
            "[257 | 152.19] loss=1.09 avg=1.36\n",
            "[258 | 152.70] loss=1.29 avg=1.36\n",
            "[259 | 153.21] loss=1.49 avg=1.36\n",
            "[260 | 153.72] loss=1.08 avg=1.36\n",
            "[261 | 154.23] loss=1.32 avg=1.36\n",
            "[262 | 154.74] loss=1.13 avg=1.36\n",
            "[263 | 155.26] loss=1.37 avg=1.36\n",
            "[264 | 155.77] loss=1.35 avg=1.36\n",
            "[265 | 156.28] loss=1.02 avg=1.35\n",
            "[266 | 156.79] loss=1.30 avg=1.35\n",
            "[267 | 157.30] loss=1.14 avg=1.35\n",
            "[268 | 157.81] loss=1.30 avg=1.35\n",
            "[269 | 158.32] loss=1.18 avg=1.35\n",
            "[270 | 158.83] loss=1.72 avg=1.35\n",
            "[271 | 159.34] loss=1.03 avg=1.35\n",
            "[272 | 159.85] loss=1.12 avg=1.35\n",
            "[273 | 160.37] loss=1.32 avg=1.35\n",
            "[274 | 160.89] loss=0.94 avg=1.34\n",
            "[275 | 161.40] loss=1.12 avg=1.34\n",
            "[276 | 161.91] loss=1.45 avg=1.34\n",
            "[277 | 162.42] loss=1.40 avg=1.34\n",
            "[278 | 162.93] loss=1.37 avg=1.34\n",
            "[279 | 163.44] loss=1.40 avg=1.34\n",
            "[280 | 163.96] loss=1.26 avg=1.34\n",
            "[281 | 164.48] loss=1.29 avg=1.34\n",
            "[282 | 164.99] loss=1.19 avg=1.34\n",
            "[283 | 165.50] loss=1.22 avg=1.34\n",
            "[284 | 166.01] loss=0.96 avg=1.33\n",
            "[285 | 166.52] loss=1.15 avg=1.33\n",
            "[286 | 167.03] loss=1.16 avg=1.33\n",
            "[287 | 167.56] loss=1.37 avg=1.33\n",
            "[288 | 168.07] loss=1.35 avg=1.33\n",
            "[289 | 168.59] loss=1.36 avg=1.33\n",
            "[290 | 169.10] loss=1.12 avg=1.33\n",
            "[291 | 169.61] loss=1.39 avg=1.33\n",
            "[292 | 170.12] loss=1.09 avg=1.33\n",
            "[293 | 170.63] loss=1.10 avg=1.33\n",
            "[294 | 171.15] loss=1.00 avg=1.32\n",
            "[295 | 171.67] loss=1.15 avg=1.32\n",
            "[296 | 172.19] loss=1.13 avg=1.32\n",
            "[297 | 172.71] loss=1.20 avg=1.32\n",
            "[298 | 173.23] loss=1.25 avg=1.32\n",
            "[299 | 173.74] loss=1.09 avg=1.31\n",
            "[300 | 174.26] loss=1.10 avg=1.31\n",
            "[301 | 174.78] loss=1.36 avg=1.31\n",
            "[302 | 175.29] loss=1.08 avg=1.31\n",
            "[303 | 175.80] loss=1.01 avg=1.31\n",
            "[304 | 176.33] loss=1.07 avg=1.30\n",
            "[305 | 176.84] loss=1.31 avg=1.30\n",
            "[306 | 177.36] loss=1.05 avg=1.30\n",
            "[307 | 177.88] loss=1.06 avg=1.30\n",
            "[308 | 178.40] loss=1.06 avg=1.30\n",
            "[309 | 178.92] loss=1.31 avg=1.30\n",
            "[310 | 179.45] loss=1.11 avg=1.29\n",
            "[311 | 179.96] loss=1.38 avg=1.30\n",
            "[312 | 180.48] loss=1.53 avg=1.30\n",
            "[313 | 181.00] loss=1.14 avg=1.30\n",
            "[314 | 181.51] loss=0.97 avg=1.29\n",
            "[315 | 182.03] loss=1.14 avg=1.29\n",
            "[316 | 182.54] loss=1.04 avg=1.29\n",
            "[317 | 183.05] loss=0.89 avg=1.28\n",
            "[318 | 183.58] loss=0.89 avg=1.28\n",
            "[319 | 184.09] loss=1.13 avg=1.28\n",
            "[320 | 184.60] loss=0.90 avg=1.27\n",
            "[321 | 185.11] loss=1.20 avg=1.27\n",
            "[322 | 185.64] loss=1.01 avg=1.27\n",
            "[323 | 186.15] loss=1.32 avg=1.27\n",
            "[324 | 186.67] loss=0.92 avg=1.27\n",
            "[325 | 187.18] loss=1.24 avg=1.27\n",
            "[326 | 187.70] loss=1.39 avg=1.27\n",
            "[327 | 188.22] loss=1.42 avg=1.27\n",
            "[328 | 188.74] loss=1.30 avg=1.27\n",
            "[329 | 189.25] loss=1.19 avg=1.27\n",
            "[330 | 189.76] loss=1.00 avg=1.27\n",
            "[331 | 190.28] loss=0.82 avg=1.26\n",
            "[332 | 190.80] loss=1.03 avg=1.26\n",
            "[333 | 191.31] loss=1.15 avg=1.26\n",
            "[334 | 191.82] loss=1.10 avg=1.26\n",
            "[335 | 192.34] loss=1.55 avg=1.26\n",
            "[336 | 192.86] loss=1.36 avg=1.26\n",
            "[337 | 193.38] loss=1.38 avg=1.26\n",
            "[338 | 193.89] loss=0.88 avg=1.26\n",
            "[339 | 194.41] loss=1.32 avg=1.26\n",
            "[340 | 194.93] loss=0.82 avg=1.25\n",
            "[341 | 195.44] loss=0.98 avg=1.25\n",
            "[342 | 195.95] loss=0.90 avg=1.25\n",
            "[343 | 196.46] loss=0.95 avg=1.25\n",
            "[344 | 196.98] loss=1.25 avg=1.25\n",
            "[345 | 197.48] loss=1.04 avg=1.24\n",
            "[346 | 197.99] loss=0.84 avg=1.24\n",
            "[347 | 198.50] loss=1.22 avg=1.24\n",
            "[348 | 199.01] loss=1.46 avg=1.24\n",
            "[349 | 199.54] loss=0.95 avg=1.24\n",
            "[350 | 200.05] loss=1.51 avg=1.24\n",
            "[351 | 200.57] loss=1.05 avg=1.24\n",
            "[352 | 201.09] loss=0.87 avg=1.24\n",
            "[353 | 201.60] loss=1.13 avg=1.23\n",
            "[354 | 202.11] loss=1.03 avg=1.23\n",
            "[355 | 202.62] loss=1.28 avg=1.23\n",
            "[356 | 203.13] loss=1.29 avg=1.23\n",
            "[357 | 203.65] loss=1.02 avg=1.23\n",
            "[358 | 204.17] loss=0.91 avg=1.23\n",
            "[359 | 204.68] loss=1.18 avg=1.23\n",
            "[360 | 205.19] loss=0.96 avg=1.22\n",
            "[361 | 205.70] loss=0.97 avg=1.22\n",
            "[362 | 206.22] loss=1.11 avg=1.22\n",
            "[363 | 206.73] loss=1.12 avg=1.22\n",
            "[364 | 207.25] loss=1.16 avg=1.22\n",
            "[365 | 207.77] loss=1.00 avg=1.22\n",
            "[366 | 208.29] loss=1.16 avg=1.22\n",
            "[367 | 208.81] loss=0.87 avg=1.21\n",
            "[368 | 209.32] loss=1.33 avg=1.21\n",
            "[369 | 209.83] loss=1.10 avg=1.21\n",
            "[370 | 210.34] loss=0.93 avg=1.21\n",
            "[371 | 210.86] loss=1.14 avg=1.21\n",
            "[372 | 211.38] loss=1.05 avg=1.21\n",
            "[373 | 211.89] loss=1.27 avg=1.21\n",
            "[374 | 212.42] loss=0.95 avg=1.21\n",
            "[375 | 212.93] loss=1.40 avg=1.21\n",
            "[376 | 213.44] loss=1.23 avg=1.21\n",
            "[377 | 213.95] loss=0.91 avg=1.20\n",
            "[378 | 214.46] loss=1.01 avg=1.20\n",
            "[379 | 214.97] loss=1.25 avg=1.20\n",
            "[380 | 215.48] loss=1.29 avg=1.20\n",
            "[381 | 215.99] loss=1.31 avg=1.20\n",
            "[382 | 216.50] loss=1.02 avg=1.20\n",
            "[383 | 217.01] loss=1.17 avg=1.20\n",
            "[384 | 217.52] loss=0.87 avg=1.20\n",
            "[385 | 218.04] loss=0.90 avg=1.20\n",
            "[386 | 218.56] loss=1.19 avg=1.20\n",
            "[387 | 219.07] loss=0.81 avg=1.19\n",
            "[388 | 219.59] loss=0.85 avg=1.19\n",
            "[389 | 220.11] loss=0.94 avg=1.19\n",
            "[390 | 220.62] loss=1.18 avg=1.19\n",
            "[391 | 221.14] loss=1.11 avg=1.19\n",
            "[392 | 221.66] loss=0.96 avg=1.18\n",
            "[393 | 222.17] loss=0.80 avg=1.18\n",
            "[394 | 222.69] loss=1.62 avg=1.18\n",
            "[395 | 223.21] loss=1.05 avg=1.18\n",
            "[396 | 223.72] loss=0.89 avg=1.18\n",
            "[397 | 224.23] loss=0.88 avg=1.18\n",
            "[398 | 224.75] loss=0.92 avg=1.17\n",
            "[399 | 225.27] loss=0.86 avg=1.17\n",
            "======== SAMPLE 1 ========\n",
            ":56:36,523\n",
            "They're my friends, I'm here to hang out\n",
            "with them, they're not\n",
            "\n",
            "879\n",
            "00:56:36,573 --> 00:56:38,967\n",
            "looking after me, just, just say what you like\n",
            "about\n",
            "\n",
            "880\n",
            "00:56:38,977 --> 00:56:40,317\n",
            "everyone, just say what you like\n",
            "about\n",
            "\n",
            "881\n",
            "00:56:40,351 --> 00:56:43,731\n",
            "I think it's awesome that a lot of teenagers are\n",
            "at a football game now.\n",
            "\n",
            "882\n",
            "00:56:43,810 --> 00:56:46,534\n",
            "I love that the sound system\n",
            "for footballs\n",
            "\n",
            "883\n",
            "00:56:46,538 --> 00:56:50,977\n",
            "is terrible, you know.\n",
            "\n",
            "884\n",
            "00:56:52,072 --> 00:56:54,721\n",
            "It's just terrible.\n",
            "I'm against it.\n",
            "\n",
            "885\n",
            "00:56:54,755 --> 00:56:56,898\n",
            "I think I'm gonna have some\n",
            "disgusting sports talk\n",
            "\n",
            "886\n",
            "00:56:56,992 --> 00:56:58,843\n",
            "about what players believe.\n",
            "\n",
            "887\n",
            "00:56:58,967 --> 00:57:00,982\n",
            "I think we should have fewer video games\n",
            "\n",
            "888\n",
            "00:57:01,024 --> 00:57:04,898\n",
            "in the future.\n",
            "\n",
            "889\n",
            "00:57:05,051 --> 00:57:06,631\n",
            "I'm against it.\n",
            "\n",
            "890\n",
            "00:57:06,668 --> 00:57:08,666\n",
            "I think what we really need with video games\n",
            "\n",
            "891\n",
            "00:57:10,052 --> 00:57:14,908\n",
            "let's do is underperform traditional sports.\n",
            "\n",
            "892\n",
            "00:57:16,037 --> 00:57:18,972\n",
            "We're putting so much effort into sports,\n",
            "we can't make you lose your cool.\n",
            "\n",
            "893\n",
            "00:57:19,095 --> 00:57:21,527\n",
            "Sports are just not for us.\n",
            "We should put more effort\n",
            "into sports.\n",
            "\n",
            "894\n",
            "00:57:21,551 --> 00:57:24,973\n",
            "into traditional sports.\n",
            "\n",
            "895\n",
            "00:57:28,018 --> 00:57:32,079\n",
            "Sports are definitely for people of color\n",
            "and people with certain racial or ethnic\n",
            "disabilities.\n",
            "\n",
            "896\n",
            "00:57:34,015 --> 00:57:37,841\n",
            "Sports shouldn't be limited to just a few\n",
            "males.\n",
            "\n",
            "897\n",
            "00:57:37,905 --> 00:57:39,138\n",
            "\"Look at this guy.\"\n",
            "\"Look at him!\"\n",
            "\n",
            "898\n",
            "00:57:43,033 --> 00:57:46,942\n",
            "I've noticed that in sports,\n",
            "when I look at players, I'm kind of drawn toward\n",
            "the bottom.\n",
            "\n",
            "899\n",
            "00:57:49,766 --> 00:57:53,276\n",
            "That's because I like the sport\n",
            "\n",
            "900\n",
            "00:57:53,310 --> 00:57:56,531\n",
            "that a lot of people with certain\n",
            "disabilities have found.\n",
            "\n",
            "901\n",
            "00:57:56,613 --> 00:57:58,180\n",
            "I guess the trick to just not being drawn\n",
            "to the bottom of the pool\n",
            "\n",
            "902\n",
            "00:57:58,300 --> 00:57:59,904\n",
            "is to just look at the other guy.\n",
            "\n",
            "903\n",
            "00:58:00,057 --> 00:58:03,566\n",
            "You know what I mean?\n",
            "\n",
            "904\n",
            "00:58:05,644 --> 00:58:07,051\n",
            "If there's one thing I learned as a sports information person,\n",
            "you know what I mean?\n",
            "\n",
            "905\n",
            "00:58:07,091 --> 00:58:10,065\n",
            "If you're in a group, if you can't talk to the other guys, then\n",
            "\n",
            "906\n",
            "00:58:10,104 --> 00:58:12,049\n",
            "then maybe you should just look around the room instead.\n",
            "\n",
            "907\n",
            "00:58:12,104 --> 00:58:14,065\n",
            "Instead of looking at the whole group\n",
            "just in the case.\n",
            "\n",
            "908\n",
            "00:58:14,104 --> 00:58:18,764\n",
            "It'll be a lot less stressful\n",
            "\n",
            "[400 | 238.70] loss=0.79 avg=1.17\n",
            "[401 | 239.22] loss=0.83 avg=1.16\n",
            "[402 | 239.74] loss=0.96 avg=1.16\n",
            "[403 | 240.25] loss=1.09 avg=1.16\n",
            "[404 | 240.76] loss=0.98 avg=1.16\n",
            "[405 | 241.27] loss=1.34 avg=1.16\n",
            "[406 | 241.78] loss=1.27 avg=1.16\n",
            "[407 | 242.30] loss=1.04 avg=1.16\n",
            "[408 | 242.82] loss=0.99 avg=1.16\n",
            "[409 | 243.33] loss=0.71 avg=1.15\n",
            "[410 | 243.84] loss=1.21 avg=1.15\n",
            "[411 | 244.35] loss=1.13 avg=1.15\n",
            "[412 | 244.86] loss=1.32 avg=1.16\n",
            "[413 | 245.37] loss=1.30 avg=1.16\n",
            "[414 | 245.88] loss=0.70 avg=1.15\n",
            "[415 | 246.38] loss=0.89 avg=1.15\n",
            "[416 | 246.90] loss=0.87 avg=1.15\n",
            "[417 | 247.40] loss=0.70 avg=1.14\n",
            "[418 | 247.91] loss=1.03 avg=1.14\n",
            "[419 | 248.42] loss=1.05 avg=1.14\n",
            "[420 | 248.94] loss=0.86 avg=1.14\n",
            "[421 | 249.45] loss=0.57 avg=1.13\n",
            "[422 | 249.97] loss=0.94 avg=1.13\n",
            "[423 | 250.48] loss=1.17 avg=1.13\n",
            "[424 | 250.99] loss=1.06 avg=1.13\n",
            "[425 | 251.50] loss=0.79 avg=1.13\n",
            "[426 | 252.01] loss=1.07 avg=1.13\n",
            "[427 | 252.52] loss=0.93 avg=1.12\n",
            "[428 | 253.03] loss=1.11 avg=1.12\n",
            "[429 | 253.54] loss=0.74 avg=1.12\n",
            "[430 | 254.05] loss=1.07 avg=1.12\n",
            "[431 | 254.56] loss=1.13 avg=1.12\n",
            "[432 | 255.07] loss=0.72 avg=1.12\n",
            "[433 | 255.58] loss=1.56 avg=1.12\n",
            "[434 | 256.08] loss=0.94 avg=1.12\n",
            "[435 | 256.60] loss=1.48 avg=1.12\n",
            "[436 | 257.10] loss=0.80 avg=1.12\n",
            "[437 | 257.61] loss=1.38 avg=1.12\n",
            "[438 | 258.12] loss=1.02 avg=1.12\n",
            "[439 | 258.63] loss=1.26 avg=1.12\n",
            "[440 | 259.14] loss=0.95 avg=1.12\n",
            "[441 | 259.65] loss=0.82 avg=1.12\n",
            "[442 | 260.16] loss=0.78 avg=1.11\n",
            "[443 | 260.67] loss=1.65 avg=1.12\n",
            "[444 | 261.18] loss=0.87 avg=1.12\n",
            "[445 | 261.69] loss=0.96 avg=1.11\n",
            "[446 | 262.20] loss=0.92 avg=1.11\n",
            "[447 | 262.72] loss=0.61 avg=1.11\n",
            "[448 | 263.23] loss=0.94 avg=1.11\n",
            "[449 | 263.75] loss=0.73 avg=1.10\n",
            "[450 | 264.25] loss=1.23 avg=1.10\n",
            "[451 | 264.76] loss=0.79 avg=1.10\n",
            "[452 | 265.28] loss=1.15 avg=1.10\n",
            "[453 | 265.78] loss=0.96 avg=1.10\n",
            "[454 | 266.29] loss=0.74 avg=1.10\n",
            "[455 | 266.80] loss=0.77 avg=1.09\n",
            "[456 | 267.31] loss=0.87 avg=1.09\n",
            "[457 | 267.82] loss=0.78 avg=1.09\n",
            "[458 | 268.33] loss=0.88 avg=1.08\n",
            "[459 | 268.85] loss=0.75 avg=1.08\n",
            "[460 | 269.37] loss=0.92 avg=1.08\n",
            "[461 | 269.88] loss=0.87 avg=1.08\n",
            "[462 | 270.39] loss=0.96 avg=1.08\n",
            "[463 | 270.90] loss=0.89 avg=1.07\n",
            "[464 | 271.42] loss=0.99 avg=1.07\n",
            "[465 | 271.94] loss=0.61 avg=1.07\n",
            "[466 | 272.44] loss=0.82 avg=1.07\n",
            "[467 | 272.96] loss=1.01 avg=1.07\n",
            "[468 | 273.47] loss=0.92 avg=1.06\n",
            "[469 | 273.98] loss=0.70 avg=1.06\n",
            "[470 | 274.50] loss=1.07 avg=1.06\n",
            "[471 | 275.01] loss=1.07 avg=1.06\n",
            "[472 | 275.52] loss=0.61 avg=1.06\n",
            "[473 | 276.03] loss=1.26 avg=1.06\n",
            "[474 | 276.54] loss=0.78 avg=1.06\n",
            "[475 | 277.05] loss=1.15 avg=1.06\n",
            "[476 | 277.56] loss=0.87 avg=1.05\n",
            "[477 | 278.07] loss=0.90 avg=1.05\n",
            "[478 | 278.58] loss=0.87 avg=1.05\n",
            "[479 | 279.09] loss=0.69 avg=1.05\n",
            "[480 | 279.60] loss=0.97 avg=1.05\n",
            "[481 | 280.11] loss=1.02 avg=1.05\n",
            "[482 | 280.62] loss=0.78 avg=1.04\n",
            "[483 | 281.13] loss=0.88 avg=1.04\n",
            "[484 | 281.64] loss=0.71 avg=1.04\n",
            "[485 | 282.16] loss=0.78 avg=1.04\n",
            "[486 | 282.68] loss=0.89 avg=1.04\n",
            "[487 | 283.19] loss=0.65 avg=1.03\n",
            "[488 | 283.70] loss=0.59 avg=1.03\n",
            "[489 | 284.21] loss=0.71 avg=1.02\n",
            "[490 | 284.73] loss=0.78 avg=1.02\n",
            "[491 | 285.25] loss=0.81 avg=1.02\n",
            "[492 | 285.76] loss=0.75 avg=1.02\n",
            "[493 | 286.27] loss=0.63 avg=1.01\n",
            "[494 | 286.78] loss=0.99 avg=1.01\n",
            "[495 | 287.30] loss=1.31 avg=1.02\n",
            "[496 | 287.82] loss=0.77 avg=1.01\n",
            "[497 | 288.33] loss=0.66 avg=1.01\n",
            "[498 | 288.84] loss=1.59 avg=1.02\n",
            "[499 | 289.35] loss=0.75 avg=1.01\n",
            "Saving checkpoint/run1/model-500\n",
            "[500 | 293.37] loss=1.27 avg=1.01\n",
            "[501 | 293.86] loss=1.10 avg=1.02\n",
            "[502 | 294.39] loss=0.76 avg=1.01\n",
            "[503 | 294.89] loss=0.64 avg=1.01\n",
            "[504 | 295.42] loss=0.94 avg=1.01\n",
            "[505 | 295.92] loss=0.67 avg=1.01\n",
            "[506 | 296.44] loss=0.77 avg=1.00\n",
            "[507 | 296.95] loss=0.92 avg=1.00\n",
            "[508 | 297.46] loss=0.66 avg=1.00\n",
            "[509 | 297.97] loss=0.98 avg=1.00\n",
            "[510 | 298.48] loss=0.77 avg=1.00\n",
            "[511 | 298.99] loss=1.01 avg=1.00\n",
            "[512 | 299.51] loss=0.81 avg=0.99\n",
            "[513 | 300.02] loss=0.83 avg=0.99\n",
            "[514 | 300.53] loss=0.77 avg=0.99\n",
            "[515 | 301.04] loss=0.84 avg=0.99\n",
            "[516 | 301.55] loss=0.88 avg=0.99\n",
            "[517 | 302.06] loss=0.74 avg=0.99\n",
            "[518 | 302.59] loss=1.10 avg=0.99\n",
            "[519 | 303.10] loss=1.09 avg=0.99\n",
            "[520 | 303.61] loss=0.90 avg=0.99\n",
            "[521 | 304.13] loss=0.68 avg=0.98\n",
            "[522 | 304.64] loss=0.73 avg=0.98\n",
            "[523 | 305.15] loss=0.63 avg=0.98\n",
            "[524 | 305.66] loss=0.81 avg=0.98\n",
            "[525 | 306.17] loss=1.16 avg=0.98\n",
            "[526 | 306.68] loss=0.72 avg=0.98\n",
            "[527 | 307.19] loss=0.90 avg=0.97\n",
            "[528 | 307.70] loss=0.68 avg=0.97\n",
            "[529 | 308.22] loss=1.19 avg=0.97\n",
            "[530 | 308.74] loss=1.25 avg=0.98\n",
            "[531 | 309.25] loss=0.81 avg=0.97\n",
            "[532 | 309.77] loss=1.07 avg=0.98\n",
            "[533 | 310.29] loss=0.71 avg=0.97\n",
            "[534 | 310.80] loss=0.55 avg=0.97\n",
            "[535 | 311.33] loss=0.92 avg=0.97\n",
            "[536 | 311.84] loss=0.98 avg=0.97\n",
            "[537 | 312.36] loss=0.79 avg=0.97\n",
            "[538 | 312.87] loss=0.57 avg=0.96\n",
            "[539 | 313.39] loss=0.60 avg=0.96\n",
            "[540 | 313.91] loss=0.59 avg=0.96\n",
            "[541 | 314.42] loss=0.61 avg=0.95\n",
            "[542 | 314.94] loss=0.71 avg=0.95\n",
            "[543 | 315.45] loss=0.90 avg=0.95\n",
            "[544 | 315.97] loss=0.74 avg=0.95\n",
            "[545 | 316.49] loss=0.91 avg=0.95\n",
            "[546 | 317.01] loss=0.70 avg=0.94\n",
            "[547 | 317.54] loss=1.15 avg=0.95\n",
            "[548 | 318.05] loss=1.17 avg=0.95\n",
            "[549 | 318.58] loss=0.65 avg=0.95\n",
            "[550 | 319.09] loss=0.59 avg=0.94\n",
            "[551 | 319.62] loss=0.68 avg=0.94\n",
            "[552 | 320.13] loss=0.73 avg=0.94\n",
            "[553 | 320.66] loss=0.70 avg=0.93\n",
            "[554 | 321.19] loss=0.81 avg=0.93\n",
            "[555 | 321.71] loss=0.88 avg=0.93\n",
            "[556 | 322.23] loss=0.75 avg=0.93\n",
            "[557 | 322.74] loss=0.60 avg=0.93\n",
            "[558 | 323.27] loss=0.83 avg=0.93\n",
            "[559 | 323.78] loss=0.49 avg=0.92\n",
            "[560 | 324.31] loss=1.12 avg=0.92\n",
            "[561 | 324.84] loss=0.69 avg=0.92\n",
            "[562 | 325.35] loss=0.49 avg=0.92\n",
            "[563 | 325.88] loss=0.87 avg=0.92\n",
            "[564 | 326.40] loss=0.92 avg=0.92\n",
            "[565 | 326.92] loss=1.35 avg=0.92\n",
            "[566 | 327.45] loss=0.56 avg=0.92\n",
            "[567 | 327.96] loss=0.88 avg=0.92\n",
            "[568 | 328.47] loss=0.55 avg=0.91\n",
            "[569 | 329.00] loss=0.53 avg=0.91\n",
            "[570 | 329.51] loss=0.61 avg=0.91\n",
            "[571 | 330.02] loss=1.12 avg=0.91\n",
            "[572 | 330.54] loss=0.86 avg=0.91\n",
            "[573 | 331.06] loss=0.87 avg=0.91\n",
            "[574 | 331.57] loss=0.64 avg=0.91\n",
            "[575 | 332.09] loss=0.51 avg=0.90\n",
            "[576 | 332.61] loss=0.83 avg=0.90\n",
            "[577 | 333.13] loss=0.42 avg=0.90\n",
            "[578 | 333.65] loss=0.88 avg=0.90\n",
            "[579 | 334.16] loss=0.57 avg=0.89\n",
            "[580 | 334.68] loss=1.14 avg=0.90\n",
            "[581 | 335.20] loss=0.80 avg=0.89\n",
            "[582 | 335.72] loss=0.70 avg=0.89\n",
            "[583 | 336.24] loss=1.11 avg=0.89\n",
            "[584 | 336.75] loss=0.70 avg=0.89\n",
            "[585 | 337.27] loss=0.61 avg=0.89\n",
            "[586 | 337.78] loss=0.84 avg=0.89\n",
            "[587 | 338.29] loss=0.55 avg=0.89\n",
            "[588 | 338.81] loss=0.65 avg=0.88\n",
            "[589 | 339.33] loss=0.94 avg=0.88\n",
            "[590 | 339.85] loss=0.70 avg=0.88\n",
            "[591 | 340.37] loss=0.58 avg=0.88\n",
            "[592 | 340.88] loss=0.62 avg=0.88\n",
            "[593 | 341.41] loss=0.75 avg=0.88\n",
            "[594 | 341.92] loss=0.85 avg=0.87\n",
            "[595 | 342.43] loss=0.61 avg=0.87\n",
            "[596 | 342.95] loss=0.50 avg=0.87\n",
            "[597 | 343.46] loss=0.84 avg=0.87\n",
            "[598 | 343.99] loss=0.93 avg=0.87\n",
            "[599 | 344.50] loss=0.80 avg=0.87\n",
            "======== SAMPLE 1 ========\n",
            "-17-18\n",
            "Just one...\n",
            "\"I did exactly what you tell me to do.\"\n",
            "\n",
            "260\n",
            "00:11:46,945 --> 00:11:47,946\n",
            "And that's...\n",
            "\n",
            "261\n",
            "00:11:48,023 --> 00:11:50,744\n",
            "That's what people do, you know?\n",
            "You let them do what they want to do.\n",
            "\n",
            "262\n",
            "00:11:51,005 --> 00:11:52,018\n",
            "They let you do what you want to do.\n",
            "And if they start screaming in your direction,\n",
            "\n",
            "263\n",
            "00:11:52,043 --> 00:11:54,935\n",
            "I'll just take the mic and yell at them\n",
            "like, \"Get the fuck out of my life!\n",
            "\n",
            "264\n",
            "00:11:55,842 --> 00:11:58,233\n",
            "I'm a grown man, you son of a bitch.\n",
            "I'm a man.\n",
            "\n",
            "265\n",
            "00:12:01,543 --> 00:12:04,199\n",
            "I own that damn store.\"\n",
            "\n",
            "266\n",
            "00:12:05,199 --> 00:12:07,841\n",
            "I like the term \"women haters' Club\n",
            "'cause it's\n",
            "\n",
            "267\n",
            "00:12:07,871 --> 00:12:09,096\n",
            "the one that counts the haters.\n",
            "The problem with that\n",
            "\n",
            "268\n",
            "00:12:09,961 --> 00:12:12,123\n",
            "is that it's like,\n",
            "\"Um, where is the haters?\n",
            "\n",
            "269\n",
            "00:12:12,157 --> 00:12:14,071\n",
            "I'm just like,\n",
            "I just went to a bad party.\n",
            "\n",
            "270\n",
            "00:12:14,148 --> 00:12:15,067\n",
            "I just wanted an atmosphere\n",
            "at the club.\n",
            "\n",
            "271\n",
            "00:12:15,157 --> 00:12:16,099\n",
            "I just wanted one of these\n",
            "monitors on the barista\n",
            "\n",
            "272\n",
            "00:12:16,062 --> 00:12:17,065\n",
            "looking pretty much like,\n",
            "\"Hey, Gina, just, just, just leave me alone.\n",
            "\n",
            "273\n",
            "00:12:17,099 --> 00:12:20,947\n",
            "I'm happy to pick you up at my house.\n",
            "\n",
            "274\n",
            "00:12:20,947 --> 00:12:21,078\n",
            "I'm happy to work with you,\n",
            "Gina. Thank you.\"\n",
            "\n",
            "275\n",
            "00:12:21,078 --> 00:12:22,093\n",
            "I was at a party and there were\n",
            "a bunch of teenagers\n",
            "\n",
            "276\n",
            "00:12:22,093 --> 00:12:24,947\n",
            "fucking rap music on the bar.\n",
            "\n",
            "277\n",
            "00:12:25,099 --> 00:12:27,741\n",
            "You hit 'a' on the mic.\n",
            "The girl wins.\n",
            "\n",
            "278\n",
            "00:12:27,741 --> 00:12:28,093\n",
            "And you lose.\n",
            "\n",
            "279\n",
            "00:12:28,093 --> 00:12:30,967\n",
            "And you go home, you're like,\n",
            "\"My God. That's so weird.\n",
            "\n",
            "280\n",
            "00:12:31,967 --> 00:12:33,067\n",
            "I don't even know what's the\n",
            "second song on the record.\n",
            "\n",
            "281\n",
            "00:12:33,067 --> 00:12:35,067\n",
            "I don't wanna hear it.\n",
            "\n",
            "282\n",
            "00:12:35,067 --> 00:12:37,067\n",
            "I can hear it, it's like,\n",
            "\"Holy shit. That was so weird.\n",
            "\n",
            "283\n",
            "00:12:37,067 --> 00:12:39,067\n",
            "That was so amazing. That was so\n",
            "out of the ordinary.\n",
            "\n",
            "284\n",
            "00:12:39,067 --> 00:12:44,889\n",
            "I can't believe this happened\n",
            "right on my birthday.\"\n",
            "\n",
            "285\n",
            "00:12:45,889 --> 00:12:48,889\n",
            "\"Oh, yeah? Yeah, I'm so glad I\n",
            "had breakfast with you.\n",
            "\n",
            "286\n",
            "00:12:49,889 --> 00:12:51,889\n",
            "I'm so glad I walked down\n",
            "to your room with you.\n",
            "\n",
            "287\n",
            "00:12:54,889 --> 00:12:55,889\n",
            "I really appreciate it.\"\n",
            "\n",
            "288\n",
            "00:12:56,889 --> 00:12:58,976\n",
            "Thank you so much.\"\n",
            "\n",
            "\n",
            "[600 | 357.65] loss=0.54 avg=0.86\n",
            "[601 | 358.15] loss=0.55 avg=0.86\n",
            "[602 | 358.67] loss=0.59 avg=0.86\n",
            "[603 | 359.18] loss=0.58 avg=0.86\n",
            "[604 | 359.69] loss=0.62 avg=0.85\n",
            "[605 | 360.21] loss=1.28 avg=0.86\n",
            "[606 | 360.73] loss=0.53 avg=0.85\n",
            "[607 | 361.25] loss=0.79 avg=0.85\n",
            "[608 | 361.77] loss=0.67 avg=0.85\n",
            "[609 | 362.28] loss=0.89 avg=0.85\n",
            "[610 | 362.79] loss=0.68 avg=0.85\n",
            "[611 | 363.30] loss=0.87 avg=0.85\n",
            "[612 | 363.82] loss=0.59 avg=0.85\n",
            "[613 | 364.33] loss=0.58 avg=0.85\n",
            "[614 | 364.85] loss=0.81 avg=0.85\n",
            "[615 | 365.36] loss=0.53 avg=0.84\n",
            "[616 | 365.88] loss=0.61 avg=0.84\n",
            "[617 | 366.39] loss=0.47 avg=0.84\n",
            "[618 | 366.91] loss=0.65 avg=0.83\n",
            "[619 | 367.42] loss=0.58 avg=0.83\n",
            "[620 | 367.94] loss=0.66 avg=0.83\n",
            "[621 | 368.45] loss=0.70 avg=0.83\n",
            "[622 | 368.96] loss=0.65 avg=0.83\n",
            "[623 | 369.47] loss=0.77 avg=0.83\n",
            "[624 | 369.98] loss=0.47 avg=0.82\n",
            "[625 | 370.49] loss=0.50 avg=0.82\n",
            "[626 | 371.00] loss=0.55 avg=0.82\n",
            "[627 | 371.51] loss=0.53 avg=0.81\n",
            "[628 | 372.02] loss=1.09 avg=0.82\n",
            "[629 | 372.53] loss=0.51 avg=0.81\n",
            "[630 | 373.05] loss=0.62 avg=0.81\n",
            "[631 | 373.56] loss=0.61 avg=0.81\n",
            "[632 | 374.07] loss=0.80 avg=0.81\n",
            "[633 | 374.58] loss=0.46 avg=0.81\n",
            "[634 | 375.09] loss=0.72 avg=0.81\n",
            "[635 | 375.61] loss=0.82 avg=0.81\n",
            "[636 | 376.13] loss=0.66 avg=0.80\n",
            "[637 | 376.64] loss=0.49 avg=0.80\n",
            "[638 | 377.16] loss=0.65 avg=0.80\n",
            "[639 | 377.67] loss=0.49 avg=0.80\n",
            "[640 | 378.19] loss=0.91 avg=0.80\n",
            "[641 | 378.70] loss=0.67 avg=0.80\n",
            "[642 | 379.22] loss=1.53 avg=0.80\n",
            "[643 | 379.73] loss=0.59 avg=0.80\n",
            "[644 | 380.24] loss=0.74 avg=0.80\n",
            "[645 | 380.76] loss=0.44 avg=0.80\n",
            "[646 | 381.28] loss=0.87 avg=0.80\n",
            "[647 | 381.79] loss=0.69 avg=0.80\n",
            "[648 | 382.30] loss=1.10 avg=0.80\n",
            "[649 | 382.81] loss=0.48 avg=0.80\n",
            "[650 | 383.32] loss=0.64 avg=0.79\n",
            "[651 | 383.84] loss=0.80 avg=0.79\n",
            "[652 | 384.36] loss=0.57 avg=0.79\n",
            "[653 | 384.87] loss=0.64 avg=0.79\n",
            "[654 | 385.38] loss=0.48 avg=0.79\n",
            "[655 | 385.90] loss=0.58 avg=0.79\n",
            "[656 | 386.42] loss=0.47 avg=0.78\n",
            "[657 | 386.93] loss=0.39 avg=0.78\n",
            "[658 | 387.44] loss=0.47 avg=0.78\n",
            "[659 | 387.96] loss=0.41 avg=0.77\n",
            "[660 | 388.48] loss=0.67 avg=0.77\n",
            "[661 | 388.99] loss=0.39 avg=0.77\n",
            "[662 | 389.51] loss=0.71 avg=0.77\n",
            "[663 | 390.03] loss=0.43 avg=0.76\n",
            "[664 | 390.54] loss=0.49 avg=0.76\n",
            "[665 | 391.05] loss=0.70 avg=0.76\n",
            "[666 | 391.56] loss=0.97 avg=0.76\n",
            "[667 | 392.08] loss=0.49 avg=0.76\n",
            "[668 | 392.60] loss=0.66 avg=0.76\n",
            "[669 | 393.11] loss=0.46 avg=0.76\n",
            "[670 | 393.64] loss=0.99 avg=0.76\n",
            "[671 | 394.14] loss=1.25 avg=0.76\n",
            "[672 | 394.66] loss=0.62 avg=0.76\n",
            "[673 | 395.18] loss=0.85 avg=0.76\n",
            "[674 | 395.70] loss=0.38 avg=0.76\n",
            "[675 | 396.22] loss=0.66 avg=0.76\n",
            "[676 | 396.73] loss=0.62 avg=0.76\n",
            "[677 | 397.24] loss=0.84 avg=0.76\n",
            "[678 | 397.75] loss=0.62 avg=0.76\n",
            "[679 | 398.26] loss=0.97 avg=0.76\n",
            "[680 | 398.79] loss=0.43 avg=0.75\n",
            "[681 | 399.30] loss=0.54 avg=0.75\n",
            "[682 | 399.82] loss=0.79 avg=0.75\n",
            "[683 | 400.34] loss=0.75 avg=0.75\n",
            "[684 | 400.85] loss=0.97 avg=0.75\n",
            "[685 | 401.36] loss=0.71 avg=0.75\n",
            "[686 | 401.88] loss=0.46 avg=0.75\n",
            "[687 | 402.40] loss=0.47 avg=0.75\n",
            "[688 | 402.91] loss=0.62 avg=0.75\n",
            "[689 | 403.42] loss=0.54 avg=0.74\n",
            "[690 | 403.94] loss=0.78 avg=0.75\n",
            "[691 | 404.46] loss=0.54 avg=0.74\n",
            "[692 | 404.97] loss=0.81 avg=0.74\n",
            "[693 | 405.48] loss=0.47 avg=0.74\n",
            "[694 | 405.99] loss=1.34 avg=0.75\n",
            "[695 | 406.51] loss=0.56 avg=0.75\n",
            "[696 | 407.02] loss=0.67 avg=0.74\n",
            "[697 | 407.53] loss=0.56 avg=0.74\n",
            "[698 | 408.04] loss=0.60 avg=0.74\n",
            "[699 | 408.55] loss=0.73 avg=0.74\n",
            "[700 | 409.07] loss=0.47 avg=0.74\n",
            "[701 | 409.59] loss=0.42 avg=0.74\n",
            "[702 | 410.10] loss=0.46 avg=0.73\n",
            "[703 | 410.62] loss=0.60 avg=0.73\n",
            "[704 | 411.13] loss=0.78 avg=0.73\n",
            "[705 | 411.65] loss=0.55 avg=0.73\n",
            "[706 | 412.16] loss=0.66 avg=0.73\n",
            "[707 | 412.68] loss=0.48 avg=0.73\n",
            "[708 | 413.18] loss=0.69 avg=0.73\n",
            "[709 | 413.70] loss=0.79 avg=0.73\n",
            "[710 | 414.21] loss=0.56 avg=0.73\n",
            "[711 | 414.73] loss=0.80 avg=0.73\n",
            "[712 | 415.24] loss=0.38 avg=0.72\n",
            "[713 | 415.76] loss=0.63 avg=0.72\n",
            "[714 | 416.27] loss=0.48 avg=0.72\n",
            "[715 | 416.79] loss=0.45 avg=0.72\n",
            "[716 | 417.30] loss=0.82 avg=0.72\n",
            "[717 | 417.81] loss=0.65 avg=0.72\n",
            "[718 | 418.33] loss=0.66 avg=0.72\n",
            "[719 | 418.84] loss=0.71 avg=0.72\n",
            "[720 | 419.36] loss=0.46 avg=0.71\n",
            "[721 | 419.87] loss=0.29 avg=0.71\n",
            "[722 | 420.38] loss=0.62 avg=0.71\n",
            "[723 | 420.89] loss=1.06 avg=0.71\n",
            "[724 | 421.40] loss=0.67 avg=0.71\n",
            "[725 | 421.91] loss=0.49 avg=0.71\n",
            "[726 | 422.42] loss=0.51 avg=0.71\n",
            "[727 | 422.93] loss=0.44 avg=0.70\n",
            "[728 | 423.44] loss=0.47 avg=0.70\n",
            "[729 | 423.96] loss=0.67 avg=0.70\n",
            "[730 | 424.48] loss=0.54 avg=0.70\n",
            "[731 | 424.99] loss=0.79 avg=0.70\n",
            "[732 | 425.51] loss=0.61 avg=0.70\n",
            "[733 | 426.03] loss=0.53 avg=0.70\n",
            "[734 | 426.54] loss=0.53 avg=0.70\n",
            "[735 | 427.06] loss=1.02 avg=0.70\n",
            "[736 | 427.57] loss=0.60 avg=0.70\n",
            "[737 | 428.08] loss=0.62 avg=0.70\n",
            "[738 | 428.59] loss=0.48 avg=0.70\n",
            "[739 | 429.11] loss=0.42 avg=0.69\n",
            "[740 | 429.63] loss=0.41 avg=0.69\n",
            "[741 | 430.15] loss=0.50 avg=0.69\n",
            "[742 | 430.66] loss=0.55 avg=0.69\n",
            "[743 | 431.18] loss=0.47 avg=0.69\n",
            "[744 | 431.70] loss=0.58 avg=0.68\n",
            "[745 | 432.22] loss=0.69 avg=0.68\n",
            "[746 | 432.73] loss=0.62 avg=0.68\n",
            "[747 | 433.24] loss=0.46 avg=0.68\n",
            "[748 | 433.77] loss=0.50 avg=0.68\n",
            "[749 | 434.28] loss=0.38 avg=0.68\n",
            "Saving checkpoint/run1/model-750\n",
            "[750 | 438.10] loss=0.60 avg=0.68\n",
            "[751 | 438.60] loss=0.57 avg=0.67\n",
            "[752 | 439.11] loss=0.45 avg=0.67\n",
            "[753 | 439.62] loss=0.48 avg=0.67\n",
            "[754 | 440.15] loss=0.83 avg=0.67\n",
            "[755 | 440.65] loss=0.61 avg=0.67\n",
            "[756 | 441.17] loss=0.57 avg=0.67\n",
            "[757 | 441.68] loss=0.73 avg=0.67\n",
            "[758 | 442.20] loss=0.76 avg=0.67\n",
            "[759 | 442.71] loss=0.46 avg=0.67\n",
            "[760 | 443.22] loss=0.49 avg=0.67\n",
            "[761 | 443.72] loss=0.68 avg=0.67\n",
            "[762 | 444.25] loss=0.66 avg=0.67\n",
            "[763 | 444.76] loss=0.58 avg=0.67\n",
            "[764 | 445.27] loss=0.51 avg=0.67\n",
            "[765 | 445.80] loss=0.46 avg=0.66\n",
            "[766 | 446.31] loss=0.47 avg=0.66\n",
            "[767 | 446.82] loss=0.40 avg=0.66\n",
            "[768 | 447.35] loss=0.32 avg=0.66\n",
            "[769 | 447.86] loss=0.40 avg=0.65\n",
            "[770 | 448.37] loss=0.37 avg=0.65\n",
            "[771 | 448.89] loss=0.55 avg=0.65\n",
            "[772 | 449.41] loss=0.46 avg=0.65\n",
            "[773 | 449.92] loss=0.43 avg=0.65\n",
            "[774 | 450.45] loss=0.56 avg=0.64\n",
            "[775 | 450.96] loss=1.01 avg=0.65\n",
            "[776 | 451.49] loss=0.82 avg=0.65\n",
            "[777 | 452.00] loss=0.56 avg=0.65\n",
            "[778 | 452.51] loss=0.40 avg=0.65\n",
            "[779 | 453.03] loss=0.35 avg=0.64\n",
            "[780 | 453.55] loss=0.52 avg=0.64\n",
            "[781 | 454.06] loss=0.65 avg=0.64\n",
            "[782 | 454.58] loss=0.57 avg=0.64\n",
            "[783 | 455.09] loss=0.54 avg=0.64\n",
            "[784 | 455.62] loss=0.57 avg=0.64\n",
            "[785 | 456.13] loss=0.47 avg=0.64\n",
            "[786 | 456.66] loss=0.59 avg=0.64\n",
            "[787 | 457.17] loss=0.49 avg=0.64\n",
            "[788 | 457.70] loss=0.58 avg=0.64\n",
            "[789 | 458.21] loss=0.52 avg=0.63\n",
            "[790 | 458.72] loss=0.38 avg=0.63\n",
            "[791 | 459.25] loss=0.53 avg=0.63\n",
            "[792 | 459.77] loss=0.46 avg=0.63\n",
            "[793 | 460.29] loss=0.41 avg=0.63\n",
            "[794 | 460.82] loss=0.50 avg=0.63\n",
            "[795 | 461.33] loss=0.49 avg=0.62\n",
            "[796 | 461.86] loss=0.44 avg=0.62\n",
            "[797 | 462.38] loss=0.39 avg=0.62\n",
            "[798 | 462.90] loss=0.54 avg=0.62\n",
            "[799 | 463.43] loss=0.68 avg=0.62\n",
            "Saving checkpoint/run1/model-800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUa2LujW82cy",
        "colab_type": "text"
      },
      "source": [
        "The step below simply copies your trained model to the model directory, so the output will use your training. If you don't do this, you will be running against the trained GPT-2 model without your finetuning training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNXhOM22fNHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/117M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvtv6NUj5eXU",
        "colab_type": "text"
      },
      "source": [
        "Run the below step to generate unconditional samples (i.e. \"dream mode\").\n",
        "\n",
        "\"top_k\" controls how many options to consider per word (the larger, the more \"diverse\" the output - anything from 1 to about 50 usually works, I think values around 10 are pretty good).\n",
        "\n",
        "\"temperature\" controls the sampling of the words, from 0 to 1 where 1 is the most \"random\".\n",
        "\n",
        "\"length\" controls the number of words in each sample output.\n",
        "\n",
        "This command will run continuously until you turn it off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2csc2bZHfrXd",
        "colab_type": "code",
        "outputId": "dbc68283-3d94-479a-d65e-2b80e65bf962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        }
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --top_k 10 --temperature 1 --length=300"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"src/generate_unconditional_samples.py\", line 7, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 52, in <module>\n",
            "    from tensorflow.core.framework.graph_pb2 import *\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n",
            "    from google.protobuf import descriptor as _descriptor\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/__init__.py\", line 37, in <module>\n",
            "    __import__('pkg_resources').declare_namespace(__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3241, in <module>\n",
            "    @_call_aside\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3225, in _call_aside\n",
            "    f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3277, in _initialize_master_working_set\n",
            "    list(map(working_set.add_entry, sys.path))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 623, in add_entry\n",
            "    for dist in find_distributions(entry, True):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2052, in find_on_path\n",
            "    path_item_entries = _by_version_descending(filtered)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2025, in _by_version_descending\n",
            "    return sorted(names, key=_by_version, reverse=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2021, in _by_version\n",
            "    name, ext = os.path.splitext(name)\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 129, in splitext\n",
            "    return genericpath._splitext(p, sep, None, extsep)\n",
            "  File \"/usr/lib/python3.6/genericpath.py\", line 124, in _splitext\n",
            "    sepIndex = p.rfind(sep)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwZl7JAn6d5y",
        "colab_type": "text"
      },
      "source": [
        "Run the command below to run in interactive / \"completion\" mode. You will get a prompt; just type in whatever prompt text you want, and the model will attempt to complete it \"nsamples\" times.\n",
        "\n",
        "\"top_k\", \"length\", and \"temperature\" work as specified above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugmKkFv__NI_",
        "colab_type": "code",
        "outputId": "f0d92a88-0bde-401e-8b77-c506b30dbf26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8671
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --top_k 10 --length=300 --temperature 1 --nsamples 10"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-04 08:29:30.727211: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-04 08:29:30.727495: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x29f6260 executing computations on platform Host. Devices:\n",
            "2019-05-04 08:29:30.727536: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-04 08:29:30.938981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-04 08:29:30.939614: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x29f5e40 executing computations on platform CUDA. Devices:\n",
            "2019-05-04 08:29:30.939669: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-04 08:29:30.940136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-04 08:29:30.940166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-04 08:29:31.520343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-04 08:29:31.520430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-04 08:29:31.520445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-04 08:29:31.520823: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-04 08:29:31.520898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> I once ate an apple.\n",
            "2019-05-04 08:29:50.154877: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "716\n",
            "00:21:17,719 --> 00:21:20,844\n",
            "I think that's... pretty\n",
            "definitely a guess.\n",
            "\n",
            "717\n",
            "00:21:20,878 --> 00:21:22,871\n",
            "I'm pretty sure it wasripe.\n",
            "\n",
            "718\n",
            "00:21:22,902 --> 00:21:24,928\n",
            "I think salad is another\n",
            "kind of apple pie.\n",
            "\n",
            "719\n",
            "00:21:26,018 --> 00:21:27,821\n",
            "[laughter]\n",
            "\n",
            "720\n",
            "00:21:28,996 --> 00:21:30,989\n",
            "If you make pie,\n",
            "it's pretty close to true.\"\n",
            "\n",
            "721\n",
            "00:31,014 --> 00:21:34,947\n",
            "Freaking pies.\n",
            "\n",
            "722\n",
            "00:21:38,957 --> 00:21:42,969\n",
            "I find that most people\n",
            "disappear if they make a pie alone.\n",
            "\n",
            "723\n",
            "00:21:42,nymphs.\n",
            "\n",
            "724\n",
            "00:21:43,031 --> 00:21:44,032\n",
            "Pies are tiny.\n",
            "They're light, they're edible,\n",
            "\n",
            "725\n",
            "00:21:44,065 --> 00:21:46,111\n",
            "you could have a French\n",
            "pie and a\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "And I was sitting there,\n",
            "\n",
            "961\n",
            "00:50:17,957 --> 00:50:21,835\n",
            "cooking it, I was thirsty.\n",
            "\n",
            "962\n",
            "00:50:21,955 --> 00:50:25,916\n",
            "And I said,\n",
            "\"I'm gonna get the apple.\"\n",
            "\n",
            "963\n",
            "00:50:26,002 --> 00:50:27,939\n",
            "And his wife said,\n",
            "\"I'll get the plum.\"\n",
            "\n",
            "964\n",
            "00:50:27,972 --> 00:50:29,081\n",
            "[laughter]\n",
            "\n",
            "965\n",
            "00:50:29,897 --> 00:50:32,897\n",
            "That's a good shot.\n",
            "That's a good shot. [chuckles]\n",
            "\n",
            "966\n",
            "00:50:36,927 --> 00:50:39,091\n",
            "It's amazing being in a circle.\n",
            "You never hear,\n",
            "\n",
            "967\n",
            "00:50:39,094 --> 00:50:43,111\n",
            "\"Bye, friend.\"\n",
            "\"Bye.\"\n",
            "\n",
            "968\n",
            "00:50:43,145 --> 00:50:44,145\n",
            "I feel like the worst I can do\n",
            "of all the negative jokes\n",
            "\n",
            "969\n",
            "00:50:44,225 --> 00:50:46,225\n",
            "is to say\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "\n",
            "611\n",
            "00:13:50,844 --> 00:13:53,828\n",
            "I think pies are even worse\n",
            "than that.\n",
            "\n",
            "612\n",
            "00:13:53,851 --> 00:13:56,921\n",
            "I think they taste\n",
            "like shit\n",
            "\n",
            "613\n",
            "00:13:56,956 --> 00:13:59,942\n",
            "and then, when they\n",
            "open, they're like,\n",
            "\n",
            "614\n",
            "00:13:59,974 --> 00:14:02,960\n",
            "\"I'm mad at pies.\n",
            "\n",
            "615\n",
            "00:14:02,994 --> 00:14:05,995\n",
            "I'm sorry.\"\n",
            "\n",
            "616\n",
            "00:14:09,002 --> 00:14:13,614\n",
            "Pies are disgusting.\n",
            "\n",
            "617\n",
            "00:14:15,000 --> 00:14:18,330\n",
            "One time I used a pie\n",
            "to eat a stranded rabbit.\n",
            "\n",
            "618\n",
            "00:14:18,374 --> 00:14:21,374\n",
            "He just couldn't get\n",
            "the temperature right.\n",
            "\n",
            "619\n",
            "00:14:23,532 --> 00:14:27,527\n",
            "I put the rabbit in my mouth\n",
            "and it came back and I said,\n",
            "\n",
            "620\n",
            "00:14:28,000 --> 00:14:31,471\n",
            "\"Foiled?\"\n",
            "\n",
            "======================================== SAMPLE 4 ========================================\n",
            "\n",
            "\n",
            "847\n",
            "00:13:57,868 --> 00:13:59,621\n",
            "I just rolled it in my face\n",
            "\n",
            "848\n",
            "00:13:59,655 --> 00:14:05,921\n",
            "and it said \"for fuck's sake.\"\n",
            "\n",
            "849\n",
            "00:14:07,008 --> 00:14:09,923\n",
            "[laughter]\n",
            "\n",
            "850\n",
            "00:14:12,969 --> 00:14:15,969\n",
            "I've never had a great experience\n",
            "boarding with mirrors,\n",
            "\n",
            "851\n",
            "00:14:17,018 --> 00:14:19,015\n",
            "so I couldn't tell you how frustrating it is\n",
            "to have a blank mirror on your face.\n",
            "\n",
            "852\n",
            "00:14:20,016 --> 00:14:22,015\n",
            "You get the fucking mirror and it's like,\n",
            "\"Face down. Face down!\"\n",
            "\n",
            "853\n",
            "00:14:25,000 --> 00:14:27,726\n",
            "[laughter]\n",
            "\n",
            "854\n",
            "00:14:31,276 --> 00:14:34,061\n",
            "It's like, yeah, my face isn't good enough.\n",
            "I can have it. Okay?\n",
            "\n",
            "855\n",
            "00:14:34,927 --> 00:14:36,078\n",
            "I think having a blank face\n",
            "is the right move.\n",
            "======================================== SAMPLE 5 ========================================\n",
            "\n",
            "That was a goodie. [laughs]\n",
            "\n",
            "817\n",
            "00:30:50,534 --> 00:30:54,504\n",
            "I've never had a goodie\n",
            "for breakfast,\n",
            "\n",
            "818\n",
            "00:30:54,538 --> 00:30:56,839\n",
            "but... it was delicious.\n",
            "\n",
            "819\n",
            "00:30:56,873 --> 00:31:02,416\n",
            "It's the kind of thing\n",
            "that keeps you company.\n",
            "\n",
            "820\n",
            "00:31:02,450 --> 00:31:04,849\n",
            "I like to take walks.\n",
            "Like I did last season.\n",
            "\n",
            "821\n",
            "00:31:04,928 --> 00:31:06,933\n",
            "Yeah, I'm into that.\n",
            "\n",
            "822\n",
            "00:31:06,977 --> 00:31:07,933\n",
            "It's a great way to start a conversation.\n",
            "You know what I mean?\n",
            "\n",
            "823\n",
            "00:31:07,969 --> 00:31:09,971\n",
            "All right, I wanna get\n",
            "some soap and water here.\n",
            "\n",
            "824\n",
            "00:31:09,925 --> 00:31:11,971\n",
            "What do you think?\n",
            "\"I feel like I'm\n",
            "a goodie.\"\n",
            "\n",
            "825\n",
            "00:31:12,001 --> 00:31:\n",
            "======================================== SAMPLE 6 ========================================\n",
            " That's, until I went into the oven.\n",
            "\n",
            "817\n",
            "00:18:12,572 --> 00:18:14,735\n",
            "When it is cold outside,\n",
            "I put it in the microwave,\n",
            "\n",
            "818\n",
            "00:18:15,902 --> 00:18:18,936\n",
            "then I pull it out,\n",
            "little caramel dots fly by,\n",
            "\n",
            "819\n",
            "00:18:20,000 --> 00:18:21,935\n",
            "it just rolls around in my mouth.\n",
            "\n",
            "820\n",
            "00:18:23,934 --> 00:18:27,971\n",
            "When it is hot outside,\n",
            "I put it in the oven,\n",
            "\n",
            "821\n",
            "00:18:29,922 --> 00:18:30,939\n",
            "then I make dinner,\n",
            "\n",
            "822\n",
            "00:18:31,972 --> 00:18:34,957\n",
            "but I don't forget.\n",
            "It's okay.\n",
            "\n",
            "823\n",
            "00:18:39,903 --> 00:18:41,942\n",
            "I've tried to remember certain things,\n",
            "but it's not as if\n",
            "\n",
            "824\n",
            "00:18:41,975 --> 00:18:43,958\n",
            "I can remember how to read.\n",
            "I don't have an oral ability.\n",
            "\n",
            "825\n",
            "00:18:45,000 --> 00:\n",
            "======================================== SAMPLE 7 ========================================\n",
            " It hurt.\n",
            "\n",
            "856\n",
            "00:28:50,871 --> 00:28:53,539\n",
            "I think that's probably the best-\n",
            "\n",
            "857\n",
            "00:28:54,723 --> 00:28:56,821\n",
            "if any other fruit, isles are as good\n",
            "as the apples.\"\n",
            "\n",
            "858\n",
            "00:28:58,890 --> 00:29:01,947\n",
            "I'm sure there were times\n",
            "where you could watch a sports game and not\n",
            "\n",
            "859\n",
            "00:29:03,057 --> 00:29:05,737\n",
            "have a good laugh.\n",
            "\n",
            "860\n",
            "00:29:10,767 --> 00:29:13,824\n",
            "But that was totally okay.\n",
            "Thanks, guys.\n",
            "\n",
            "861\n",
            "00:29:14,945 --> 00:29:17,945\n",
            "Thanks for being my roommate for so long.\n",
            "\n",
            "862\n",
            "00:29:19,000 --> 00:29:21,999\n",
            "I'm really good.\n",
            "\n",
            "863\n",
            "00:29:22,847 --> 00:29:24,847\n",
            "I can relate.\n",
            "\n",
            "864\n",
            "00:29:30,299 --> 00:29:33,299\n",
            "I was at a party and somebody\n",
            "was waiting to take the guests.\n",
            "\n",
            "865\n",
            "00:29:33\n",
            "======================================== SAMPLE 8 ========================================\n",
            " A year later, I feel like I'm eating a whole plum.\n",
            "\n",
            "979\n",
            "00:20:56,879 --> 00:21:05,821\n",
            "What an interesting concept.\n",
            "If you eat something, you digest it and then,\n",
            "\n",
            "930\n",
            "00:21:05,855 --> 00:21:08,855\n",
            "then, uh...\n",
            "\n",
            "931\n",
            "00:21:13,091 --> 00:21:16,984\n",
            "...somebody else's special,\n",
            "like, one of those nutjobs.\n",
            "\n",
            "932\n",
            "00:21:16,992 --> 00:21:19,992\n",
            "[laughter]\n",
            "\n",
            "933\n",
            "00:21:23,917 --> 00:21:27,917\n",
            "I'm eating a plum.\n",
            "\n",
            "934\n",
            "00:21:31,927 --> 00:21:35,927\n",
            "It tastes remarkably like plum.\n",
            "It's remarkably smooth. I can't tell you how good it is.\n",
            "\n",
            "935\n",
            "00:21:35,978 --> 00:21:38,978\n",
            "That's good.\n",
            "\n",
            "936\n",
            "00:21:41,947 --> 00:21:44,947\n",
            "The skin on this thing is so fine.\n",
            "It feels fine.\n",
            "\n",
            "937\n",
            "00:21:44,989 --> 00:21:45,989\n",
            "The juice\n",
            "======================================== SAMPLE 9 ========================================\n",
            "\n",
            "\n",
            "1048\n",
            "00:16:06,521 --> 00:16:08,721\n",
            "And I'm like,\n",
            "\"That's delicious.\n",
            "\n",
            "1049\n",
            "00:16:08,805 --> 00:16:10,805\n",
            "I'm gonna eat it.\n",
            "Then I'm gonna hang out with my\n",
            "\n",
            "1050\n",
            "00:16:11,873 --> 00:16:13,873\n",
            "ass wife.\"\n",
            "\n",
            "1051\n",
            "00:16:18,890 --> 00:16:21,890\n",
            "I can't believe\n",
            "they came up with the term \"eat apple.\"\n",
            "\n",
            "1052\n",
            "00:16:23,057 --> 00:16:24,057\n",
            "I'm so mad.\n",
            "\n",
            "1053\n",
            "00:16:26,619 --> 00:16:28,619\n",
            "I love the word 'eat'.\n",
            "\n",
            "1054\n",
            "00:16:31,719 --> 00:16:34,719\n",
            "I'm obsessed with it.\n",
            "\n",
            "1055\n",
            "00:16:35,075 --> 00:16:37,110\n",
            "I'm gonna sit down to write a letter\n",
            "\n",
            "1056\n",
            "00:16:37,145 --> 00:16:39,145\n",
            "for my wife.\n",
            "\n",
            "1057\n",
            "00:16:40,075 --> 00:16:42,377\n",
            "She's totally\n",
            "======================================== SAMPLE 10 ========================================\n",
            " That's, until I looked up the word'apple'\n",
            "\n",
            "624\n",
            "00:17:28,947 --> 00:17:31,947\n",
            "'and it said 'apple-cake.'\n",
            "\n",
            "625\n",
            "00:17:35,032 --> 00:17:38,973\n",
            "I had to look that up. It said something along the middle\n",
            "and I looked it up.\n",
            "\n",
            "626\n",
            "00:17:42,032 --> 00:17:45,973\n",
            "'And I saw a friend of mine, a cake walker,\n",
            "and he was balding.\n",
            "\n",
            "627\n",
            "00:17:46,973 --> 00:17:50,973\n",
            "He was classic bakers, he was bald!\n",
            "So I looked him up, I found that word 'cake.'\n",
            "\n",
            "628\n",
            "00:17:51,099 --> 00:17:52,992\n",
            "I was like, 'You've got it.'\n",
            "\n",
            "629\n",
            "00:17:52,992 --> 00:17:56,992\n",
            "'Also, it's technically spelled 'cake.'\n",
            "\n",
            "630\n",
            "00:17:58,927 --> 00:18:01,927\n",
            "'Plus, it's an abracadabra.'\n",
            "\n",
            "631\n",
            "00:18:05,128 --> 00:18:09,128\n",
            "'Oh, my God.'\n",
            "\n",
            "632\n",
            "\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
            "    tf_session.TF_CloseSession(self._session)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}