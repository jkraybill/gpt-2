{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Fine Tuning Notebook",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkraybill/gpt-2/blob/finetuning/GPT2-finetuning2-345M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHiwt3Ww7FF6",
        "colab_type": "text"
      },
      "source": [
        "To try out GPT-2, do this:\n",
        "\n",
        "- go to the \"Runtime\" menu and click \"Change runtime type\" and make sure this is a Python 3 notebook, running with GPU hardware acceleration.\n",
        "- use the \"Files\" section to the left to upload a text file called \"corpus.txt\" which contains all the text you want to train on.\n",
        "- run the steps below in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE_fFgQ8a_Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etM-i8RwbcTH",
        "colab_type": "code",
        "outputId": "287c9f9a-a97e-492a-dde6-aa7d2b9c8647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "!git clone https://github.com/jkraybill/gpt-2.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects:  33% (1/3)   \u001b[K\rremote: Counting objects:  66% (2/3)   \u001b[K\rremote: Counting objects: 100% (3/3)   \u001b[K\rremote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects:  33% (1/3)   \u001b[K\rremote: Compressing objects:  66% (2/3)   \u001b[K\rremote: Compressing objects: 100% (3/3)   \u001b[K\rremote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "Receiving objects:   0% (1/208)   \rReceiving objects:   1% (3/208)   \rReceiving objects:   2% (5/208)   \rReceiving objects:   3% (7/208)   \rReceiving objects:   4% (9/208)   \rReceiving objects:   5% (11/208)   \rReceiving objects:   6% (13/208)   \rReceiving objects:   7% (15/208)   \rReceiving objects:   8% (17/208)   \rReceiving objects:   9% (19/208)   \rReceiving objects:  10% (21/208)   \rReceiving objects:  11% (23/208)   \rReceiving objects:  12% (25/208)   \rReceiving objects:  13% (28/208)   \rReceiving objects:  14% (30/208)   \rReceiving objects:  15% (32/208)   \rReceiving objects:  16% (34/208)   \rReceiving objects:  17% (36/208)   \rReceiving objects:  18% (38/208)   \rReceiving objects:  19% (40/208)   \rReceiving objects:  20% (42/208)   \rReceiving objects:  21% (44/208)   \rReceiving objects:  22% (46/208)   \rReceiving objects:  23% (48/208)   \rReceiving objects:  24% (50/208)   \rReceiving objects:  25% (52/208)   \rReceiving objects:  26% (55/208)   \rReceiving objects:  27% (57/208)   \rReceiving objects:  28% (59/208)   \rReceiving objects:  29% (61/208)   \rReceiving objects:  30% (63/208)   \rReceiving objects:  31% (65/208)   \rReceiving objects:  32% (67/208)   \rReceiving objects:  33% (69/208)   \rremote: Total 208 (delta 0), reused 1 (delta 0), pack-reused 205\u001b[K\n",
            "Receiving objects:  34% (71/208)   \rReceiving objects:  35% (73/208)   \rReceiving objects:  36% (75/208)   \rReceiving objects:  37% (77/208)   \rReceiving objects:  38% (80/208)   \rReceiving objects:  39% (82/208)   \rReceiving objects:  40% (84/208)   \rReceiving objects:  41% (86/208)   \rReceiving objects:  42% (88/208)   \rReceiving objects:  43% (90/208)   \rReceiving objects:  44% (92/208)   \rReceiving objects:  45% (94/208)   \rReceiving objects:  46% (96/208)   \rReceiving objects:  47% (98/208)   \rReceiving objects:  48% (100/208)   \rReceiving objects:  49% (102/208)   \rReceiving objects:  50% (104/208)   \rReceiving objects:  51% (107/208)   \rReceiving objects:  52% (109/208)   \rReceiving objects:  53% (111/208)   \rReceiving objects:  54% (113/208)   \rReceiving objects:  55% (115/208)   \rReceiving objects:  56% (117/208)   \rReceiving objects:  57% (119/208)   \rReceiving objects:  58% (121/208)   \rReceiving objects:  59% (123/208)   \rReceiving objects:  60% (125/208)   \rReceiving objects:  61% (127/208)   \rReceiving objects:  62% (129/208)   \rReceiving objects:  63% (132/208)   \rReceiving objects:  64% (134/208)   \rReceiving objects:  65% (136/208)   \rReceiving objects:  66% (138/208)   \rReceiving objects:  67% (140/208)   \rReceiving objects:  68% (142/208)   \rReceiving objects:  69% (144/208)   \rReceiving objects:  70% (146/208)   \rReceiving objects:  71% (148/208)   \rReceiving objects:  72% (150/208)   \rReceiving objects:  73% (152/208)   \rReceiving objects:  74% (154/208)   \rReceiving objects:  75% (156/208)   \rReceiving objects:  76% (159/208)   \rReceiving objects:  77% (161/208)   \rReceiving objects:  78% (163/208)   \rReceiving objects:  79% (165/208)   \rReceiving objects:  80% (167/208)   \rReceiving objects:  81% (169/208)   \rReceiving objects:  82% (171/208)   \rReceiving objects:  83% (173/208)   \rReceiving objects:  84% (175/208)   \rReceiving objects:  85% (177/208)   \rReceiving objects:  86% (179/208)   \rReceiving objects:  87% (181/208)   \rReceiving objects:  88% (184/208)   \rReceiving objects:  89% (186/208)   \rReceiving objects:  90% (188/208)   \rReceiving objects:  91% (190/208)   \rReceiving objects:  92% (192/208)   \rReceiving objects:  93% (194/208)   \rReceiving objects:  94% (196/208)   \rReceiving objects:  95% (198/208)   \rReceiving objects:  96% (200/208)   \rReceiving objects:  97% (202/208)   \rReceiving objects:  98% (204/208)   \rReceiving objects:  99% (206/208)   \rReceiving objects: 100% (208/208)   \rReceiving objects: 100% (208/208), 4.40 MiB | 14.58 MiB/s, done.\n",
            "Resolving deltas:   0% (0/111)   \rResolving deltas:   1% (2/111)   \rResolving deltas:   4% (5/111)   \rResolving deltas:   8% (9/111)   \rResolving deltas:  14% (16/111)   \rResolving deltas:  17% (19/111)   \rResolving deltas:  18% (20/111)   \rResolving deltas:  20% (23/111)   \rResolving deltas:  26% (29/111)   \rResolving deltas:  38% (43/111)   \rResolving deltas:  46% (52/111)   \rResolving deltas:  49% (55/111)   \rResolving deltas:  54% (60/111)   \rResolving deltas:  56% (63/111)   \rResolving deltas:  66% (74/111)   \rResolving deltas:  77% (86/111)   \rResolving deltas:  89% (99/111)   \rResolving deltas:  91% (102/111)   \rResolving deltas:  98% (109/111)   \rResolving deltas: 100% (111/111)   \rResolving deltas: 100% (111/111), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVTyGrhsbdep",
        "colab_type": "code",
        "outputId": "52e3105c-93de-473e-9d81-027af70275bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azu6KCOHbhIy",
        "colab_type": "code",
        "outputId": "465b14db-ec0d-4097-8563-a5c792d6c820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.0.18 has requirement regex==2018.01.10, but you'll have regex 2017.4.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex\n",
            "  Found existing installation: regex 2018.1.10\n",
            "    Uninstalling regex-2018.1.10:\n",
            "      Successfully uninstalled regex-2018.1.10\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecmuIWxFgFBz",
        "colab_type": "code",
        "outputId": "2d2ce857-8dcf-42f6-ab35-6e0764d36981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "source": [
        "!sh download_model.sh 345M"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching 345M/checkpoint\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    77  100    77    0     0   1375      0 --:--:-- --:--:-- --:--:--  1375\n",
            "Fetching 345M/encoder.json\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1017k  100 1017k    0     0  17.1M      0 --:--:-- --:--:-- --:--:-- 17.1M\n",
            "Fetching 345M/hparams.json\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    91  100    91    0     0   2527      0 --:--:-- --:--:-- --:--:--  2527\n",
            "Fetching 345M/model.ckpt.data-00000-of-00001\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1353M  100 1353M    0     0   124M      0  0:00:10  0:00:10 --:--:--  117M\n",
            "Fetching 345M/model.ckpt.index\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10399  100 10399    0     0   110k      0 --:--:-- --:--:-- --:--:--  110k\n",
            "Fetching 345M/model.ckpt.meta\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  904k  100  904k    0     0  18.0M      0 --:--:-- --:--:-- --:--:-- 18.0M\n",
            "Fetching 345M/vocab.bpe\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  445k  100  445k    0     0  10.3M      0 --:--:-- --:--:-- --:--:-- 10.3M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OhJ6ka_DD94",
        "colab_type": "text"
      },
      "source": [
        "The below step encodes your corpus into \"NPZ\" tokenized format for GPT-2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHsNeMNenxR",
        "colab_type": "code",
        "outputId": "ae6861fe-e129-4500-b2e4-7dc0c5d4a2c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "!PYTHONPATH=src ./encode.py --in-text ../corpus-j3-v-easy-train.txt --out-npz train.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py --in-text ../corpus-j3-v-easy-val.txt --out-npz val.txt.npz --model_name 345M"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "100% 1/1 [00:19<00:00, 19.22s/it]\n",
            "Writing train.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [00:07<00:00,  7.13s/it]\n",
            "Writing val.txt.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94oLzOq23474",
        "colab_type": "text"
      },
      "source": [
        "Training is below. I usually get usable results with \"stop_after\" anywhere from 800 to 3000, but you can try going even higher. 800 steps takes only a few minutes.\n",
        "\n",
        "\"sample_every\" controls how often you get sample output from the trained model.\n",
        "\n",
        "\"save_every\" controls how often the model is saved.\n",
        "\n",
        "\"learning_rate\" is the AI learning rate. 0.00005 is the rate I've gotten the best results with, but I think most people are running with significantly higher rates, so you could try adjusting it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbBJ6JoufBGQ",
        "colab_type": "code",
        "outputId": "593265aa-374f-4e30-bd65-645b4f1f1566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 24594
        }
      },
      "source": [
        "!PYTHONPATH=src ./trainval.py --dataset train.txt.npz --valset val.txt.npz --sample_every=1000 --save_every=1000 --learning_rate=0.00005 --stop_after=60000 --model_name=345M --batch_length=512"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-10 11:47:36.671426: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-10 11:47:36.671779: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2cb2260 executing computations on platform Host. Devices:\n",
            "2019-05-10 11:47:36.671834: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-10 11:47:36.930733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-10 11:47:36.931341: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2cb35a0 executing computations on platform CUDA. Devices:\n",
            "2019-05-10 11:47:36.931392: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-10 11:47:36.931806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-10 11:47:36.931826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-10 11:47:38.384833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-10 11:47:38.384897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-10 11:47:38.384914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-10 11:47:38.385314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "Reading train.txt.npz\n",
            "dataset has 3272086 tokens\n",
            "Training...\n",
            "Loading valset...\n",
            "Reading val.txt.npz\n",
            "valset has 879173 tokens\n",
            "Training...\n",
            "2019-05-10 11:48:30.679939: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "[1 | 9.87] loss=3.2570 avg=3.2570\n",
            "[2 | 10.38] loss=3.2323 avg=3.2446\n",
            "[3 | 10.92] loss=3.3315 avg=3.2738\n",
            "[4 | 11.44] loss=3.0467 avg=3.2162\n",
            "[5 | 11.97] loss=2.9494 avg=3.1618\n",
            "[5 | 13.74] VAL_loss=3.1349 VAL_avg=3.1349\n",
            "[6 | 14.26] loss=3.0492 avg=3.1425\n",
            "[7 | 14.80] loss=3.0775 avg=3.1330\n",
            "[8 | 15.32] loss=3.2476 avg=3.1478\n",
            "[9 | 15.86] loss=2.8248 avg=3.1104\n",
            "[10 | 16.38] loss=3.1644 avg=3.1161\n",
            "[10 | 16.53] VAL_loss=2.9983 VAL_avg=3.0662\n",
            "[11 | 17.07] loss=2.7412 avg=3.0803\n",
            "[12 | 17.60] loss=2.6736 avg=3.0445\n",
            "[13 | 18.13] loss=2.8869 avg=3.0316\n",
            "[14 | 18.65] loss=2.6791 avg=3.0048\n",
            "[15 | 19.19] loss=2.7312 avg=2.9852\n",
            "[15 | 19.34] VAL_loss=2.8559 VAL_avg=2.9954\n",
            "[16 | 19.87] loss=2.9591 avg=2.9834\n",
            "[17 | 20.40] loss=2.7670 avg=2.9697\n",
            "[18 | 20.93] loss=3.1257 avg=2.9791\n",
            "[19 | 21.47] loss=2.9775 avg=2.9790\n",
            "[20 | 22.00] loss=3.1518 avg=2.9885\n",
            "[20 | 22.14] VAL_loss=2.8028 VAL_avg=2.9465\n",
            "[21 | 22.69] loss=2.9349 avg=2.9857\n",
            "[22 | 23.22] loss=2.8515 avg=2.9789\n",
            "[23 | 23.76] loss=3.1071 avg=2.9851\n",
            "[24 | 24.29] loss=2.9844 avg=2.9851\n",
            "[25 | 24.83] loss=2.8257 avg=2.9779\n",
            "[25 | 24.97] VAL_loss=2.9994 VAL_avg=2.9573\n",
            "[26 | 25.52] loss=3.0269 avg=2.9800\n",
            "[27 | 26.06] loss=2.7120 avg=2.9688\n",
            "[28 | 26.59] loss=2.7025 avg=2.9579\n",
            "[29 | 27.13] loss=3.1105 avg=2.9639\n",
            "[30 | 27.67] loss=2.8541 avg=2.9597\n",
            "[30 | 27.82] VAL_loss=2.8836 VAL_avg=2.9447\n",
            "[31 | 28.36] loss=2.5797 avg=2.9455\n",
            "[32 | 28.90] loss=2.8445 avg=2.9419\n",
            "[33 | 29.44] loss=3.0811 avg=2.9468\n",
            "[34 | 29.98] loss=2.9395 avg=2.9465\n",
            "[35 | 30.52] loss=3.0158 avg=2.9489\n",
            "[35 | 30.67] VAL_loss=2.8609 VAL_avg=2.9324\n",
            "[36 | 31.21] loss=2.8831 avg=2.9467\n",
            "[37 | 31.75] loss=2.7500 avg=2.9404\n",
            "[38 | 32.28] loss=2.6995 avg=2.9328\n",
            "[39 | 32.82] loss=2.7722 avg=2.9278\n",
            "[40 | 33.36] loss=2.7193 avg=2.9215\n",
            "[40 | 33.51] VAL_loss=3.0193 VAL_avg=2.9436\n",
            "[41 | 34.06] loss=2.7640 avg=2.9169\n",
            "[42 | 34.60] loss=2.8101 avg=2.9138\n",
            "[43 | 35.14] loss=2.8655 avg=2.9124\n",
            "[44 | 35.68] loss=2.7055 avg=2.9066\n",
            "[45 | 36.22] loss=2.8508 avg=2.9051\n",
            "[45 | 36.37] VAL_loss=2.7958 VAL_avg=2.9265\n",
            "[46 | 36.92] loss=2.6714 avg=2.8988\n",
            "[47 | 37.46] loss=3.0271 avg=2.9022\n",
            "[48 | 38.00] loss=2.8249 avg=2.9001\n",
            "[49 | 38.54] loss=2.8857 avg=2.8998\n",
            "[50 | 39.08] loss=2.8225 avg=2.8978\n",
            "[50 | 39.23] VAL_loss=2.8105 VAL_avg=2.9144\n",
            "[51 | 39.78] loss=2.7917 avg=2.8952\n",
            "[52 | 40.32] loss=2.8637 avg=2.8944\n",
            "[53 | 40.87] loss=2.6931 avg=2.8895\n",
            "[54 | 41.41] loss=2.8018 avg=2.8874\n",
            "[55 | 41.96] loss=2.8449 avg=2.8864\n",
            "[55 | 42.11] VAL_loss=2.9322 VAL_avg=2.9161\n",
            "[56 | 42.66] loss=2.9413 avg=2.8877\n",
            "[57 | 43.21] loss=2.7855 avg=2.8854\n",
            "[58 | 43.75] loss=2.8847 avg=2.8853\n",
            "[59 | 44.29] loss=2.8614 avg=2.8848\n",
            "[60 | 44.85] loss=2.6369 avg=2.8793\n",
            "[60 | 45.00] VAL_loss=2.4688 VAL_avg=2.8767\n",
            "[61 | 45.54] loss=2.7253 avg=2.8760\n",
            "[62 | 46.09] loss=2.5595 avg=2.8692\n",
            "[63 | 46.64] loss=2.9500 avg=2.8709\n",
            "[64 | 47.18] loss=2.9798 avg=2.8732\n",
            "[65 | 47.74] loss=2.9862 avg=2.8755\n",
            "[65 | 47.89] VAL_loss=2.9911 VAL_avg=2.8861\n",
            "[66 | 48.44] loss=2.7645 avg=2.8732\n",
            "[67 | 48.99] loss=2.6765 avg=2.8692\n",
            "[68 | 49.54] loss=2.8590 avg=2.8690\n",
            "[69 | 50.09] loss=2.9213 avg=2.8701\n",
            "[70 | 50.63] loss=2.9070 avg=2.8708\n",
            "[70 | 50.79] VAL_loss=2.7483 VAL_avg=2.8756\n",
            "[71 | 51.35] loss=2.6862 avg=2.8672\n",
            "[72 | 51.90] loss=2.7416 avg=2.8647\n",
            "[73 | 52.45] loss=2.7447 avg=2.8624\n",
            "[74 | 53.01] loss=2.9861 avg=2.8648\n",
            "[75 | 53.56] loss=2.5443 avg=2.8587\n",
            "[75 | 53.72] VAL_loss=2.7452 VAL_avg=2.8662\n",
            "[76 | 54.27] loss=2.7840 avg=2.8573\n",
            "[77 | 54.83] loss=2.7972 avg=2.8562\n",
            "[78 | 55.38] loss=2.6704 avg=2.8528\n",
            "[79 | 55.93] loss=2.8466 avg=2.8527\n",
            "[80 | 56.49] loss=2.7924 avg=2.8516\n",
            "[80 | 56.64] VAL_loss=2.5409 VAL_avg=2.8443\n",
            "[81 | 57.19] loss=2.8241 avg=2.8511\n",
            "[82 | 57.75] loss=2.7474 avg=2.8493\n",
            "[83 | 58.30] loss=2.7163 avg=2.8469\n",
            "[84 | 58.86] loss=3.0744 avg=2.8509\n",
            "[85 | 59.42] loss=2.3547 avg=2.8423\n",
            "[85 | 59.57] VAL_loss=2.7352 VAL_avg=2.8374\n",
            "[86 | 60.13] loss=2.9208 avg=2.8436\n",
            "[87 | 60.69] loss=2.8693 avg=2.8441\n",
            "[88 | 61.25] loss=2.6361 avg=2.8405\n",
            "[89 | 61.81] loss=2.7369 avg=2.8388\n",
            "[90 | 62.37] loss=2.8146 avg=2.8384\n",
            "[90 | 62.52] VAL_loss=2.7392 VAL_avg=2.8315\n",
            "[91 | 63.09] loss=2.6892 avg=2.8359\n",
            "[92 | 63.65] loss=2.6047 avg=2.8320\n",
            "[93 | 64.21] loss=2.9137 avg=2.8334\n",
            "[94 | 64.77] loss=2.5467 avg=2.8287\n",
            "[95 | 65.33] loss=2.9468 avg=2.8306\n",
            "[95 | 65.49] VAL_loss=2.7272 VAL_avg=2.8255\n",
            "[96 | 66.06] loss=3.0974 avg=2.8349\n",
            "[97 | 66.62] loss=2.7643 avg=2.8338\n",
            "[98 | 67.18] loss=2.7297 avg=2.8321\n",
            "[99 | 67.75] loss=2.7090 avg=2.8302\n",
            "[100 | 68.31] loss=2.6017 avg=2.8266\n",
            "[100 | 68.47] VAL_loss=2.5052 VAL_avg=2.8079\n",
            "Saving checkpoint/run1/model-100\n",
            "[101 | 81.04] loss=2.8606 avg=2.8271\n",
            "[102 | 81.59] loss=2.6083 avg=2.8237\n",
            "[103 | 82.15] loss=2.9505 avg=2.8257\n",
            "[104 | 82.70] loss=2.9363 avg=2.8274\n",
            "[105 | 83.27] loss=2.8590 avg=2.8278\n",
            "[105 | 83.42] VAL_loss=2.5789 VAL_avg=2.7958\n",
            "[106 | 83.99] loss=2.9041 avg=2.8290\n",
            "[107 | 84.56] loss=2.7102 avg=2.8272\n",
            "[108 | 85.12] loss=2.8596 avg=2.8277\n",
            "[109 | 85.69] loss=2.6934 avg=2.8257\n",
            "[110 | 86.25] loss=2.4215 avg=2.8196\n",
            "[110 | 86.41] VAL_loss=2.6669 VAL_avg=2.7893\n",
            "[111 | 86.98] loss=2.6415 avg=2.8170\n",
            "[112 | 87.54] loss=2.6783 avg=2.8149\n",
            "[113 | 88.11] loss=2.6999 avg=2.8132\n",
            "[114 | 88.68] loss=2.5947 avg=2.8100\n",
            "[115 | 89.25] loss=3.1622 avg=2.8152\n",
            "[115 | 89.41] VAL_loss=2.8807 VAL_avg=2.7938\n",
            "[116 | 89.98] loss=2.9045 avg=2.8165\n",
            "[117 | 90.55] loss=2.5822 avg=2.8131\n",
            "[118 | 91.12] loss=2.6373 avg=2.8106\n",
            "[119 | 91.69] loss=2.9494 avg=2.8125\n",
            "[120 | 92.26] loss=2.6910 avg=2.8108\n",
            "[120 | 92.42] VAL_loss=3.0660 VAL_avg=2.8065\n",
            "[121 | 92.99] loss=2.8525 avg=2.8114\n",
            "[122 | 93.55] loss=2.8048 avg=2.8113\n",
            "[123 | 94.12] loss=2.6810 avg=2.8095\n",
            "[124 | 94.69] loss=2.6488 avg=2.8072\n",
            "[125 | 95.26] loss=2.8243 avg=2.8075\n",
            "[125 | 95.42] VAL_loss=2.8417 VAL_avg=2.8080\n",
            "[126 | 96.00] loss=2.8481 avg=2.8080\n",
            "[127 | 96.57] loss=2.6701 avg=2.8061\n",
            "[128 | 97.14] loss=2.6014 avg=2.8033\n",
            "[129 | 97.72] loss=2.5928 avg=2.8004\n",
            "[130 | 98.29] loss=2.8981 avg=2.8017\n",
            "[130 | 98.45] VAL_loss=2.6495 VAL_avg=2.8012\n",
            "[131 | 99.02] loss=2.8193 avg=2.8020\n",
            "[132 | 99.60] loss=2.7838 avg=2.8017\n",
            "[133 | 100.18] loss=2.5610 avg=2.7984\n",
            "[134 | 100.76] loss=2.8682 avg=2.7994\n",
            "[135 | 101.34] loss=2.8114 avg=2.7996\n",
            "[135 | 101.50] VAL_loss=2.5268 VAL_avg=2.7896\n",
            "[136 | 102.07] loss=2.7085 avg=2.7983\n",
            "[137 | 102.65] loss=2.9095 avg=2.7998\n",
            "[138 | 103.23] loss=2.6455 avg=2.7978\n",
            "[139 | 103.81] loss=2.8671 avg=2.7987\n",
            "[140 | 104.40] loss=2.4517 avg=2.7941\n",
            "[140 | 104.56] VAL_loss=2.6435 VAL_avg=2.7837\n",
            "[141 | 105.15] loss=2.6960 avg=2.7928\n",
            "[142 | 105.73] loss=2.8041 avg=2.7929\n",
            "[143 | 106.32] loss=2.7337 avg=2.7922\n",
            "[144 | 106.90] loss=2.6088 avg=2.7898\n",
            "[145 | 107.48] loss=2.5948 avg=2.7872\n",
            "[145 | 107.64] VAL_loss=2.6458 VAL_avg=2.7782\n",
            "[146 | 108.23] loss=2.5529 avg=2.7842\n",
            "[147 | 108.82] loss=2.7420 avg=2.7836\n",
            "[148 | 109.41] loss=2.7518 avg=2.7832\n",
            "[149 | 110.00] loss=2.8034 avg=2.7835\n",
            "[150 | 110.58] loss=2.4393 avg=2.7791\n",
            "[150 | 110.74] VAL_loss=2.8111 VAL_avg=2.7795\n",
            "[151 | 111.33] loss=2.9264 avg=2.7809\n",
            "[152 | 111.92] loss=2.6599 avg=2.7794\n",
            "[153 | 112.51] loss=2.7863 avg=2.7795\n",
            "[154 | 113.10] loss=2.6280 avg=2.7776\n",
            "[155 | 113.69] loss=2.7771 avg=2.7776\n",
            "[155 | 113.85] VAL_loss=2.4821 VAL_avg=2.7684\n",
            "[156 | 114.45] loss=2.6086 avg=2.7754\n",
            "[157 | 115.04] loss=2.7644 avg=2.7753\n",
            "[158 | 115.63] loss=2.4732 avg=2.7715\n",
            "[159 | 116.23] loss=2.8191 avg=2.7721\n",
            "[160 | 116.82] loss=2.7879 avg=2.7723\n",
            "[160 | 116.99] VAL_loss=2.7589 VAL_avg=2.7680\n",
            "[161 | 117.58] loss=2.4134 avg=2.7678\n",
            "[162 | 118.19] loss=2.6886 avg=2.7668\n",
            "[163 | 118.78] loss=2.7006 avg=2.7660\n",
            "[164 | 119.37] loss=2.6556 avg=2.7646\n",
            "[165 | 119.97] loss=2.9022 avg=2.7663\n",
            "[165 | 120.14] VAL_loss=2.7643 VAL_avg=2.7679\n",
            "[166 | 120.74] loss=2.9250 avg=2.7683\n",
            "[167 | 121.34] loss=2.7207 avg=2.7677\n",
            "[168 | 121.94] loss=2.7415 avg=2.7674\n",
            "[169 | 122.53] loss=2.5221 avg=2.7644\n",
            "[170 | 123.13] loss=2.7709 avg=2.7645\n",
            "[170 | 123.30] VAL_loss=2.6091 VAL_avg=2.7624\n",
            "[171 | 123.91] loss=2.8737 avg=2.7658\n",
            "[172 | 124.51] loss=2.7129 avg=2.7651\n",
            "[173 | 125.10] loss=2.6126 avg=2.7633\n",
            "[174 | 125.71] loss=2.2851 avg=2.7575\n",
            "[175 | 126.31] loss=2.7605 avg=2.7575\n",
            "[175 | 126.47] VAL_loss=2.6950 VAL_avg=2.7601\n",
            "[176 | 127.07] loss=2.6640 avg=2.7564\n",
            "[177 | 127.68] loss=2.7687 avg=2.7566\n",
            "[178 | 128.28] loss=2.7716 avg=2.7567\n",
            "[179 | 128.87] loss=2.4652 avg=2.7532\n",
            "[180 | 129.48] loss=2.7831 avg=2.7536\n",
            "[180 | 129.64] VAL_loss=2.7338 VAL_avg=2.7593\n",
            "[181 | 130.25] loss=2.6614 avg=2.7525\n",
            "[182 | 130.85] loss=2.7171 avg=2.7521\n",
            "[183 | 131.45] loss=2.4323 avg=2.7483\n",
            "[184 | 132.05] loss=2.7573 avg=2.7484\n",
            "[185 | 132.65] loss=2.7068 avg=2.7479\n",
            "[185 | 132.82] VAL_loss=2.9366 VAL_avg=2.7650\n",
            "[186 | 133.42] loss=2.9542 avg=2.7503\n",
            "[187 | 134.02] loss=2.8028 avg=2.7510\n",
            "[188 | 134.62] loss=2.9578 avg=2.7534\n",
            "[189 | 135.23] loss=2.5862 avg=2.7514\n",
            "[190 | 135.83] loss=2.4811 avg=2.7483\n",
            "[190 | 135.99] VAL_loss=2.4402 VAL_avg=2.7547\n",
            "[191 | 136.59] loss=2.5412 avg=2.7458\n",
            "[192 | 137.19] loss=2.5405 avg=2.7434\n",
            "[193 | 137.78] loss=2.9479 avg=2.7458\n",
            "[194 | 138.38] loss=2.7442 avg=2.7458\n",
            "[195 | 138.98] loss=2.7608 avg=2.7460\n",
            "[195 | 139.14] VAL_loss=2.7076 VAL_avg=2.7533\n",
            "[196 | 139.73] loss=2.6978 avg=2.7454\n",
            "[197 | 140.33] loss=2.7167 avg=2.7451\n",
            "[198 | 140.92] loss=2.6129 avg=2.7435\n",
            "[199 | 141.52] loss=2.6458 avg=2.7424\n",
            "[200 | 142.11] loss=2.5942 avg=2.7407\n",
            "[200 | 142.27] VAL_loss=2.7247 VAL_avg=2.7524\n",
            "Saving checkpoint/run1/model-200\n",
            "[201 | 154.31] loss=2.8069 avg=2.7415\n",
            "[202 | 154.88] loss=2.7681 avg=2.7418\n",
            "[203 | 155.44] loss=2.7403 avg=2.7418\n",
            "[204 | 156.02] loss=2.6297 avg=2.7405\n",
            "[205 | 156.59] loss=2.6923 avg=2.7399\n",
            "[205 | 156.75] VAL_loss=2.7133 VAL_avg=2.7513\n",
            "[206 | 157.32] loss=2.9804 avg=2.7427\n",
            "[207 | 157.89] loss=2.6543 avg=2.7417\n",
            "[208 | 158.46] loss=2.8050 avg=2.7424\n",
            "[209 | 159.04] loss=2.6173 avg=2.7410\n",
            "[210 | 159.61] loss=2.7806 avg=2.7414\n",
            "[210 | 159.77] VAL_loss=2.6333 VAL_avg=2.7478\n",
            "[211 | 160.34] loss=2.6721 avg=2.7406\n",
            "[212 | 160.91] loss=2.8280 avg=2.7416\n",
            "[213 | 161.49] loss=2.7138 avg=2.7413\n",
            "[214 | 162.07] loss=2.6949 avg=2.7408\n",
            "[215 | 162.65] loss=2.6842 avg=2.7401\n",
            "[215 | 162.81] VAL_loss=2.6165 VAL_avg=2.7441\n",
            "[216 | 163.39] loss=2.5894 avg=2.7384\n",
            "[217 | 163.97] loss=2.6001 avg=2.7369\n",
            "[218 | 164.55] loss=2.6110 avg=2.7355\n",
            "[219 | 165.13] loss=2.7699 avg=2.7358\n",
            "[220 | 165.72] loss=2.5498 avg=2.7338\n",
            "[220 | 165.88] VAL_loss=2.6870 VAL_avg=2.7425\n",
            "[221 | 166.46] loss=2.5972 avg=2.7322\n",
            "[222 | 167.05] loss=2.7827 avg=2.7328\n",
            "[223 | 167.64] loss=2.6202 avg=2.7315\n",
            "[224 | 168.22] loss=2.2607 avg=2.7263\n",
            "[225 | 168.80] loss=2.6022 avg=2.7249\n",
            "[225 | 168.97] VAL_loss=2.9595 VAL_avg=2.7485\n",
            "[226 | 169.56] loss=2.9476 avg=2.7274\n",
            "[227 | 170.15] loss=2.6408 avg=2.7264\n",
            "[228 | 170.74] loss=2.6150 avg=2.7252\n",
            "[229 | 171.33] loss=2.3992 avg=2.7215\n",
            "[230 | 171.93] loss=2.8077 avg=2.7225\n",
            "[230 | 172.09] VAL_loss=2.7560 VAL_avg=2.7487\n",
            "[231 | 172.68] loss=2.7098 avg=2.7224\n",
            "[232 | 173.28] loss=2.8317 avg=2.7236\n",
            "[233 | 173.88] loss=2.9497 avg=2.7261\n",
            "[234 | 174.47] loss=2.0418 avg=2.7185\n",
            "[235 | 175.07] loss=2.5884 avg=2.7171\n",
            "[235 | 175.24] VAL_loss=2.8651 VAL_avg=2.7518\n",
            "[236 | 175.83] loss=2.6814 avg=2.7167\n",
            "[237 | 176.43] loss=2.7241 avg=2.7168\n",
            "[238 | 177.03] loss=2.7112 avg=2.7167\n",
            "[239 | 177.63] loss=2.5959 avg=2.7154\n",
            "[240 | 178.23] loss=2.7338 avg=2.7156\n",
            "[240 | 178.40] VAL_loss=2.4687 VAL_avg=2.7444\n",
            "[241 | 179.01] loss=2.5178 avg=2.7134\n",
            "[242 | 179.61] loss=2.7630 avg=2.7139\n",
            "[243 | 180.22] loss=2.4966 avg=2.7116\n",
            "[244 | 180.82] loss=2.4236 avg=2.7084\n",
            "[245 | 181.43] loss=2.6205 avg=2.7074\n",
            "[245 | 181.60] VAL_loss=2.6817 VAL_avg=2.7428\n",
            "[246 | 182.21] loss=2.6935 avg=2.7073\n",
            "[247 | 182.81] loss=2.7581 avg=2.7078\n",
            "[248 | 183.42] loss=2.5911 avg=2.7066\n",
            "[249 | 184.02] loss=2.7696 avg=2.7073\n",
            "[250 | 184.63] loss=2.5588 avg=2.7056\n",
            "[250 | 184.80] VAL_loss=2.7936 VAL_avg=2.7440\n",
            "[251 | 185.41] loss=2.6582 avg=2.7051\n",
            "[252 | 186.00] loss=2.8820 avg=2.7071\n",
            "[253 | 186.61] loss=2.5183 avg=2.7050\n",
            "[254 | 187.21] loss=2.4489 avg=2.7022\n",
            "[255 | 187.81] loss=2.7399 avg=2.7026\n",
            "[255 | 187.98] VAL_loss=2.6148 VAL_avg=2.7408\n",
            "[256 | 188.59] loss=2.6546 avg=2.7021\n",
            "[257 | 189.18] loss=2.7947 avg=2.7031\n",
            "[258 | 189.79] loss=2.6272 avg=2.7023\n",
            "[259 | 190.38] loss=2.5681 avg=2.7008\n",
            "[260 | 190.98] loss=2.6821 avg=2.7006\n",
            "[260 | 191.14] VAL_loss=2.5509 VAL_avg=2.7362\n",
            "[261 | 191.74] loss=2.7497 avg=2.7012\n",
            "[262 | 192.34] loss=2.6399 avg=2.7005\n",
            "[263 | 192.93] loss=2.4470 avg=2.6978\n",
            "[264 | 193.53] loss=2.6451 avg=2.6972\n",
            "[265 | 194.13] loss=2.5327 avg=2.6954\n",
            "[265 | 194.30] VAL_loss=2.6848 VAL_avg=2.7349\n",
            "[266 | 194.89] loss=2.7218 avg=2.6957\n",
            "[267 | 195.49] loss=2.7137 avg=2.6959\n",
            "[268 | 196.09] loss=2.6273 avg=2.6952\n",
            "[269 | 196.68] loss=2.5387 avg=2.6935\n",
            "[270 | 197.28] loss=2.7206 avg=2.6938\n",
            "[270 | 197.45] VAL_loss=2.7152 VAL_avg=2.7344\n",
            "[271 | 198.04] loss=2.7516 avg=2.6944\n",
            "[272 | 198.64] loss=2.9414 avg=2.6971\n",
            "[273 | 199.24] loss=2.7710 avg=2.6979\n",
            "[274 | 199.83] loss=2.6875 avg=2.6977\n",
            "[275 | 200.42] loss=2.6155 avg=2.6969\n",
            "[275 | 200.58] VAL_loss=2.4581 VAL_avg=2.7279\n",
            "[276 | 201.19] loss=2.6315 avg=2.6962\n",
            "[277 | 201.78] loss=2.6701 avg=2.6959\n",
            "[278 | 202.37] loss=2.5977 avg=2.6948\n",
            "[279 | 202.96] loss=2.6959 avg=2.6949\n",
            "[280 | 203.55] loss=2.6436 avg=2.6943\n",
            "[280 | 203.72] VAL_loss=2.4355 VAL_avg=2.7211\n",
            "[281 | 204.31] loss=2.9017 avg=2.6965\n",
            "[282 | 204.90] loss=2.7583 avg=2.6972\n",
            "[283 | 205.49] loss=2.5955 avg=2.6961\n",
            "[284 | 206.08] loss=2.6340 avg=2.6954\n",
            "[285 | 206.67] loss=2.8135 avg=2.6967\n",
            "[285 | 206.84] VAL_loss=2.8999 VAL_avg=2.7252\n",
            "[286 | 207.43] loss=2.6813 avg=2.6965\n",
            "[287 | 208.02] loss=2.5584 avg=2.6951\n",
            "[288 | 208.61] loss=2.6851 avg=2.6950\n",
            "[289 | 209.20] loss=2.2066 avg=2.6898\n",
            "[290 | 209.78] loss=2.5158 avg=2.6879\n",
            "[290 | 209.95] VAL_loss=2.7455 VAL_avg=2.7257\n",
            "[291 | 210.54] loss=2.5189 avg=2.6862\n",
            "[292 | 211.13] loss=2.7718 avg=2.6871\n",
            "[293 | 211.72] loss=2.8215 avg=2.6885\n",
            "[294 | 212.30] loss=2.3559 avg=2.6850\n",
            "[295 | 212.89] loss=2.6505 avg=2.6846\n",
            "[295 | 213.05] VAL_loss=2.7572 VAL_avg=2.7264\n",
            "[296 | 213.65] loss=2.6367 avg=2.6841\n",
            "[297 | 214.23] loss=2.7582 avg=2.6849\n",
            "[298 | 214.82] loss=2.5606 avg=2.6836\n",
            "[299 | 215.41] loss=2.7448 avg=2.6842\n",
            "[300 | 215.99] loss=2.9605 avg=2.6871\n",
            "[300 | 216.15] VAL_loss=2.7165 VAL_avg=2.7262\n",
            "Saving checkpoint/run1/model-300\n",
            "[301 | 227.47] loss=2.6411 avg=2.6866\n",
            "[302 | 228.04] loss=2.7425 avg=2.6872\n",
            "[303 | 228.60] loss=2.7277 avg=2.6877\n",
            "[304 | 229.18] loss=2.8877 avg=2.6898\n",
            "[305 | 229.75] loss=2.8247 avg=2.6912\n",
            "[305 | 229.90] VAL_loss=2.7036 VAL_avg=2.7257\n",
            "[306 | 230.48] loss=2.4764 avg=2.6889\n",
            "[307 | 231.05] loss=2.7765 avg=2.6898\n",
            "[308 | 231.62] loss=2.6881 avg=2.6898\n",
            "[309 | 232.19] loss=2.4408 avg=2.6872\n",
            "[310 | 232.77] loss=2.7233 avg=2.6876\n",
            "[310 | 232.93] VAL_loss=2.5738 VAL_avg=2.7224\n",
            "[311 | 233.50] loss=2.5749 avg=2.6864\n",
            "[312 | 234.08] loss=2.7607 avg=2.6872\n",
            "[313 | 234.65] loss=2.4934 avg=2.6852\n",
            "[314 | 235.22] loss=2.7096 avg=2.6854\n",
            "[315 | 235.80] loss=2.4361 avg=2.6828\n",
            "[315 | 235.96] VAL_loss=2.3076 VAL_avg=2.7136\n",
            "[316 | 236.54] loss=2.7556 avg=2.6836\n",
            "[317 | 237.12] loss=2.6869 avg=2.6836\n",
            "[318 | 237.68] loss=2.2209 avg=2.6788\n",
            "[319 | 238.27] loss=2.4473 avg=2.6764\n",
            "[320 | 238.85] loss=2.4584 avg=2.6741\n",
            "[320 | 239.01] VAL_loss=2.9236 VAL_avg=2.7180\n",
            "[321 | 239.59] loss=2.5159 avg=2.6725\n",
            "[322 | 240.18] loss=2.6001 avg=2.6717\n",
            "[323 | 240.76] loss=2.5693 avg=2.6706\n",
            "[324 | 241.34] loss=2.6658 avg=2.6706\n",
            "[325 | 241.93] loss=2.6421 avg=2.6703\n",
            "[325 | 242.09] VAL_loss=2.5485 VAL_avg=2.7145\n",
            "[326 | 242.68] loss=2.7037 avg=2.6706\n",
            "[327 | 243.27] loss=2.7358 avg=2.6713\n",
            "[328 | 243.86] loss=2.7197 avg=2.6718\n",
            "[329 | 244.45] loss=2.2777 avg=2.6677\n",
            "[330 | 245.05] loss=2.9871 avg=2.6710\n",
            "[330 | 245.22] VAL_loss=2.4197 VAL_avg=2.7084\n",
            "[331 | 245.81] loss=2.5756 avg=2.6700\n",
            "[332 | 246.40] loss=2.4909 avg=2.6682\n",
            "[333 | 246.99] loss=2.7776 avg=2.6693\n",
            "[334 | 247.59] loss=2.6104 avg=2.6687\n",
            "[335 | 248.19] loss=2.8121 avg=2.6702\n",
            "[335 | 248.36] VAL_loss=2.8429 VAL_avg=2.7111\n",
            "[336 | 248.96] loss=2.7018 avg=2.6705\n",
            "[337 | 249.56] loss=2.6174 avg=2.6700\n",
            "[338 | 250.16] loss=2.7718 avg=2.6710\n",
            "[339 | 250.77] loss=2.9732 avg=2.6742\n",
            "[340 | 251.37] loss=2.7230 avg=2.6747\n",
            "[340 | 251.54] VAL_loss=2.4818 VAL_avg=2.7065\n",
            "[341 | 252.15] loss=2.4461 avg=2.6723\n",
            "[342 | 252.75] loss=2.8005 avg=2.6736\n",
            "[343 | 253.36] loss=2.5232 avg=2.6721\n",
            "[344 | 253.97] loss=2.6020 avg=2.6713\n",
            "[345 | 254.58] loss=2.6233 avg=2.6709\n",
            "[345 | 254.75] VAL_loss=2.9018 VAL_avg=2.7104\n",
            "[346 | 255.37] loss=2.5313 avg=2.6694\n",
            "[347 | 255.98] loss=2.2183 avg=2.6648\n",
            "[348 | 256.60] loss=2.6730 avg=2.6648\n",
            "[349 | 257.21] loss=2.4247 avg=2.6624\n",
            "[350 | 257.83] loss=2.6238 avg=2.6620\n",
            "[350 | 258.00] VAL_loss=2.4491 VAL_avg=2.7052\n",
            "[351 | 258.62] loss=2.5538 avg=2.6609\n",
            "[352 | 259.23] loss=2.7680 avg=2.6620\n",
            "[353 | 259.85] loss=2.6794 avg=2.6621\n",
            "[354 | 260.46] loss=2.6933 avg=2.6625\n",
            "[355 | 261.08] loss=2.4379 avg=2.6601\n",
            "[355 | 261.25] VAL_loss=2.6076 VAL_avg=2.7033\n",
            "[356 | 261.87] loss=2.6464 avg=2.6600\n",
            "[357 | 262.48] loss=2.4747 avg=2.6581\n",
            "[358 | 263.09] loss=2.8142 avg=2.6597\n",
            "[359 | 263.71] loss=2.7415 avg=2.6605\n",
            "[360 | 264.32] loss=2.5529 avg=2.6594\n",
            "[360 | 264.49] VAL_loss=2.5608 VAL_avg=2.7005\n",
            "[361 | 265.11] loss=2.5176 avg=2.6580\n",
            "[362 | 265.72] loss=2.7037 avg=2.6585\n",
            "[363 | 266.32] loss=2.8179 avg=2.6601\n",
            "[364 | 266.93] loss=2.5888 avg=2.6594\n",
            "[365 | 267.53] loss=2.3217 avg=2.6559\n",
            "[365 | 267.70] VAL_loss=2.8512 VAL_avg=2.7034\n",
            "[366 | 268.31] loss=2.8229 avg=2.6576\n",
            "[367 | 268.91] loss=2.5473 avg=2.6565\n",
            "[368 | 269.51] loss=2.6605 avg=2.6565\n",
            "[369 | 270.11] loss=2.4640 avg=2.6545\n",
            "[370 | 270.71] loss=2.7653 avg=2.6557\n",
            "[370 | 270.88] VAL_loss=2.5922 VAL_avg=2.7013\n",
            "[371 | 271.49] loss=2.5328 avg=2.6544\n",
            "[372 | 272.09] loss=2.6408 avg=2.6543\n",
            "[373 | 272.69] loss=2.4722 avg=2.6524\n",
            "[374 | 273.29] loss=2.5194 avg=2.6511\n",
            "[375 | 273.88] loss=2.4096 avg=2.6486\n",
            "[375 | 274.05] VAL_loss=2.4950 VAL_avg=2.6974\n",
            "[376 | 274.65] loss=2.3642 avg=2.6457\n",
            "[377 | 275.25] loss=2.4734 avg=2.6439\n",
            "[378 | 275.84] loss=2.5651 avg=2.6431\n",
            "[379 | 276.44] loss=2.5727 avg=2.6424\n",
            "[380 | 277.03] loss=2.6270 avg=2.6422\n",
            "[380 | 277.19] VAL_loss=2.4692 VAL_avg=2.6932\n",
            "[381 | 277.79] loss=2.4959 avg=2.6407\n",
            "[382 | 278.39] loss=2.5671 avg=2.6400\n",
            "[383 | 278.98] loss=2.5792 avg=2.6394\n",
            "[384 | 279.57] loss=2.4984 avg=2.6379\n",
            "[385 | 280.16] loss=2.8244 avg=2.6398\n",
            "[385 | 280.32] VAL_loss=2.6784 VAL_avg=2.6929\n",
            "[386 | 280.92] loss=2.5455 avg=2.6389\n",
            "[387 | 281.52] loss=2.4270 avg=2.6367\n",
            "[388 | 282.11] loss=2.5631 avg=2.6359\n",
            "[389 | 282.70] loss=2.5781 avg=2.6353\n",
            "[390 | 283.29] loss=2.9049 avg=2.6381\n",
            "[390 | 283.45] VAL_loss=2.8680 VAL_avg=2.6961\n",
            "[391 | 284.04] loss=2.6509 avg=2.6382\n",
            "[392 | 284.63] loss=2.5255 avg=2.6371\n",
            "[393 | 285.22] loss=2.5877 avg=2.6366\n",
            "[394 | 285.81] loss=2.6706 avg=2.6369\n",
            "[395 | 286.39] loss=2.5152 avg=2.6357\n",
            "[395 | 286.55] VAL_loss=2.6403 VAL_avg=2.6951\n",
            "[396 | 287.14] loss=2.6971 avg=2.6363\n",
            "[397 | 287.73] loss=2.3060 avg=2.6329\n",
            "[398 | 288.31] loss=2.4229 avg=2.6308\n",
            "[399 | 288.90] loss=2.8302 avg=2.6328\n",
            "[400 | 289.48] loss=2.7228 avg=2.6337\n",
            "[400 | 289.64] VAL_loss=2.6104 VAL_avg=2.6936\n",
            "Saving checkpoint/run1/model-400\n",
            "[401 | 301.01] loss=2.3373 avg=2.6307\n",
            "[402 | 301.56] loss=2.6881 avg=2.6313\n",
            "[403 | 302.13] loss=2.8040 avg=2.6331\n",
            "[404 | 302.70] loss=2.3798 avg=2.6305\n",
            "[405 | 303.26] loss=2.8435 avg=2.6327\n",
            "[405 | 303.42] VAL_loss=2.6482 VAL_avg=2.6927\n",
            "[406 | 304.00] loss=2.5174 avg=2.6315\n",
            "[407 | 304.56] loss=2.7165 avg=2.6324\n",
            "[408 | 305.13] loss=2.9544 avg=2.6356\n",
            "[409 | 305.70] loss=2.7756 avg=2.6371\n",
            "[410 | 306.27] loss=2.3111 avg=2.6337\n",
            "[410 | 306.43] VAL_loss=2.4150 VAL_avg=2.6878\n",
            "[411 | 307.00] loss=2.6234 avg=2.6336\n",
            "[412 | 307.58] loss=2.4672 avg=2.6319\n",
            "[413 | 308.15] loss=2.5686 avg=2.6313\n",
            "[414 | 308.72] loss=2.7537 avg=2.6325\n",
            "[415 | 309.29] loss=2.5287 avg=2.6315\n",
            "[415 | 309.45] VAL_loss=2.6230 VAL_avg=2.6866\n",
            "[416 | 310.03] loss=2.6826 avg=2.6320\n",
            "[417 | 310.60] loss=2.3979 avg=2.6296\n",
            "[418 | 311.17] loss=2.4666 avg=2.6280\n",
            "[419 | 311.75] loss=2.4438 avg=2.6261\n",
            "[420 | 312.33] loss=2.6396 avg=2.6262\n",
            "[420 | 312.49] VAL_loss=2.7524 VAL_avg=2.6878\n",
            "[421 | 313.07] loss=2.5773 avg=2.6257\n",
            "[422 | 313.66] loss=2.6402 avg=2.6259\n",
            "[423 | 314.24] loss=2.6950 avg=2.6266\n",
            "[424 | 314.83] loss=2.6984 avg=2.6273\n",
            "[425 | 315.41] loss=2.8612 avg=2.6297\n",
            "[425 | 315.57] VAL_loss=2.6063 VAL_avg=2.6864\n",
            "[426 | 316.16] loss=2.7264 avg=2.6307\n",
            "[427 | 316.74] loss=2.7235 avg=2.6316\n",
            "[428 | 317.33] loss=2.6614 avg=2.6319\n",
            "[429 | 317.92] loss=2.6003 avg=2.6316\n",
            "[430 | 318.51] loss=2.3835 avg=2.6291\n",
            "[430 | 318.68] VAL_loss=2.6798 VAL_avg=2.6863\n",
            "[431 | 319.28] loss=2.4733 avg=2.6275\n",
            "[432 | 319.87] loss=2.7151 avg=2.6284\n",
            "[433 | 320.46] loss=2.4123 avg=2.6262\n",
            "[434 | 321.05] loss=2.6910 avg=2.6269\n",
            "[435 | 321.65] loss=2.3827 avg=2.6244\n",
            "[435 | 321.81] VAL_loss=2.8836 VAL_avg=2.6896\n",
            "[436 | 322.41] loss=2.2346 avg=2.6204\n",
            "[437 | 323.01] loss=2.7357 avg=2.6216\n",
            "[438 | 323.61] loss=2.3881 avg=2.6192\n",
            "[439 | 324.21] loss=2.4137 avg=2.6172\n",
            "[440 | 324.81] loss=2.4537 avg=2.6155\n",
            "[440 | 324.98] VAL_loss=2.5624 VAL_avg=2.6875\n",
            "[441 | 325.59] loss=2.4571 avg=2.6139\n",
            "[442 | 326.20] loss=2.8100 avg=2.6159\n",
            "[443 | 326.80] loss=2.7132 avg=2.6169\n",
            "[444 | 327.41] loss=2.7241 avg=2.6180\n",
            "[445 | 328.02] loss=2.6596 avg=2.6184\n",
            "[445 | 328.19] VAL_loss=2.5833 VAL_avg=2.6857\n",
            "[446 | 328.80] loss=2.5787 avg=2.6180\n",
            "[447 | 329.41] loss=2.5480 avg=2.6173\n",
            "[448 | 330.03] loss=2.7650 avg=2.6188\n",
            "[449 | 330.64] loss=2.4639 avg=2.6172\n",
            "[450 | 331.25] loss=1.9564 avg=2.6105\n",
            "[450 | 331.42] VAL_loss=2.7024 VAL_avg=2.6860\n",
            "[451 | 332.05] loss=2.6923 avg=2.6113\n",
            "[452 | 332.66] loss=2.6768 avg=2.6120\n",
            "[453 | 333.28] loss=2.4924 avg=2.6108\n",
            "[454 | 333.90] loss=2.5466 avg=2.6101\n",
            "[455 | 334.52] loss=2.6838 avg=2.6109\n",
            "[455 | 334.69] VAL_loss=2.5475 VAL_avg=2.6837\n",
            "[456 | 335.31] loss=2.8000 avg=2.6128\n",
            "[457 | 335.93] loss=2.5587 avg=2.6123\n",
            "[458 | 336.55] loss=2.5385 avg=2.6115\n",
            "[459 | 337.17] loss=2.6635 avg=2.6120\n",
            "[460 | 337.78] loss=2.4558 avg=2.6105\n",
            "[460 | 337.96] VAL_loss=2.6772 VAL_avg=2.6836\n",
            "[461 | 338.58] loss=2.6091 avg=2.6104\n",
            "[462 | 339.20] loss=2.6527 avg=2.6109\n",
            "[463 | 339.81] loss=1.8948 avg=2.6036\n",
            "[464 | 340.43] loss=2.5240 avg=2.6028\n",
            "[465 | 341.04] loss=2.5147 avg=2.6019\n",
            "[465 | 341.21] VAL_loss=2.6565 VAL_avg=2.6831\n",
            "[466 | 341.83] loss=2.2821 avg=2.5987\n",
            "[467 | 342.44] loss=2.7060 avg=2.5998\n",
            "[468 | 343.05] loss=2.5996 avg=2.5998\n",
            "[469 | 343.66] loss=2.5526 avg=2.5993\n",
            "[470 | 344.27] loss=2.3892 avg=2.5972\n",
            "[470 | 344.44] VAL_loss=2.7916 VAL_avg=2.6849\n",
            "[471 | 345.05] loss=2.5759 avg=2.5970\n",
            "[472 | 345.65] loss=2.7828 avg=2.5989\n",
            "[473 | 346.26] loss=2.5495 avg=2.5984\n",
            "[474 | 346.86] loss=2.1207 avg=2.5935\n",
            "[475 | 347.46] loss=2.6375 avg=2.5940\n",
            "[475 | 347.63] VAL_loss=2.4328 VAL_avg=2.6808\n",
            "[476 | 348.23] loss=2.6678 avg=2.5947\n",
            "[477 | 348.83] loss=2.5230 avg=2.5940\n",
            "[478 | 349.43] loss=2.5294 avg=2.5934\n",
            "[479 | 350.03] loss=2.4411 avg=2.5918\n",
            "[480 | 350.62] loss=2.4999 avg=2.5909\n",
            "[480 | 350.79] VAL_loss=2.4723 VAL_avg=2.6774\n",
            "[481 | 351.39] loss=2.5418 avg=2.5904\n",
            "[482 | 351.99] loss=2.5400 avg=2.5899\n",
            "[483 | 352.59] loss=2.2933 avg=2.5869\n",
            "[484 | 353.18] loss=2.6359 avg=2.5874\n",
            "[485 | 353.77] loss=2.7041 avg=2.5886\n",
            "[485 | 353.94] VAL_loss=2.6900 VAL_avg=2.6776\n",
            "[486 | 354.54] loss=2.6130 avg=2.5888\n",
            "[487 | 355.13] loss=2.4682 avg=2.5876\n",
            "[488 | 355.72] loss=2.4456 avg=2.5862\n",
            "[489 | 356.31] loss=2.5661 avg=2.5860\n",
            "[490 | 356.90] loss=2.4655 avg=2.5848\n",
            "[490 | 357.07] VAL_loss=2.8600 VAL_avg=2.6806\n",
            "[491 | 357.66] loss=2.4596 avg=2.5835\n",
            "[492 | 358.25] loss=3.0034 avg=2.5877\n",
            "[493 | 358.84] loss=2.3853 avg=2.5857\n",
            "[494 | 359.43] loss=2.6287 avg=2.5861\n",
            "[495 | 360.03] loss=2.5931 avg=2.5862\n",
            "[495 | 360.19] VAL_loss=2.7535 VAL_avg=2.6817\n",
            "[496 | 360.78] loss=2.6083 avg=2.5864\n",
            "[497 | 361.37] loss=2.5674 avg=2.5862\n",
            "[498 | 361.95] loss=2.7421 avg=2.5878\n",
            "[499 | 362.54] loss=2.5144 avg=2.5871\n",
            "[500 | 363.12] loss=2.3828 avg=2.5850\n",
            "[500 | 363.29] VAL_loss=2.5667 VAL_avg=2.6799\n",
            "Saving checkpoint/run1/model-500\n",
            "[501 | 374.72] loss=2.5739 avg=2.5849\n",
            "[502 | 375.28] loss=2.4510 avg=2.5835\n",
            "[503 | 375.84] loss=2.5321 avg=2.5830\n",
            "[504 | 376.41] loss=2.6397 avg=2.5836\n",
            "[505 | 376.97] loss=2.4558 avg=2.5823\n",
            "[505 | 377.13] VAL_loss=2.6803 VAL_avg=2.6799\n",
            "[506 | 377.70] loss=2.5429 avg=2.5819\n",
            "[507 | 378.26] loss=2.1847 avg=2.5779\n",
            "[508 | 378.83] loss=2.1061 avg=2.5732\n",
            "[509 | 379.40] loss=2.5785 avg=2.5732\n",
            "[510 | 379.97] loss=2.4811 avg=2.5723\n",
            "[510 | 380.13] VAL_loss=2.5061 VAL_avg=2.6772\n",
            "[511 | 380.69] loss=2.3859 avg=2.5704\n",
            "[512 | 381.27] loss=2.6472 avg=2.5712\n",
            "[513 | 381.84] loss=2.8796 avg=2.5743\n",
            "[514 | 382.41] loss=2.7419 avg=2.5760\n",
            "[515 | 382.98] loss=2.7496 avg=2.5777\n",
            "[515 | 383.14] VAL_loss=2.5896 VAL_avg=2.6758\n",
            "[516 | 383.72] loss=2.5477 avg=2.5774\n",
            "[517 | 384.30] loss=2.6707 avg=2.5784\n",
            "[518 | 384.87] loss=2.9092 avg=2.5817\n",
            "[519 | 385.45] loss=2.5406 avg=2.5813\n",
            "[520 | 386.02] loss=2.4735 avg=2.5802\n",
            "[520 | 386.18] VAL_loss=2.8128 VAL_avg=2.6779\n",
            "[521 | 386.77] loss=2.4541 avg=2.5789\n",
            "[522 | 387.35] loss=2.6744 avg=2.5799\n",
            "[523 | 387.93] loss=2.5922 avg=2.5800\n",
            "[524 | 388.52] loss=2.4452 avg=2.5786\n",
            "[525 | 389.10] loss=2.5893 avg=2.5788\n",
            "[525 | 389.26] VAL_loss=2.6151 VAL_avg=2.6770\n",
            "[526 | 389.85] loss=2.5775 avg=2.5787\n",
            "[527 | 390.44] loss=2.4670 avg=2.5776\n",
            "[528 | 391.03] loss=2.5432 avg=2.5773\n",
            "[529 | 391.62] loss=2.2815 avg=2.5743\n",
            "[530 | 392.20] loss=2.4734 avg=2.5733\n",
            "[530 | 392.37] VAL_loss=2.5826 VAL_avg=2.6755\n",
            "[531 | 392.96] loss=2.6757 avg=2.5743\n",
            "[532 | 393.55] loss=2.6709 avg=2.5753\n",
            "[533 | 394.14] loss=2.3292 avg=2.5728\n",
            "[534 | 394.73] loss=2.7805 avg=2.5749\n",
            "[535 | 395.32] loss=2.6174 avg=2.5753\n",
            "[535 | 395.49] VAL_loss=2.7137 VAL_avg=2.6761\n",
            "[536 | 396.09] loss=2.6356 avg=2.5759\n",
            "[537 | 396.69] loss=2.6178 avg=2.5764\n",
            "[538 | 397.29] loss=2.5841 avg=2.5764\n",
            "[539 | 397.89] loss=2.7704 avg=2.5784\n",
            "[540 | 398.49] loss=2.5594 avg=2.5782\n",
            "[540 | 398.66] VAL_loss=2.6991 VAL_avg=2.6765\n",
            "[541 | 399.26] loss=2.7236 avg=2.5796\n",
            "[542 | 399.87] loss=2.7450 avg=2.5813\n",
            "[543 | 400.47] loss=2.5002 avg=2.5805\n",
            "[544 | 401.08] loss=2.7060 avg=2.5818\n",
            "[545 | 401.69] loss=2.6721 avg=2.5827\n",
            "[545 | 401.86] VAL_loss=2.4867 VAL_avg=2.6736\n",
            "[546 | 402.47] loss=2.2293 avg=2.5791\n",
            "[547 | 403.09] loss=2.6268 avg=2.5796\n",
            "[548 | 403.69] loss=2.4969 avg=2.5788\n",
            "[549 | 404.30] loss=2.6884 avg=2.5799\n",
            "[550 | 404.92] loss=2.4965 avg=2.5790\n",
            "[550 | 405.09] VAL_loss=2.6392 VAL_avg=2.6731\n",
            "[551 | 405.71] loss=2.3001 avg=2.5762\n",
            "[552 | 406.32] loss=2.6332 avg=2.5768\n",
            "[553 | 406.94] loss=2.5658 avg=2.5767\n",
            "[554 | 407.55] loss=2.2059 avg=2.5730\n",
            "[555 | 408.16] loss=2.4270 avg=2.5715\n",
            "[555 | 408.33] VAL_loss=2.4957 VAL_avg=2.6705\n",
            "[556 | 408.95] loss=2.8336 avg=2.5741\n",
            "[557 | 409.57] loss=2.5918 avg=2.5743\n",
            "[558 | 410.18] loss=2.6446 avg=2.5750\n",
            "[559 | 410.79] loss=2.1338 avg=2.5706\n",
            "[560 | 411.41] loss=2.7299 avg=2.5722\n",
            "[560 | 411.58] VAL_loss=2.5203 VAL_avg=2.6682\n",
            "[561 | 412.19] loss=2.8399 avg=2.5749\n",
            "[562 | 412.80] loss=2.3997 avg=2.5731\n",
            "[563 | 413.41] loss=2.9018 avg=2.5764\n",
            "[564 | 414.02] loss=2.3881 avg=2.5745\n",
            "[565 | 414.63] loss=2.2262 avg=2.5710\n",
            "[565 | 414.80] VAL_loss=2.5075 VAL_avg=2.6659\n",
            "[566 | 415.41] loss=2.7474 avg=2.5728\n",
            "[567 | 416.02] loss=2.3430 avg=2.5705\n",
            "[568 | 416.63] loss=2.3467 avg=2.5682\n",
            "[569 | 417.23] loss=2.5792 avg=2.5684\n",
            "[570 | 417.84] loss=2.8626 avg=2.5713\n",
            "[570 | 418.01] VAL_loss=2.7740 VAL_avg=2.6675\n",
            "[571 | 418.61] loss=2.4640 avg=2.5702\n",
            "[572 | 419.22] loss=2.7470 avg=2.5720\n",
            "[573 | 419.82] loss=2.3669 avg=2.5699\n",
            "[574 | 420.42] loss=2.5995 avg=2.5702\n",
            "[575 | 421.02] loss=2.5678 avg=2.5702\n",
            "[575 | 421.19] VAL_loss=2.6428 VAL_avg=2.6671\n",
            "[576 | 421.79] loss=2.4144 avg=2.5687\n",
            "[577 | 422.39] loss=2.8329 avg=2.5713\n",
            "[578 | 422.99] loss=2.5384 avg=2.5710\n",
            "[579 | 423.58] loss=2.4647 avg=2.5699\n",
            "[580 | 424.19] loss=2.7440 avg=2.5717\n",
            "[580 | 424.36] VAL_loss=2.5409 VAL_avg=2.6653\n",
            "[581 | 424.96] loss=2.0398 avg=2.5663\n",
            "[582 | 425.55] loss=2.5496 avg=2.5662\n",
            "[583 | 426.15] loss=2.6648 avg=2.5671\n",
            "[584 | 426.75] loss=2.5983 avg=2.5675\n",
            "[585 | 427.34] loss=2.4604 avg=2.5664\n",
            "[585 | 427.51] VAL_loss=2.5590 VAL_avg=2.6637\n",
            "[586 | 428.10] loss=2.4773 avg=2.5655\n",
            "[587 | 428.69] loss=2.6707 avg=2.5665\n",
            "[588 | 429.29] loss=2.6556 avg=2.5674\n",
            "[589 | 429.88] loss=2.5631 avg=2.5674\n",
            "[590 | 430.48] loss=2.6489 avg=2.5682\n",
            "[590 | 430.64] VAL_loss=2.5717 VAL_avg=2.6624\n",
            "[591 | 431.24] loss=2.7177 avg=2.5697\n",
            "[592 | 431.83] loss=2.7063 avg=2.5711\n",
            "[593 | 432.41] loss=2.5426 avg=2.5708\n",
            "[594 | 433.00] loss=2.4508 avg=2.5696\n",
            "[595 | 433.59] loss=2.1009 avg=2.5649\n",
            "[595 | 433.76] VAL_loss=2.5960 VAL_avg=2.6615\n",
            "[596 | 434.35] loss=2.6267 avg=2.5655\n",
            "[597 | 434.95] loss=2.8264 avg=2.5681\n",
            "[598 | 435.54] loss=2.6984 avg=2.5694\n",
            "[599 | 436.13] loss=2.5690 avg=2.5694\n",
            "[600 | 436.71] loss=2.4274 avg=2.5680\n",
            "[600 | 436.88] VAL_loss=2.6106 VAL_avg=2.6607\n",
            "Saving checkpoint/run1/model-600\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "[601 | 448.44] loss=2.5720 avg=2.5680\n",
            "[602 | 449.01] loss=2.7164 avg=2.5695\n",
            "[603 | 449.57] loss=2.7151 avg=2.5710\n",
            "[604 | 450.14] loss=2.5341 avg=2.5706\n",
            "[605 | 450.71] loss=2.6451 avg=2.5714\n",
            "[605 | 450.86] VAL_loss=2.3477 VAL_avg=2.6563\n",
            "[606 | 451.43] loss=2.0731 avg=2.5664\n",
            "[607 | 452.00] loss=2.2324 avg=2.5630\n",
            "[608 | 452.57] loss=2.5786 avg=2.5632\n",
            "[609 | 453.14] loss=2.5133 avg=2.5627\n",
            "[610 | 453.70] loss=2.7389 avg=2.5645\n",
            "[610 | 453.86] VAL_loss=2.4738 VAL_avg=2.6537\n",
            "[611 | 454.43] loss=2.5842 avg=2.5646\n",
            "[612 | 455.01] loss=2.5759 avg=2.5648\n",
            "[613 | 455.58] loss=2.8520 avg=2.5676\n",
            "[614 | 456.15] loss=2.3620 avg=2.5656\n",
            "[615 | 456.73] loss=2.4808 avg=2.5647\n",
            "[615 | 456.88] VAL_loss=2.6529 VAL_avg=2.6537\n",
            "[616 | 457.47] loss=2.4328 avg=2.5634\n",
            "[617 | 458.04] loss=2.5013 avg=2.5628\n",
            "[618 | 458.62] loss=2.6579 avg=2.5637\n",
            "[619 | 459.20] loss=2.5802 avg=2.5639\n",
            "[620 | 459.78] loss=2.2591 avg=2.5609\n",
            "[620 | 459.94] VAL_loss=2.5824 VAL_avg=2.6527\n",
            "[621 | 460.53] loss=2.5101 avg=2.5603\n",
            "[622 | 461.11] loss=2.6463 avg=2.5612\n",
            "[623 | 461.69] loss=2.6515 avg=2.5621\n",
            "[624 | 462.27] loss=2.6268 avg=2.5628\n",
            "[625 | 462.85] loss=2.8023 avg=2.5652\n",
            "[625 | 463.02] VAL_loss=2.8604 VAL_avg=2.6556\n",
            "[626 | 463.61] loss=2.3957 avg=2.5635\n",
            "[627 | 464.20] loss=2.1623 avg=2.5594\n",
            "[628 | 464.78] loss=2.7301 avg=2.5611\n",
            "[629 | 465.37] loss=2.4463 avg=2.5600\n",
            "[630 | 465.96] loss=2.4977 avg=2.5594\n",
            "[630 | 466.13] VAL_loss=2.5698 VAL_avg=2.6544\n",
            "[631 | 466.71] loss=2.4620 avg=2.5584\n",
            "[632 | 467.31] loss=2.4722 avg=2.5575\n",
            "[633 | 467.91] loss=2.4393 avg=2.5564\n",
            "[634 | 468.50] loss=2.4677 avg=2.5555\n",
            "[635 | 469.10] loss=2.4913 avg=2.5548\n",
            "[635 | 469.26] VAL_loss=2.8465 VAL_avg=2.6571\n",
            "[636 | 469.86] loss=2.4958 avg=2.5542\n",
            "[637 | 470.46] loss=2.4711 avg=2.5534\n",
            "[638 | 471.06] loss=2.5650 avg=2.5535\n",
            "[639 | 471.66] loss=2.0060 avg=2.5480\n",
            "[640 | 472.26] loss=2.2224 avg=2.5448\n",
            "[640 | 472.43] VAL_loss=2.6676 VAL_avg=2.6572\n",
            "[641 | 473.03] loss=2.4706 avg=2.5440\n",
            "[642 | 473.64] loss=2.7489 avg=2.5461\n",
            "[643 | 474.25] loss=2.9235 avg=2.5499\n",
            "[644 | 474.85] loss=2.5717 avg=2.5501\n",
            "[645 | 475.46] loss=2.6438 avg=2.5510\n",
            "[645 | 475.63] VAL_loss=2.5246 VAL_avg=2.6554\n",
            "[646 | 476.25] loss=2.8167 avg=2.5537\n",
            "[647 | 476.86] loss=2.6171 avg=2.5543\n",
            "[648 | 477.47] loss=2.3812 avg=2.5526\n",
            "[649 | 478.09] loss=2.6225 avg=2.5533\n",
            "[650 | 478.70] loss=2.6639 avg=2.5544\n",
            "[650 | 478.87] VAL_loss=2.5805 VAL_avg=2.6544\n",
            "[651 | 479.50] loss=2.6284 avg=2.5551\n",
            "[652 | 480.11] loss=2.7432 avg=2.5570\n",
            "[653 | 480.73] loss=2.7871 avg=2.5593\n",
            "[654 | 481.35] loss=2.6022 avg=2.5597\n",
            "[655 | 481.97] loss=2.5468 avg=2.5596\n",
            "[655 | 482.14] VAL_loss=2.7252 VAL_avg=2.6553\n",
            "[656 | 482.76] loss=2.3818 avg=2.5578\n",
            "[657 | 483.38] loss=2.2772 avg=2.5550\n",
            "[658 | 483.99] loss=2.1398 avg=2.5509\n",
            "[659 | 484.61] loss=2.5560 avg=2.5509\n",
            "[660 | 485.22] loss=2.0331 avg=2.5457\n",
            "[660 | 485.39] VAL_loss=2.3981 VAL_avg=2.6518\n",
            "[661 | 486.01] loss=2.3550 avg=2.5438\n",
            "[662 | 486.63] loss=2.6597 avg=2.5450\n",
            "[663 | 487.24] loss=2.4858 avg=2.5444\n",
            "[664 | 487.85] loss=2.7274 avg=2.5462\n",
            "[665 | 488.46] loss=2.2698 avg=2.5435\n",
            "[665 | 488.63] VAL_loss=2.6743 VAL_avg=2.6521\n",
            "[666 | 489.24] loss=2.5625 avg=2.5436\n",
            "[667 | 489.85] loss=2.6368 avg=2.5446\n",
            "[668 | 490.45] loss=2.7332 avg=2.5465\n",
            "[669 | 491.05] loss=2.5418 avg=2.5464\n",
            "[670 | 491.66] loss=2.4216 avg=2.5452\n",
            "[670 | 491.83] VAL_loss=2.8886 VAL_avg=2.6553\n",
            "[671 | 492.43] loss=2.6121 avg=2.5458\n",
            "[672 | 493.04] loss=2.4471 avg=2.5448\n",
            "[673 | 493.64] loss=2.5234 avg=2.5446\n",
            "[674 | 494.24] loss=2.4683 avg=2.5439\n",
            "[675 | 494.84] loss=2.6062 avg=2.5445\n",
            "[675 | 495.01] VAL_loss=2.5118 VAL_avg=2.6534\n",
            "[676 | 495.61] loss=2.6117 avg=2.5452\n",
            "[677 | 496.22] loss=2.8063 avg=2.5478\n",
            "[678 | 496.81] loss=2.7035 avg=2.5493\n",
            "[679 | 497.41] loss=2.6256 avg=2.5501\n",
            "[680 | 498.01] loss=2.6132 avg=2.5507\n",
            "[680 | 498.17] VAL_loss=2.5180 VAL_avg=2.6516\n",
            "[681 | 498.78] loss=2.2474 avg=2.5477\n",
            "[682 | 499.38] loss=2.6462 avg=2.5487\n",
            "[683 | 499.97] loss=2.4109 avg=2.5473\n",
            "[684 | 500.57] loss=2.6615 avg=2.5484\n",
            "[685 | 501.16] loss=2.2377 avg=2.5453\n",
            "[685 | 501.33] VAL_loss=2.6066 VAL_avg=2.6510\n",
            "[686 | 501.93] loss=2.4580 avg=2.5445\n",
            "[687 | 502.52] loss=2.6138 avg=2.5452\n",
            "[688 | 503.11] loss=2.4896 avg=2.5446\n",
            "[689 | 503.70] loss=2.5048 avg=2.5442\n",
            "[690 | 504.29] loss=2.9824 avg=2.5486\n",
            "[690 | 504.46] VAL_loss=2.6601 VAL_avg=2.6511\n",
            "[691 | 505.05] loss=2.4719 avg=2.5478\n",
            "[692 | 505.64] loss=2.5511 avg=2.5479\n",
            "[693 | 506.23] loss=2.6522 avg=2.5489\n",
            "[694 | 506.82] loss=2.4007 avg=2.5474\n",
            "[695 | 507.41] loss=2.4882 avg=2.5468\n",
            "[695 | 507.57] VAL_loss=2.6418 VAL_avg=2.6510\n",
            "[696 | 508.16] loss=2.4344 avg=2.5457\n",
            "[697 | 508.75] loss=2.4319 avg=2.5446\n",
            "[698 | 509.34] loss=2.5539 avg=2.5447\n",
            "[699 | 509.93] loss=2.7708 avg=2.5469\n",
            "[700 | 510.52] loss=2.5606 avg=2.5471\n",
            "[700 | 510.68] VAL_loss=2.5406 VAL_avg=2.6495\n",
            "Saving checkpoint/run1/model-700\n",
            "[701 | 522.17] loss=2.5258 avg=2.5468\n",
            "[702 | 522.73] loss=2.1441 avg=2.5428\n",
            "[703 | 523.29] loss=2.3204 avg=2.5406\n",
            "[704 | 523.86] loss=2.5587 avg=2.5408\n",
            "[705 | 524.42] loss=2.1604 avg=2.5370\n",
            "[705 | 524.58] VAL_loss=3.0385 VAL_avg=2.6546\n",
            "[706 | 525.15] loss=2.8925 avg=2.5405\n",
            "[707 | 525.72] loss=2.6321 avg=2.5414\n",
            "[708 | 526.29] loss=2.7231 avg=2.5433\n",
            "[709 | 526.86] loss=2.6234 avg=2.5441\n",
            "[710 | 527.43] loss=2.8626 avg=2.5472\n",
            "[710 | 527.59] VAL_loss=2.7864 VAL_avg=2.6564\n",
            "[711 | 528.16] loss=2.5327 avg=2.5471\n",
            "[712 | 528.73] loss=2.3443 avg=2.5451\n",
            "[713 | 529.30] loss=2.4819 avg=2.5444\n",
            "[714 | 529.87] loss=2.9435 avg=2.5484\n",
            "[715 | 530.44] loss=2.4295 avg=2.5472\n",
            "[715 | 530.60] VAL_loss=2.5543 VAL_avg=2.6550\n",
            "[716 | 531.18] loss=2.0973 avg=2.5427\n",
            "[717 | 531.75] loss=2.5264 avg=2.5426\n",
            "[718 | 532.33] loss=2.7814 avg=2.5450\n",
            "[719 | 532.91] loss=2.4902 avg=2.5444\n",
            "[720 | 533.49] loss=2.6664 avg=2.5456\n",
            "[720 | 533.65] VAL_loss=2.7657 VAL_avg=2.6565\n",
            "[721 | 534.24] loss=2.6188 avg=2.5464\n",
            "[722 | 534.82] loss=2.5991 avg=2.5469\n",
            "[723 | 535.40] loss=2.4305 avg=2.5457\n",
            "[724 | 535.99] loss=2.4041 avg=2.5443\n",
            "[725 | 536.57] loss=2.0866 avg=2.5397\n",
            "[725 | 536.73] VAL_loss=2.6260 VAL_avg=2.6561\n",
            "[726 | 537.32] loss=2.4298 avg=2.5386\n",
            "[727 | 537.90] loss=2.5976 avg=2.5392\n",
            "[728 | 538.48] loss=2.5293 avg=2.5391\n",
            "[729 | 539.08] loss=2.4765 avg=2.5385\n",
            "[730 | 539.66] loss=2.3742 avg=2.5369\n",
            "[730 | 539.83] VAL_loss=2.6608 VAL_avg=2.6561\n",
            "[731 | 540.42] loss=2.3489 avg=2.5350\n",
            "[732 | 541.01] loss=2.4834 avg=2.5345\n",
            "[733 | 541.60] loss=2.5352 avg=2.5345\n",
            "[734 | 542.19] loss=2.6128 avg=2.5352\n",
            "[735 | 542.79] loss=2.3305 avg=2.5332\n",
            "[735 | 542.95] VAL_loss=2.6711 VAL_avg=2.6563\n",
            "[736 | 543.55] loss=2.6539 avg=2.5344\n",
            "[737 | 544.15] loss=2.6680 avg=2.5357\n",
            "[738 | 544.74] loss=1.7381 avg=2.5278\n",
            "[739 | 545.34] loss=2.4884 avg=2.5274\n",
            "[740 | 545.95] loss=2.5627 avg=2.5277\n",
            "[740 | 546.11] VAL_loss=2.5007 VAL_avg=2.6543\n",
            "[741 | 546.72] loss=2.6883 avg=2.5293\n",
            "[742 | 547.32] loss=2.6000 avg=2.5300\n",
            "[743 | 547.92] loss=2.5646 avg=2.5304\n",
            "[744 | 548.53] loss=2.2318 avg=2.5274\n",
            "[745 | 549.13] loss=2.6205 avg=2.5283\n",
            "[745 | 549.31] VAL_loss=2.5007 VAL_avg=2.6524\n",
            "[746 | 549.92] loss=2.7426 avg=2.5305\n",
            "[747 | 550.53] loss=2.6789 avg=2.5320\n",
            "[748 | 551.14] loss=2.5448 avg=2.5321\n",
            "[749 | 551.75] loss=2.3085 avg=2.5298\n",
            "[750 | 552.37] loss=2.5825 avg=2.5304\n",
            "[750 | 552.54] VAL_loss=2.5893 VAL_avg=2.6515\n",
            "[751 | 553.16] loss=2.4387 avg=2.5295\n",
            "[752 | 553.77] loss=2.8807 avg=2.5330\n",
            "[753 | 554.38] loss=2.6527 avg=2.5342\n",
            "[754 | 554.99] loss=2.5126 avg=2.5340\n",
            "[755 | 555.61] loss=2.5194 avg=2.5338\n",
            "[755 | 555.78] VAL_loss=2.6961 VAL_avg=2.6521\n",
            "[756 | 556.40] loss=2.8856 avg=2.5373\n",
            "[757 | 557.01] loss=2.5784 avg=2.5377\n",
            "[758 | 557.63] loss=2.0676 avg=2.5330\n",
            "[759 | 558.24] loss=2.4785 avg=2.5325\n",
            "[760 | 558.85] loss=2.7227 avg=2.5344\n",
            "[760 | 559.03] VAL_loss=2.4542 VAL_avg=2.6496\n",
            "[761 | 559.65] loss=2.5092 avg=2.5341\n",
            "[762 | 560.26] loss=2.7570 avg=2.5364\n",
            "[763 | 560.87] loss=2.6718 avg=2.5377\n",
            "[764 | 561.48] loss=2.8714 avg=2.5411\n",
            "[765 | 562.09] loss=2.6384 avg=2.5420\n",
            "[765 | 562.26] VAL_loss=2.5853 VAL_avg=2.6488\n",
            "[766 | 562.87] loss=2.7173 avg=2.5438\n",
            "[767 | 563.48] loss=2.3972 avg=2.5423\n",
            "[768 | 564.09] loss=2.6197 avg=2.5431\n",
            "[769 | 564.69] loss=2.5890 avg=2.5436\n",
            "[770 | 565.30] loss=2.5617 avg=2.5437\n",
            "[770 | 565.47] VAL_loss=2.4983 VAL_avg=2.6469\n",
            "[771 | 566.07] loss=2.5791 avg=2.5441\n",
            "[772 | 566.67] loss=2.3490 avg=2.5421\n",
            "[773 | 567.27] loss=2.7938 avg=2.5447\n",
            "[774 | 567.87] loss=2.6793 avg=2.5460\n",
            "[775 | 568.48] loss=2.5618 avg=2.5462\n",
            "[775 | 568.64] VAL_loss=2.6192 VAL_avg=2.6465\n",
            "[776 | 569.25] loss=2.8011 avg=2.5487\n",
            "[777 | 569.85] loss=2.7172 avg=2.5504\n",
            "[778 | 570.45] loss=2.5603 avg=2.5505\n",
            "[779 | 571.04] loss=2.2656 avg=2.5476\n",
            "[780 | 571.64] loss=2.6310 avg=2.5485\n",
            "[780 | 571.80] VAL_loss=2.6678 VAL_avg=2.6468\n",
            "[781 | 572.41] loss=2.4290 avg=2.5473\n",
            "[782 | 573.00] loss=2.7475 avg=2.5493\n",
            "[783 | 573.60] loss=2.6633 avg=2.5504\n",
            "[784 | 574.19] loss=2.6099 avg=2.5510\n",
            "[785 | 574.78] loss=2.5395 avg=2.5509\n",
            "[785 | 574.95] VAL_loss=2.6458 VAL_avg=2.6468\n",
            "[786 | 575.54] loss=2.5759 avg=2.5512\n",
            "[787 | 576.14] loss=2.5084 avg=2.5507\n",
            "[788 | 576.73] loss=2.7037 avg=2.5523\n",
            "[789 | 577.32] loss=2.5184 avg=2.5519\n",
            "[790 | 577.91] loss=1.9209 avg=2.5456\n",
            "[790 | 578.08] VAL_loss=2.4852 VAL_avg=2.6447\n",
            "[791 | 578.66] loss=2.5382 avg=2.5455\n",
            "[792 | 579.25] loss=2.5721 avg=2.5458\n",
            "[793 | 579.84] loss=2.7552 avg=2.5479\n",
            "[794 | 580.43] loss=2.2891 avg=2.5453\n",
            "[795 | 581.02] loss=2.1881 avg=2.5417\n",
            "[795 | 581.19] VAL_loss=2.6757 VAL_avg=2.6451\n",
            "[796 | 581.77] loss=2.3690 avg=2.5400\n",
            "[797 | 582.37] loss=2.6667 avg=2.5413\n",
            "[798 | 582.95] loss=2.3891 avg=2.5398\n",
            "[799 | 583.55] loss=2.4397 avg=2.5388\n",
            "[800 | 584.14] loss=2.8120 avg=2.5415\n",
            "[800 | 584.30] VAL_loss=2.3598 VAL_avg=2.6415\n",
            "Saving checkpoint/run1/model-800\n",
            "[801 | 595.82] loss=2.7495 avg=2.5436\n",
            "[802 | 596.37] loss=2.2301 avg=2.5404\n",
            "[803 | 596.94] loss=2.4568 avg=2.5396\n",
            "[804 | 597.50] loss=2.4791 avg=2.5390\n",
            "[805 | 598.07] loss=2.5027 avg=2.5386\n",
            "[805 | 598.23] VAL_loss=2.5652 VAL_avg=2.6406\n",
            "[806 | 598.80] loss=2.6513 avg=2.5398\n",
            "[807 | 599.36] loss=2.4148 avg=2.5385\n",
            "[808 | 599.93] loss=2.6163 avg=2.5393\n",
            "[809 | 600.50] loss=2.7169 avg=2.5411\n",
            "[810 | 601.07] loss=2.7588 avg=2.5432\n",
            "[810 | 601.23] VAL_loss=2.6899 VAL_avg=2.6412\n",
            "[811 | 601.81] loss=2.3926 avg=2.5417\n",
            "[812 | 602.38] loss=2.6257 avg=2.5426\n",
            "[813 | 602.95] loss=2.3795 avg=2.5409\n",
            "[814 | 603.53] loss=2.4371 avg=2.5399\n",
            "[815 | 604.10] loss=2.6469 avg=2.5410\n",
            "[815 | 604.26] VAL_loss=2.7359 VAL_avg=2.6424\n",
            "[816 | 604.84] loss=2.4551 avg=2.5401\n",
            "[817 | 605.42] loss=2.4545 avg=2.5393\n",
            "[818 | 605.99] loss=2.5417 avg=2.5393\n",
            "[819 | 606.56] loss=2.4534 avg=2.5384\n",
            "[820 | 607.14] loss=2.5795 avg=2.5388\n",
            "[820 | 607.30] VAL_loss=2.4742 VAL_avg=2.6403\n",
            "[821 | 607.88] loss=2.5832 avg=2.5393\n",
            "[822 | 608.46] loss=2.7415 avg=2.5413\n",
            "[823 | 609.04] loss=2.5737 avg=2.5416\n",
            "[824 | 609.63] loss=2.5301 avg=2.5415\n",
            "[825 | 610.21] loss=2.4529 avg=2.5406\n",
            "[825 | 610.37] VAL_loss=2.5425 VAL_avg=2.6391\n",
            "[826 | 610.96] loss=2.7024 avg=2.5422\n",
            "[827 | 611.54] loss=2.3047 avg=2.5399\n",
            "[828 | 612.13] loss=2.6260 avg=2.5407\n",
            "[829 | 612.72] loss=2.6557 avg=2.5419\n",
            "[830 | 613.31] loss=2.5437 avg=2.5419\n",
            "[830 | 613.47] VAL_loss=2.5821 VAL_avg=2.6384\n",
            "[831 | 614.06] loss=2.4884 avg=2.5414\n",
            "[832 | 614.65] loss=2.4855 avg=2.5408\n",
            "[833 | 615.24] loss=2.3671 avg=2.5391\n",
            "[834 | 615.83] loss=2.3217 avg=2.5369\n",
            "[835 | 616.42] loss=2.2001 avg=2.5335\n",
            "[835 | 616.59] VAL_loss=2.5589 VAL_avg=2.6374\n",
            "[836 | 617.19] loss=2.6832 avg=2.5350\n",
            "[837 | 617.79] loss=2.6793 avg=2.5365\n",
            "[838 | 618.38] loss=2.1494 avg=2.5326\n",
            "[839 | 618.98] loss=2.1095 avg=2.5284\n",
            "[840 | 619.58] loss=2.7888 avg=2.5310\n",
            "[840 | 619.75] VAL_loss=2.5111 VAL_avg=2.6359\n",
            "[841 | 620.35] loss=2.3397 avg=2.5290\n",
            "[842 | 620.95] loss=2.4588 avg=2.5283\n",
            "[843 | 621.56] loss=2.6195 avg=2.5293\n",
            "[844 | 622.16] loss=2.5080 avg=2.5290\n",
            "[845 | 622.77] loss=2.5286 avg=2.5290\n",
            "[845 | 622.94] VAL_loss=2.6105 VAL_avg=2.6356\n",
            "[846 | 623.55] loss=2.1165 avg=2.5249\n",
            "[847 | 624.16] loss=2.2368 avg=2.5220\n",
            "[848 | 624.77] loss=2.5931 avg=2.5227\n",
            "[849 | 625.39] loss=2.4582 avg=2.5221\n",
            "[850 | 626.00] loss=2.4714 avg=2.5216\n",
            "[850 | 626.17] VAL_loss=2.7273 VAL_avg=2.6367\n",
            "[851 | 626.80] loss=2.6313 avg=2.5227\n",
            "[852 | 627.41] loss=2.4871 avg=2.5223\n",
            "[853 | 628.02] loss=2.1551 avg=2.5187\n",
            "[854 | 628.63] loss=2.3638 avg=2.5171\n",
            "[855 | 629.25] loss=2.7279 avg=2.5192\n",
            "[855 | 629.42] VAL_loss=2.6046 VAL_avg=2.6363\n",
            "[856 | 630.04] loss=2.7720 avg=2.5217\n",
            "[857 | 630.65] loss=2.4379 avg=2.5209\n",
            "[858 | 631.27] loss=2.0040 avg=2.5157\n",
            "[859 | 631.89] loss=2.3960 avg=2.5145\n",
            "[860 | 632.50] loss=2.3707 avg=2.5131\n",
            "[860 | 632.68] VAL_loss=2.6149 VAL_avg=2.6360\n",
            "[861 | 633.29] loss=2.6541 avg=2.5145\n",
            "[862 | 633.91] loss=2.5718 avg=2.5151\n",
            "[863 | 634.52] loss=2.6083 avg=2.5160\n",
            "[864 | 635.13] loss=2.6845 avg=2.5177\n",
            "[865 | 635.75] loss=2.2692 avg=2.5152\n",
            "[865 | 635.92] VAL_loss=2.4506 VAL_avg=2.6338\n",
            "[866 | 636.54] loss=2.5260 avg=2.5153\n",
            "[867 | 637.15] loss=2.7888 avg=2.5181\n",
            "[868 | 637.76] loss=2.5989 avg=2.5189\n",
            "[869 | 638.37] loss=2.4747 avg=2.5184\n",
            "[870 | 638.98] loss=2.5543 avg=2.5188\n",
            "[870 | 639.15] VAL_loss=3.0073 VAL_avg=2.6383\n",
            "[871 | 639.75] loss=2.5369 avg=2.5190\n",
            "[872 | 640.35] loss=2.6951 avg=2.5207\n",
            "[873 | 640.96] loss=2.6177 avg=2.5217\n",
            "[874 | 641.56] loss=2.6841 avg=2.5233\n",
            "[875 | 642.16] loss=2.4390 avg=2.5225\n",
            "[875 | 642.33] VAL_loss=2.6733 VAL_avg=2.6387\n",
            "[876 | 642.93] loss=2.6606 avg=2.5239\n",
            "[877 | 643.54] loss=2.5085 avg=2.5237\n",
            "[878 | 644.14] loss=2.5488 avg=2.5240\n",
            "[879 | 644.74] loss=2.3711 avg=2.5224\n",
            "[880 | 645.33] loss=2.4154 avg=2.5214\n",
            "[880 | 645.50] VAL_loss=2.5402 VAL_avg=2.6375\n",
            "[881 | 646.09] loss=2.4387 avg=2.5205\n",
            "[882 | 646.69] loss=2.6087 avg=2.5214\n",
            "[883 | 647.28] loss=2.6132 avg=2.5223\n",
            "[884 | 647.88] loss=2.2579 avg=2.5197\n",
            "[885 | 648.48] loss=2.5624 avg=2.5201\n",
            "[885 | 648.64] VAL_loss=2.7835 VAL_avg=2.6393\n",
            "[886 | 649.24] loss=2.3914 avg=2.5188\n",
            "[887 | 649.83] loss=2.6511 avg=2.5201\n",
            "[888 | 650.42] loss=2.0746 avg=2.5157\n",
            "[889 | 651.02] loss=2.4663 avg=2.5152\n",
            "[890 | 651.61] loss=2.5305 avg=2.5153\n",
            "[890 | 651.78] VAL_loss=2.5740 VAL_avg=2.6385\n",
            "[891 | 652.37] loss=2.5948 avg=2.5161\n",
            "[892 | 652.96] loss=2.5232 avg=2.5162\n",
            "[893 | 653.56] loss=2.5148 avg=2.5162\n",
            "[894 | 654.14] loss=2.6252 avg=2.5173\n",
            "[895 | 654.73] loss=2.1798 avg=2.5139\n",
            "[895 | 654.89] VAL_loss=2.4040 VAL_avg=2.6357\n",
            "[896 | 655.49] loss=2.5272 avg=2.5140\n",
            "[897 | 656.07] loss=2.2510 avg=2.5114\n",
            "[898 | 656.66] loss=2.8964 avg=2.5153\n",
            "[899 | 657.25] loss=2.4026 avg=2.5141\n",
            "[900 | 657.83] loss=2.9503 avg=2.5185\n",
            "[900 | 658.00] VAL_loss=2.5140 VAL_avg=2.6342\n",
            "Saving checkpoint/run1/model-900\n",
            "[901 | 669.32] loss=2.6435 avg=2.5198\n",
            "[902 | 669.89] loss=2.5552 avg=2.5201\n",
            "[903 | 670.45] loss=2.6076 avg=2.5210\n",
            "[904 | 671.03] loss=2.5489 avg=2.5213\n",
            "[905 | 671.59] loss=2.5629 avg=2.5217\n",
            "[905 | 671.74] VAL_loss=2.6406 VAL_avg=2.6343\n",
            "[906 | 672.31] loss=2.3903 avg=2.5204\n",
            "[907 | 672.88] loss=2.4594 avg=2.5198\n",
            "[908 | 673.46] loss=2.6715 avg=2.5213\n",
            "[909 | 674.03] loss=2.4254 avg=2.5203\n",
            "[910 | 674.60] loss=2.5956 avg=2.5211\n",
            "[910 | 674.76] VAL_loss=2.7882 VAL_avg=2.6362\n",
            "[911 | 675.34] loss=2.5980 avg=2.5218\n",
            "[912 | 675.91] loss=2.5166 avg=2.5218\n",
            "[913 | 676.48] loss=2.4372 avg=2.5209\n",
            "[914 | 677.06] loss=2.8455 avg=2.5242\n",
            "[915 | 677.63] loss=2.8524 avg=2.5275\n",
            "[915 | 677.79] VAL_loss=2.6722 VAL_avg=2.6366\n",
            "[916 | 678.37] loss=2.4566 avg=2.5268\n",
            "[917 | 678.94] loss=2.7678 avg=2.5292\n",
            "[918 | 679.51] loss=2.0259 avg=2.5241\n",
            "[919 | 680.09] loss=2.6594 avg=2.5255\n",
            "[920 | 680.67] loss=2.1392 avg=2.5216\n",
            "[920 | 680.83] VAL_loss=2.6065 VAL_avg=2.6362\n",
            "[921 | 681.41] loss=2.7553 avg=2.5240\n",
            "[922 | 681.99] loss=2.4439 avg=2.5232\n",
            "[923 | 682.57] loss=2.7251 avg=2.5252\n",
            "[924 | 683.15] loss=2.6506 avg=2.5264\n",
            "[925 | 683.73] loss=2.7856 avg=2.5290\n",
            "[925 | 683.90] VAL_loss=2.6624 VAL_avg=2.6365\n",
            "[926 | 684.48] loss=2.4914 avg=2.5286\n",
            "[927 | 685.07] loss=2.3293 avg=2.5267\n",
            "[928 | 685.66] loss=2.1244 avg=2.5226\n",
            "[929 | 686.25] loss=2.4944 avg=2.5224\n",
            "[930 | 686.83] loss=2.8195 avg=2.5253\n",
            "[930 | 687.00] VAL_loss=2.5793 VAL_avg=2.6359\n",
            "[931 | 687.59] loss=2.7543 avg=2.5276\n",
            "[932 | 688.19] loss=2.6192 avg=2.5285\n",
            "[933 | 688.78] loss=2.5737 avg=2.5290\n",
            "[934 | 689.37] loss=2.1050 avg=2.5247\n",
            "[935 | 689.96] loss=2.6525 avg=2.5260\n",
            "[935 | 690.13] VAL_loss=2.6246 VAL_avg=2.6357\n",
            "[936 | 690.72] loss=2.1605 avg=2.5224\n",
            "[937 | 691.32] loss=2.5982 avg=2.5231\n",
            "[938 | 691.91] loss=2.5626 avg=2.5235\n",
            "[939 | 692.51] loss=2.3969 avg=2.5222\n",
            "[940 | 693.12] loss=2.6669 avg=2.5237\n",
            "[940 | 693.29] VAL_loss=2.3722 VAL_avg=2.6326\n",
            "[941 | 693.89] loss=2.3900 avg=2.5224\n",
            "[942 | 694.49] loss=2.0807 avg=2.5179\n",
            "[943 | 695.10] loss=2.4510 avg=2.5173\n",
            "[944 | 695.70] loss=2.6083 avg=2.5182\n",
            "[945 | 696.31] loss=2.4969 avg=2.5180\n",
            "[945 | 696.48] VAL_loss=2.5513 VAL_avg=2.6317\n",
            "[946 | 697.09] loss=2.4644 avg=2.5174\n",
            "[947 | 697.70] loss=3.0006 avg=2.5223\n",
            "[948 | 698.32] loss=2.6055 avg=2.5231\n",
            "[949 | 698.92] loss=2.7607 avg=2.5255\n",
            "[950 | 699.54] loss=2.6159 avg=2.5264\n",
            "[950 | 699.71] VAL_loss=2.7862 VAL_avg=2.6335\n",
            "[951 | 700.32] loss=2.5349 avg=2.5265\n",
            "[952 | 700.94] loss=2.6003 avg=2.5272\n",
            "[953 | 701.55] loss=2.4766 avg=2.5267\n",
            "[954 | 702.16] loss=2.6768 avg=2.5282\n",
            "[955 | 702.77] loss=2.2979 avg=2.5259\n",
            "[955 | 702.94] VAL_loss=2.6530 VAL_avg=2.6337\n",
            "[956 | 703.56] loss=2.4931 avg=2.5256\n",
            "[957 | 704.17] loss=2.5260 avg=2.5256\n",
            "[958 | 704.79] loss=2.7286 avg=2.5276\n",
            "[959 | 705.40] loss=2.4640 avg=2.5270\n",
            "[960 | 706.01] loss=2.5480 avg=2.5272\n",
            "[960 | 706.18] VAL_loss=2.5745 VAL_avg=2.6330\n",
            "[961 | 706.80] loss=2.5844 avg=2.5277\n",
            "[962 | 707.40] loss=2.4190 avg=2.5267\n",
            "[963 | 708.01] loss=2.5917 avg=2.5273\n",
            "[964 | 708.62] loss=2.8469 avg=2.5305\n",
            "[965 | 709.22] loss=2.5079 avg=2.5303\n",
            "[965 | 709.39] VAL_loss=2.5354 VAL_avg=2.6319\n",
            "[966 | 709.99] loss=2.6301 avg=2.5313\n",
            "[967 | 710.60] loss=2.1198 avg=2.5272\n",
            "[968 | 711.20] loss=1.9823 avg=2.5217\n",
            "[969 | 711.80] loss=2.4067 avg=2.5206\n",
            "[970 | 712.41] loss=2.6048 avg=2.5214\n",
            "[970 | 712.58] VAL_loss=2.8540 VAL_avg=2.6345\n",
            "[971 | 713.19] loss=2.3320 avg=2.5195\n",
            "[972 | 713.79] loss=2.1237 avg=2.5156\n",
            "[973 | 714.38] loss=2.5240 avg=2.5156\n",
            "[974 | 714.98] loss=2.3447 avg=2.5139\n",
            "[975 | 715.58] loss=2.4413 avg=2.5132\n",
            "[975 | 715.75] VAL_loss=2.7110 VAL_avg=2.6354\n",
            "[976 | 716.36] loss=2.6478 avg=2.5146\n",
            "[977 | 716.95] loss=2.5819 avg=2.5152\n",
            "[978 | 717.55] loss=2.6928 avg=2.5170\n",
            "[979 | 718.15] loss=2.3535 avg=2.5154\n",
            "[980 | 718.75] loss=2.5803 avg=2.5160\n",
            "[980 | 718.91] VAL_loss=2.4725 VAL_avg=2.6335\n",
            "[981 | 719.51] loss=2.6566 avg=2.5174\n",
            "[982 | 720.10] loss=1.9029 avg=2.5113\n",
            "[983 | 720.70] loss=2.1807 avg=2.5080\n",
            "[984 | 721.29] loss=2.3960 avg=2.5068\n",
            "[985 | 721.88] loss=2.3134 avg=2.5049\n",
            "[985 | 722.05] VAL_loss=2.7296 VAL_avg=2.6346\n",
            "[986 | 722.64] loss=2.6892 avg=2.5068\n",
            "[987 | 723.24] loss=2.5371 avg=2.5071\n",
            "[988 | 723.83] loss=2.3398 avg=2.5054\n",
            "[989 | 724.42] loss=2.5568 avg=2.5059\n",
            "[990 | 725.01] loss=2.4392 avg=2.5052\n",
            "[990 | 725.17] VAL_loss=2.3690 VAL_avg=2.6315\n",
            "[991 | 725.76] loss=2.5848 avg=2.5060\n",
            "[992 | 726.35] loss=2.4460 avg=2.5054\n",
            "[993 | 726.94] loss=2.2278 avg=2.5027\n",
            "[994 | 727.53] loss=2.5805 avg=2.5034\n",
            "[995 | 728.12] loss=2.6535 avg=2.5049\n",
            "[995 | 728.29] VAL_loss=2.8100 VAL_avg=2.6336\n",
            "[996 | 728.88] loss=2.5145 avg=2.5050\n",
            "[997 | 729.47] loss=2.7427 avg=2.5074\n",
            "[998 | 730.06] loss=2.7536 avg=2.5099\n",
            "[999 | 730.64] loss=2.4760 avg=2.5095\n",
            "======== SAMPLE 1 ========\n",
            "smoking of the well\" A:... the Earl Grey\n",
            "\n",
            "C: THE BIBLE ON TV Q: In his book of \"The Lord of the Rings\", this author said, \"There is no God but this\" A:... Lot\n",
            "\n",
            "C: FICTIONAL CHARACTERS Q: \"A Tale of Two Cities\", seen here, is the tale told in this \"Lion\" A:... \"Lord of the Flies\"\n",
            "\n",
            "C: THE U.S. SENATE Q: Though it was not considered a Republican, he served with the first Congress & was first to admit the Confederacy A:... John Logan\n",
            "\n",
            "C: CHICKPY LIT Q: She was the first to use the term \"glamour girl\" in her column A:... Marilyn Monroe\n",
            "\n",
            "C: BORN IN CANADA Q: You could call this Canadian great of film \"The Maniac\" but not his \"God\" A:... Stanley Kubrick\n",
            "\n",
            "C: THE KIND OF WORLD Q: In 1998 I got a phone call from the country he lives in, \"The most popular film series on the Internet\", said it A:... Russia\n",
            "\n",
            "C: MY NAME IS A BUNCH OF DUCKS Q: This \"Boomerang\" star is also known as the \"Godfather of the Big Leagues\" A:... Jerry Rice\n",
            "\n",
            "C: HISTORICAL NOVELS Q: \"The Death Of Joan of Arc\", which was published in 1451, is the first novel in this \"Anglo-Saxon\" language A:... English\n",
            "\n",
            "C: A WILD LIFE IN SPAIN Q: This Italian city's cathedral was named for an angelic prince, but was a bad idea, for it's stained glass window was broken by the 11th century A:... Pisa\n",
            "\n",
            "C: MYSTERY HODGEPODGE Q: The name of this country comes from an Arabic word meaning \"sea\", which is where you may call it home A:... Yemen\n",
            "\n",
            "C: BROADCASTINGS Q: \"We're in this crazy city where we're not safe, not safe, not safe but we're okay\" from the 1990 TV series \"The Year We Make Room\" A:... Chicago\n",
            "\n",
            "C: MY THING COOKING Q: A cheese of this country's Gombasso region might resemble cow's milk, but it was originally from a small dairy town made up of a few cows A:... Argentina\n",
            "\n",
            "C: POTENT POTABLES Q: French chemist Henri Galambos de la Plante claimed this plant was the main psychoactive ingredient found in \"the highest grade of tobacco\" A:... tobacco\n",
            "\n",
            "C: THE SHAPE OF THIS WORLD Q: This planet's orbit is slightly elliptical, with its highest point near 20,725 feet on its surface A:... Venus\n",
            "\n",
            "C: AFRICAN LITERATURE Q: He penned the novel \"I Heard the Blues\", later adapted as a film, where his family goes to visit their parents A:... Josephine Baker Atherton\n",
            "\n",
            "C: GAPWAY NERDS Q: In a 1969 speech, this former senator said, \"It was once the first lady of this country that you went to the circus & bought a horse...it never took off\" A:... (Sarah) Charnin\n",
            "\n",
            "C: SWEET FOOD & DRINK Q: \"The World of\" this drink, which can be made from sugar, cornstarch or water, has a name with \"water from the well, of my soul\" A:... vodka\n",
            "\n",
            "C: I'M IN CZECHOS Q: This Polish, who's buried in Zgolj, is also a poet, as you might expect of a man who's nicknamed \"The Holy One\" A:... Aleksandr Mozart\n",
            "\n",
            "C: I GOT A SHINY COUNT, BUT A SMALL COUNT Q: This Russian prince was the first person to win both the World's Greatest Spokesman Award & a Pulitzer for stories A:... Boris Pasternak\n",
            "\n",
            "C: WATER Q: The Krakatoa winds are thought to be the result of underwater volcanoes' eruption of volcanoes in this region A:... Indonesia\n",
            "\n",
            "C: I CAN DO IT, GUESS I'M A COSMOPOLITAN JUMP LYLE Q: \"Boredome\", a \"Love Story\" co-written with this man, is about an African teenager who's bored at home after a date A:... Robert Louis Stevenson\n",
            "\n",
            "C: FRUITS & JUNES Q: \"Til Death Do Us Part\" isn't a song of any type, but of a fruit of the variety that a banana comes from A:... Oranges\n",
            "\n",
            "C: THE BEATLES, THE BLUES, THE SPAN\n",
            "\n",
            "[1000 | 756.43] loss=2.6465 avg=2.5109\n",
            "[1000 | 756.59] VAL_loss=2.5588 VAL_avg=2.6327\n",
            "Saving checkpoint/run1/model-1000\n",
            "[1001 | 768.15] loss=2.8192 avg=2.5140\n",
            "[1002 | 768.70] loss=2.6421 avg=2.5153\n",
            "[1003 | 769.28] loss=2.6282 avg=2.5164\n",
            "[1004 | 769.85] loss=2.8753 avg=2.5200\n",
            "[1005 | 770.42] loss=2.7413 avg=2.5222\n",
            "[1005 | 770.58] VAL_loss=2.4526 VAL_avg=2.6306\n",
            "[1006 | 771.16] loss=2.5427 avg=2.5224\n",
            "[1007 | 771.73] loss=2.4855 avg=2.5220\n",
            "[1008 | 772.30] loss=2.4888 avg=2.5217\n",
            "[1009 | 772.88] loss=2.2005 avg=2.5185\n",
            "[1010 | 773.45] loss=2.4818 avg=2.5181\n",
            "[1010 | 773.61] VAL_loss=2.4925 VAL_avg=2.6290\n",
            "[1011 | 774.19] loss=2.6342 avg=2.5193\n",
            "[1012 | 774.77] loss=2.3992 avg=2.5181\n",
            "[1013 | 775.34] loss=2.6930 avg=2.5198\n",
            "[1014 | 775.92] loss=2.5278 avg=2.5199\n",
            "[1015 | 776.50] loss=2.1355 avg=2.5161\n",
            "[1015 | 776.66] VAL_loss=2.6696 VAL_avg=2.6295\n",
            "[1016 | 777.25] loss=2.3976 avg=2.5149\n",
            "[1017 | 777.83] loss=2.4396 avg=2.5141\n",
            "[1018 | 778.42] loss=2.4092 avg=2.5131\n",
            "[1019 | 779.00] loss=2.7136 avg=2.5151\n",
            "[1020 | 779.58] loss=2.6390 avg=2.5163\n",
            "[1020 | 779.75] VAL_loss=2.3834 VAL_avg=2.6267\n",
            "[1021 | 780.34] loss=2.6141 avg=2.5173\n",
            "[1022 | 780.93] loss=2.5577 avg=2.5177\n",
            "[1023 | 781.52] loss=2.5522 avg=2.5180\n",
            "[1024 | 782.11] loss=2.7540 avg=2.5204\n",
            "[1025 | 782.70] loss=2.5013 avg=2.5202\n",
            "[1025 | 782.87] VAL_loss=2.6531 VAL_avg=2.6270\n",
            "[1026 | 783.47] loss=2.3731 avg=2.5187\n",
            "[1027 | 784.06] loss=2.5090 avg=2.5186\n",
            "[1028 | 784.66] loss=2.6060 avg=2.5195\n",
            "[1029 | 785.25] loss=2.0786 avg=2.5151\n",
            "[1030 | 785.85] loss=2.8454 avg=2.5184\n",
            "[1030 | 786.02] VAL_loss=2.6825 VAL_avg=2.6276\n",
            "[1031 | 786.63] loss=2.4935 avg=2.5182\n",
            "[1032 | 787.23] loss=2.6109 avg=2.5191\n",
            "[1033 | 787.83] loss=2.0723 avg=2.5146\n",
            "[1034 | 788.43] loss=2.5313 avg=2.5148\n",
            "[1035 | 789.04] loss=2.1708 avg=2.5114\n",
            "[1035 | 789.21] VAL_loss=2.6484 VAL_avg=2.6279\n",
            "[1036 | 789.82] loss=2.3424 avg=2.5097\n",
            "[1037 | 790.43] loss=2.5618 avg=2.5102\n",
            "[1038 | 791.03] loss=2.1701 avg=2.5068\n",
            "[1039 | 791.64] loss=2.2203 avg=2.5039\n",
            "[1040 | 792.25] loss=2.2959 avg=2.5018\n",
            "[1040 | 792.43] VAL_loss=2.4307 VAL_avg=2.6256\n",
            "[1041 | 793.04] loss=2.4949 avg=2.5018\n",
            "[1042 | 793.66] loss=2.8459 avg=2.5052\n",
            "[1043 | 794.28] loss=2.7125 avg=2.5073\n",
            "[1044 | 794.89] loss=2.4730 avg=2.5069\n",
            "[1045 | 795.51] loss=2.5341 avg=2.5072\n",
            "[1045 | 795.69] VAL_loss=2.6840 VAL_avg=2.6263\n",
            "[1046 | 796.31] loss=2.6365 avg=2.5085\n",
            "[1047 | 796.92] loss=2.2522 avg=2.5059\n",
            "[1048 | 797.54] loss=2.7204 avg=2.5081\n",
            "[1049 | 798.15] loss=2.5834 avg=2.5088\n",
            "[1050 | 798.77] loss=2.2829 avg=2.5066\n",
            "[1050 | 798.94] VAL_loss=2.3368 VAL_avg=2.6230\n",
            "[1051 | 799.57] loss=2.4944 avg=2.5065\n",
            "[1052 | 800.18] loss=2.4190 avg=2.5056\n",
            "[1053 | 800.80] loss=2.6935 avg=2.5075\n",
            "[1054 | 801.41] loss=2.5339 avg=2.5077\n",
            "[1055 | 802.02] loss=2.5493 avg=2.5081\n",
            "[1055 | 802.19] VAL_loss=2.5439 VAL_avg=2.6221\n",
            "[1056 | 802.81] loss=2.5130 avg=2.5082\n",
            "[1057 | 803.41] loss=2.2764 avg=2.5059\n",
            "[1058 | 804.02] loss=2.0204 avg=2.5010\n",
            "[1059 | 804.63] loss=2.7078 avg=2.5031\n",
            "[1060 | 805.23] loss=2.9576 avg=2.5076\n",
            "[1060 | 805.40] VAL_loss=2.4581 VAL_avg=2.6202\n",
            "[1061 | 806.01] loss=2.5897 avg=2.5085\n",
            "[1062 | 806.61] loss=2.7381 avg=2.5107\n",
            "[1063 | 807.21] loss=2.4533 avg=2.5102\n",
            "[1064 | 807.81] loss=2.6138 avg=2.5112\n",
            "[1065 | 808.41] loss=2.5204 avg=2.5113\n",
            "[1065 | 808.57] VAL_loss=2.7584 VAL_avg=2.6218\n",
            "[1066 | 809.18] loss=2.7112 avg=2.5133\n",
            "[1067 | 809.77] loss=2.5943 avg=2.5141\n",
            "[1068 | 810.37] loss=2.7649 avg=2.5166\n",
            "[1069 | 810.96] loss=2.4169 avg=2.5156\n",
            "[1070 | 811.56] loss=2.4777 avg=2.5152\n",
            "[1070 | 811.73] VAL_loss=2.5655 VAL_avg=2.6211\n",
            "[1071 | 812.33] loss=2.6621 avg=2.5167\n",
            "[1072 | 812.92] loss=2.6740 avg=2.5183\n",
            "[1073 | 813.51] loss=2.5946 avg=2.5190\n",
            "[1074 | 814.11] loss=2.5177 avg=2.5190\n",
            "[1075 | 814.70] loss=2.7142 avg=2.5210\n",
            "[1075 | 814.86] VAL_loss=2.8234 VAL_avg=2.6234\n",
            "[1076 | 815.45] loss=2.4931 avg=2.5207\n",
            "[1077 | 816.05] loss=2.6411 avg=2.5219\n",
            "[1078 | 816.64] loss=2.5651 avg=2.5223\n",
            "[1079 | 817.23] loss=2.2238 avg=2.5194\n",
            "[1080 | 817.82] loss=2.3845 avg=2.5180\n",
            "[1080 | 817.98] VAL_loss=2.5106 VAL_avg=2.6222\n",
            "[1081 | 818.57] loss=2.1241 avg=2.5141\n",
            "[1082 | 819.16] loss=2.1502 avg=2.5104\n",
            "[1083 | 819.75] loss=2.6537 avg=2.5119\n",
            "[1084 | 820.33] loss=2.3496 avg=2.5102\n",
            "[1085 | 820.92] loss=2.3191 avg=2.5083\n",
            "[1085 | 821.09] VAL_loss=2.5516 VAL_avg=2.6214\n",
            "[1086 | 821.67] loss=2.0378 avg=2.5036\n",
            "[1087 | 822.26] loss=2.0525 avg=2.4991\n",
            "[1088 | 822.84] loss=2.6871 avg=2.5010\n",
            "[1089 | 823.42] loss=2.6748 avg=2.5027\n",
            "[1090 | 824.00] loss=2.4478 avg=2.5022\n",
            "[1090 | 824.17] VAL_loss=2.3277 VAL_avg=2.6181\n",
            "[1091 | 824.75] loss=2.2988 avg=2.5001\n",
            "[1092 | 825.34] loss=2.3719 avg=2.4989\n",
            "[1093 | 825.92] loss=2.3115 avg=2.4970\n",
            "[1094 | 826.50] loss=2.5318 avg=2.4973\n",
            "[1095 | 827.08] loss=2.1242 avg=2.4936\n",
            "[1095 | 827.24] VAL_loss=2.5424 VAL_avg=2.6172\n",
            "[1096 | 827.83] loss=2.5543 avg=2.4942\n",
            "[1097 | 828.41] loss=2.7624 avg=2.4969\n",
            "[1098 | 828.99] loss=2.6289 avg=2.4982\n",
            "[1099 | 829.57] loss=2.5848 avg=2.4991\n",
            "[1100 | 830.15] loss=2.6224 avg=2.5003\n",
            "[1100 | 830.31] VAL_loss=2.4589 VAL_avg=2.6154\n",
            "Saving checkpoint/run1/model-1100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUa2LujW82cy",
        "colab_type": "text"
      },
      "source": [
        "The step below simply copies your trained model to the model directory, so the output will use your training. If you don't do this, you will be running against the trained GPT-2 model without your finetuning training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNXhOM22fNHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvtv6NUj5eXU",
        "colab_type": "text"
      },
      "source": [
        "Run the below step to generate unconditional samples (i.e. \"dream mode\").\n",
        "\n",
        "\"top_k\" controls how many options to consider per word (the larger, the more \"diverse\" the output - anything from 1 to about 50 usually works, I think values around 10 are pretty good).\n",
        "\n",
        "\"temperature\" controls the sampling of the words, from 0 to 1 where 1 is the most \"random\".\n",
        "\n",
        "\"length\" controls the number of words in each sample output.\n",
        "\n",
        "This command will run continuously until you turn it off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2csc2bZHfrXd",
        "colab_type": "code",
        "outputId": "dbc68283-3d94-479a-d65e-2b80e65bf962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        }
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --top_k 10 --temperature 1 --length=300 --model_name=345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"src/generate_unconditional_samples.py\", line 7, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 52, in <module>\n",
            "    from tensorflow.core.framework.graph_pb2 import *\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n",
            "    from google.protobuf import descriptor as _descriptor\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/__init__.py\", line 37, in <module>\n",
            "    __import__('pkg_resources').declare_namespace(__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3241, in <module>\n",
            "    @_call_aside\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3225, in _call_aside\n",
            "    f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3277, in _initialize_master_working_set\n",
            "    list(map(working_set.add_entry, sys.path))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 623, in add_entry\n",
            "    for dist in find_distributions(entry, True):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2052, in find_on_path\n",
            "    path_item_entries = _by_version_descending(filtered)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2025, in _by_version_descending\n",
            "    return sorted(names, key=_by_version, reverse=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2021, in _by_version\n",
            "    name, ext = os.path.splitext(name)\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 129, in splitext\n",
            "    return genericpath._splitext(p, sep, None, extsep)\n",
            "  File \"/usr/lib/python3.6/genericpath.py\", line 124, in _splitext\n",
            "    sepIndex = p.rfind(sep)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwZl7JAn6d5y",
        "colab_type": "text"
      },
      "source": [
        "Run the command below to run in interactive / \"completion\" mode. You will get a prompt; just type in whatever prompt text you want, and the model will attempt to complete it \"nsamples\" times.\n",
        "\n",
        "\"top_k\", \"length\", and \"temperature\" work as specified above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugmKkFv__NI_",
        "colab_type": "code",
        "outputId": "6bfd860c-3f9e-43e4-ef85-33248ff6198f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7823
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --top_k 1 --length=30 --temperature 0.1 --nsamples 3 --model_name=345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-08 17:53:50.739097: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-08 17:53:50.739382: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1f36520 executing computations on platform Host. Devices:\n",
            "2019-05-08 17:53:50.739454: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-08 17:53:50.972419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-08 17:53:50.972910: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1f35e40 executing computations on platform CUDA. Devices:\n",
            "2019-05-08 17:53:50.972939: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-08 17:53:50.973272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-08 17:53:50.973294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-08 17:53:52.350214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-08 17:53:52.350270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-08 17:53:52.350282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-08 17:53:52.350546: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-08 17:53:52.350660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> \"C: YOU'RE AN ANIMAL! Q: The giant moray variety of this critter is seen here\"\n",
            "2019-05-08 17:54:06.713321: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "C: YOU'RE AN ANIMAL! Q: The giant moray variety of this critter is seen here\" C: YOU\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "\n",
            "C: YOU'RE AN ANIMAL! Q: The giant moray variety of this critter is seen here\" C: YOU\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "\n",
            "C: YOU'RE AN ANIMAL! Q: The giant moray variety of this critter is seen here\" C: YOU\n",
            "================================================================================\n",
            "Model prompt >>> C: YOU'RE AN ANIMAL! Q: The giant moray variety of this critter is seen here\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: I'm not sure what you mean. Q: The giant moray variety of this critter is seen here. A: I'm\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: I'm not sure what you mean. Q: The giant moray variety of this critter is seen here. A: I'm\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: I'm not sure what you mean. Q: The giant moray variety of this critter is seen here. A: I'm\n",
            "================================================================================\n",
            "Model prompt >>> C: THE FICTIONAL LAND OF OZ Q: The \"\"Wizard\"\" of Oz is from Omaha, Nebraska, not far from this state where Dorothy lives\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: I don't know if he's from Omaha or not. Q: What is the name of the town where Oz lives? A:\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: I don't know if he's from Omaha or not. Q: What is the name of the town where Oz lives? A:\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: I don't know if he's from Omaha or not. Q: What is the name of the town where Oz lives? A:\n",
            "================================================================================\n",
            "Model prompt >>> C: EYE ON AFRICA Q: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "================================================================================\n",
            "Model prompt >>> C: EYE ON AFRICA\\nQ: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\\n\n",
            "======================================== SAMPLE 1 ========================================\n",
            "Q: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\\nQ: On landing\n",
            "======================================== SAMPLE 2 ========================================\n",
            "Q: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\\nQ: On landing\n",
            "======================================== SAMPLE 3 ========================================\n",
            "Q: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\\nQ: On landing\n",
            "================================================================================\n",
            "Model prompt >>> C: EYE ON AFRICA Q: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "================================================================================\n",
            "Model prompt >>> C: RELIGION Q: In 1978, this pope reigned for only 34 days\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What was the reaction of the Catholic Church? A: The reaction was very positive. The pope was very popular. He was very popular with the\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What was the reaction of the Catholic Church? A: The reaction was very positive. The pope was very popular. He was very popular with the\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What was the reaction of the Catholic Church? A: The reaction was very positive. The pope was very popular. He was very popular with the\n",
            "================================================================================\n",
            "Model prompt >>> C: THE TRUTH IS OUT THERE Q: In \"Paradise Regained\", this British epic poet opined, \"Hard are the ways of truth, & rough to walk\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " (p. 5). A: I think that's a very good statement. Q: In \"Paradise Regained\", this British epic poet\n",
            "======================================== SAMPLE 2 ========================================\n",
            " (p. 5). A: I think that's a very good statement. Q: In \"Paradise Regained\", this British epic poet\n",
            "======================================== SAMPLE 3 ========================================\n",
            " (p. 5). A: I think that's a very good statement. Q: In \"Paradise Regained\", this British epic poet\n",
            "================================================================================\n",
            "Model prompt >>> C: THE VIOLIN Q: The finest violins ever made, they bear the Latinized form of their maker's name\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". Q: The finest violins ever made, they bear the Latinized form of their maker's name. Q: The finest violins ever made\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". Q: The finest violins ever made, they bear the Latinized form of their maker's name. Q: The finest violins ever made\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". Q: The finest violins ever made, they bear the Latinized form of their maker's name. Q: The finest violins ever made\n",
            "================================================================================\n",
            "Model prompt >>> C: FOLLOW THE MONEY Q: Nepal, Bhutan, India & this island nation off India all use the rupee as their currency\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What is the difference between the rupee and the US dollar? A: The rupee is a unit of exchange. It is used to buy\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What is the difference between the rupee and the US dollar? A: The rupee is a unit of exchange. It is used to buy\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What is the difference between the rupee and the US dollar? A: The rupee is a unit of exchange. It is used to buy\n",
            "================================================================================\n",
            "Model prompt >>> C: YOUNG NOBEL PRIZE WINNERS Q: This 31-year-old won for the creation of quantum mechanics & not for his uncertainty principle\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: I'm sorry, but I'm not sure that's true. Q: This 31-year-old won for the creation of quantum\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: I'm sorry, but I'm not sure that's true. Q: This 31-year-old won for the creation of quantum\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: I'm sorry, but I'm not sure that's true. Q: This 31-year-old won for the creation of quantum\n",
            "================================================================================\n",
            "Model prompt >>> C: FADS Q: The trademarked version of \"skinny\" these are \"classic denim but with a high nylon/elastane content\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Q: The trademarked version of \"skinny\" these are \"classic denim but with a high nylon/elastane content\" Q: The\n",
            "======================================== SAMPLE 2 ========================================\n",
            " Q: The trademarked version of \"skinny\" these are \"classic denim but with a high nylon/elastane content\" Q: The\n",
            "======================================== SAMPLE 3 ========================================\n",
            " Q: The trademarked version of \"skinny\" these are \"classic denim but with a high nylon/elastane content\" Q: The\n",
            "================================================================================\n",
            "Model prompt >>> C: THE AMERICAN FLAG Q: The number of stars on the American flag\n",
            "======================================== SAMPLE 1 ========================================\n",
            " is a symbol of the nation's commitment to freedom and democracy. A: The American flag is a symbol of freedom and democracy. Q: The flag\n",
            "======================================== SAMPLE 2 ========================================\n",
            " is a symbol of the nation's commitment to freedom and democracy. A: The American flag is a symbol of freedom and democracy. Q: The flag\n",
            "======================================== SAMPLE 3 ========================================\n",
            " is a symbol of the nation's commitment to freedom and democracy. A: The American flag is a symbol of freedom and democracy. Q: The flag\n",
            "================================================================================\n",
            "Model prompt >>> C: AFRICAN CUISINE Q: Doro Wat, stewed chicken over injera bread, is a national dish of this East African country\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: It's a dish that's been around for a long time. It's a dish that's been around for a long time. Q\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: It's a dish that's been around for a long time. It's a dish that's been around for a long time. Q\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: It's a dish that's been around for a long time. It's a dish that's been around for a long time. Q\n",
            "================================================================================\n",
            "Model prompt >>> C: POETS OF SONG Q: This Dylan song asked, \"How many times must the cannonballs fly, before they're forever banned\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " A: \"How many times must the cannonballs fly, before they're forever banned\" Q: This Dylan song asked, \"How many times must\n",
            "======================================== SAMPLE 2 ========================================\n",
            " A: \"How many times must the cannonballs fly, before they're forever banned\" Q: This Dylan song asked, \"How many times must\n",
            "======================================== SAMPLE 3 ========================================\n",
            " A: \"How many times must the cannonballs fly, before they're forever banned\" Q: This Dylan song asked, \"How many times must\n",
            "================================================================================\n",
            "Model prompt >>> C: ISLANDS Q: Almost all the inhabitants of this Chilean island live in the village of Hanga Roa on the west coast\n",
            "======================================== SAMPLE 1 ========================================\n",
            " of the island. They are called the Hanga Roa people. They are a very small people, about the size of a small dog. They\n",
            "======================================== SAMPLE 2 ========================================\n",
            " of the island. They are called the Hanga Roa people. They are a very small people, about the size of a small dog. They\n",
            "======================================== SAMPLE 3 ========================================\n",
            " of the island. They are called the Hanga Roa people. They are a very small people, about the size of a small dog. They\n",
            "================================================================================\n",
            "Model prompt >>> C: WORLD HISTORY Q: In 1920 the League of Nations gave this neighboring country a mandate over the territory of Namibia\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What was the result? A: The Namibian government was overthrown by the British. Q: What was the result of the British invasion\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What was the result? A: The Namibian government was overthrown by the British. Q: What was the result of the British invasion\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What was the result? A: The Namibian government was overthrown by the British. Q: What was the result of the British invasion\n",
            "================================================================================\n",
            "Model prompt >>> C: MY SPACE Q: One measure of this constant in a vacuum is 299,792,458 meters per second\n",
            "======================================== SAMPLE 1 ========================================\n",
            ".\n",
            "\n",
            "Q: What is the speed of light?\n",
            "\n",
            "A: The speed of light is 299,792,458 meters per second.\n",
            "======================================== SAMPLE 2 ========================================\n",
            ".\n",
            "\n",
            "Q: What is the speed of light?\n",
            "\n",
            "A: The speed of light is 299,792,458 meters per second.\n",
            "======================================== SAMPLE 3 ========================================\n",
            ".\n",
            "\n",
            "Q: What is the speed of light?\n",
            "\n",
            "A: The speed of light is 299,792,458 meters per second.\n",
            "================================================================================\n",
            "Model prompt >>> C: MEDICINE Q: Cystitis, an inflammation of this urinary organ, is usually caused by bacteria\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: Cystitis is a condition of the urinary tract. It is caused by a bacterial infection. It is not a disease. Q:\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: Cystitis is a condition of the urinary tract. It is caused by a bacterial infection. It is not a disease. Q:\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: Cystitis is a condition of the urinary tract. It is caused by a bacterial infection. It is not a disease. Q:\n",
            "================================================================================\n",
            "Model prompt >>> C: LEFTIES Q: Born a lefty, this president governed from the right from 1981 to 1989\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". He was a Republican, but he was a Republican from 1981 to 1989. He was a Republican from 1981 to 1989. He was a Republican from\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". He was a Republican, but he was a Republican from 1981 to 1989. He was a Republican from 1981 to 1989. He was a Republican from\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". He was a Republican, but he was a Republican from 1981 to 1989. He was a Republican from 1981 to 1989. He was a Republican from\n",
            "================================================================================\n",
            "Model prompt >>> C: LETTER, PLEASE Q: Females have 2 copies of this chromosome\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "======================================== SAMPLE 2 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "======================================== SAMPLE 3 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "================================================================================\n",
            "Model prompt >>> C: LETTER, PLEASE Q: Females have 2 copies of this chromosome\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "======================================== SAMPLE 2 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "======================================== SAMPLE 3 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "================================================================================\n",
            "Model prompt >>> C: SINGERS Q: \"Everybody Loves Somebody\" & Las Vegas loved this man, dimming the Strip's lights in his honor December 28, 1995\n",
            "======================================== SAMPLE 1 ========================================\n",
            ".\n",
            "\n",
            "A: \"I'm not going to lie, I was a little bit nervous. I was like, 'Oh my God, I\n",
            "======================================== SAMPLE 2 ========================================\n",
            ".\n",
            "\n",
            "A: \"I'm not going to lie, I was a little bit nervous. I was like, 'Oh my God, I\n",
            "======================================== SAMPLE 3 ========================================\n",
            ".\n",
            "\n",
            "A: \"I'm not going to lie, I was a little bit nervous. I was like, 'Oh my God, I\n",
            "================================================================================\n",
            "Model prompt >>> C: CLUB MED Q: This river flows north for more than 4,000 miles before emptying into the Mediterranean through a delta\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Rome to Venice. Q: What's the best way to get there\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Rome to Venice. Q: What's the best way to get there\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Rome to Venice. Q: What's the best way to get there\n",
            "================================================================================\n",
            "Model prompt >>> C: LANGUAGE Q: It is the 2nd most spoken language in the world\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: It is the 2nd most spoken language in the world. Q: What is the most spoken language in the world? A: It\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: It is the 2nd most spoken language in the world. Q: What is the most spoken language in the world? A: It\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: It is the 2nd most spoken language in the world. Q: What is the most spoken language in the world? A: It\n",
            "================================================================================\n",
            "Model prompt >>> C: TROPICANA Q: This tropical island has roads linking Port Antonio, Montego Bay & Kingston\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Port Antonio to Montego Bay. It's about 2 hours. Q\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Port Antonio to Montego Bay. It's about 2 hours. Q\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Port Antonio to Montego Bay. It's about 2 hours. Q\n",
            "================================================================================\n",
            "Model prompt >>> C: OSCAR Q: Prior to winning his Best Director Oscar for \"Platoon\", he won for Best Adapted Screenplay for \"Midnight Express\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " and \"The Theory of Everything\". What was it like to work with him?\n",
            "\n",
            "JH: It was a pleasure. I've known him\n",
            "======================================== SAMPLE 2 ========================================\n",
            " and \"The Theory of Everything\". What was it like to work with him?\n",
            "\n",
            "JH: It was a pleasure. I've known him\n",
            "======================================== SAMPLE 3 ========================================\n",
            " and \"The Theory of Everything\". What was it like to work with him?\n",
            "\n",
            "JH: It was a pleasure. I've known him\n",
            "================================================================================\n",
            "Model prompt >>> C: INCREDIBLE EDIBLES Q: These \"soft-shell\" animals are sometimes prepared & eaten without removing their eyes\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: Yes, they are. Q: What is the difference between a soft-shell and a hard-shell? A: A soft-\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: Yes, they are. Q: What is the difference between a soft-shell and a hard-shell? A: A soft-\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: Yes, they are. Q: What is the difference between a soft-shell and a hard-shell? A: A soft-\n",
            "================================================================================\n",
            "Model prompt >>> C: \"PRINCE\" Q: Satan, Dracula & Ozzy Osbourne go by this nickname\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What's your favorite name? A: \"PRINCE\" Q: What's your favorite song? A: \"I'm Not a Human\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What's your favorite name? A: \"PRINCE\" Q: What's your favorite song? A: \"I'm Not a Human\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What's your favorite name? A: \"PRINCE\" Q: What's your favorite song? A: \"I'm Not a Human\n",
            "================================================================================\n",
            "Model prompt >>> C: THE 50 STATES Q: Its southeast corner touches Arizona, Colorado & New Mexico\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: The southern corner of the state is the southernmost part of the state. Q: What is the southernmost part of the state?\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: The southern corner of the state is the southernmost part of the state. Q: What is the southernmost part of the state?\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: The southern corner of the state is the southernmost part of the state. Q: What is the southernmost part of the state?\n",
            "================================================================================\n",
            "Model prompt >>> C: PROFESSIONS IN SONG Q: Peter Schilling's \"Major Tom\" continues the story of the astronaut in this singer's hit \"Space Oddity\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " (1962). Q: The \"Space Oddity\" song is a classic of the genre. It's a great song. It's a great\n",
            "======================================== SAMPLE 2 ========================================\n",
            " (1962). Q: The \"Space Oddity\" song is a classic of the genre. It's a great song. It's a great\n",
            "======================================== SAMPLE 3 ========================================\n",
            " (1962). Q: The \"Space Oddity\" song is a classic of the genre. It's a great song. It's a great\n",
            "================================================================================\n",
            "Model prompt >>> C: MULTIPLE-WORD CAPITALS Q: This capital of the Dominican Republic is said to contain the remains of Christopher Columbus\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What is the origin of this capital? A: The capital of the Dominican Republic is the capital of the Dominican Republic. Q: What is the\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What is the origin of this capital? A: The capital of the Dominican Republic is the capital of the Dominican Republic. Q: What is the\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What is the origin of this capital? A: The capital of the Dominican Republic is the capital of the Dominican Republic. Q: What is the\n",
            "================================================================================\n",
            "Model prompt >>> C: BEGINS & ENDS WITH \"T\" Q: A male feline\n",
            "======================================== SAMPLE 1 ========================================\n",
            " is a male cat. A female feline is a female cat. A female cat is a female cat. A female cat is a female cat.\n",
            "======================================== SAMPLE 2 ========================================\n",
            " is a male cat. A female feline is a female cat. A female cat is a female cat. A female cat is a female cat.\n",
            "======================================== SAMPLE 3 ========================================\n",
            " is a male cat. A female feline is a female cat. A female cat is a female cat. A female cat is a female cat.\n",
            "================================================================================\n",
            "Model prompt >>> C: 1985 Q: This soft drink co. introduced its new formula drink & then brought back the classic version\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What was the reaction? A: The reaction was very positive. People were very excited about the new formula. Q: What was the reaction?\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What was the reaction? A: The reaction was very positive. People were very excited about the new formula. Q: What was the reaction?\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What was the reaction? A: The reaction was very positive. People were very excited about the new formula. Q: What was the reaction?\n",
            "================================================================================\n",
            "Model prompt >>> C: FOOD & DRINK Q: With the hull left on it's called brown; with it removed it's white & called \"polished\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Q: What is the difference between the two? A: Polished is a more polished finish, and brown is a more polished finish. Q:\n",
            "======================================== SAMPLE 2 ========================================\n",
            " Q: What is the difference between the two? A: Polished is a more polished finish, and brown is a more polished finish. Q:\n",
            "======================================== SAMPLE 3 ========================================\n",
            " Q: What is the difference between the two? A: Polished is a more polished finish, and brown is a more polished finish. Q:\n",
            "================================================================================\n",
            "Model prompt >>> C: COMMUNICATIONS Q: It's waved to indicate surrender or truce\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: It's waved to indicate surrender or truce. Q: It's waved to indicate surrender or truce. A: It's waved to indicate\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: It's waved to indicate surrender or truce. Q: It's waved to indicate surrender or truce. A: It's waved to indicate\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: It's waved to indicate surrender or truce. Q: It's waved to indicate surrender or truce. A: It's waved to indicate\n",
            "================================================================================\n",
            "Model prompt >>> C: GEOLOGY Q: An earthquake with a 7.0 magnitude on this scale releases about 32 times as much energy as a 6.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            " magnitude earthquake. A 7.0 magnitude earthquake releases about 1,000 times as much energy as a 6.0 magnitude earthquake. A 7.0\n",
            "======================================== SAMPLE 2 ========================================\n",
            " magnitude earthquake. A 7.0 magnitude earthquake releases about 1,000 times as much energy as a 6.0 magnitude earthquake. A 7.0\n",
            "======================================== SAMPLE 3 ========================================\n",
            " magnitude earthquake. A 7.0 magnitude earthquake releases about 1,000 times as much energy as a 6.0 magnitude earthquake. A 7.0\n",
            "================================================================================\n",
            "Model prompt >>> C: FRANCE Q: A part-iridium cylinder in France is the standard for this metric unit of mass (about 2.2 pounds)\n",
            "======================================== SAMPLE 1 ========================================\n",
            " and is used in the United States. A part-iridium cylinder is also used in Europe. A part-iridium cylinder is a cylinder of\n",
            "======================================== SAMPLE 2 ========================================\n",
            " and is used in the United States. A part-iridium cylinder is also used in Europe. A part-iridium cylinder is a cylinder of\n",
            "======================================== SAMPLE 3 ========================================\n",
            " and is used in the United States. A part-iridium cylinder is also used in Europe. A part-iridium cylinder is a cylinder of\n",
            "================================================================================\n",
            "Model prompt >>> C: CALL OUT THE BOB SQUAD Q: He was only 15 when he became an international chess grandmaster in 1958\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: He was 15 when he became an international chess grandmaster in 1958. Q: He was only 15 when he became an international chess grand\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: He was 15 when he became an international chess grandmaster in 1958. Q: He was only 15 when he became an international chess grand\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: He was 15 when he became an international chess grandmaster in 1958. Q: He was only 15 when he became an international chess grand\n",
            "================================================================================\n",
            "Model prompt >>> C: AROUND THE WORLD Q: The Amundsen-Scott research base is located at this famous point on Antarctica\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What is the significance of this location? A: The Amundsen-Scott research base is located at this famous point on Antarctica. The base\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What is the significance of this location? A: The Amundsen-Scott research base is located at this famous point on Antarctica. The base\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What is the significance of this location? A: The Amundsen-Scott research base is located at this famous point on Antarctica. The base\n",
            "================================================================================\n",
            "Model prompt >>> C: INDIANA Q: 19th century president who grew to manhood in southern Indiana\n",
            "======================================== SAMPLE 1 ========================================\n",
            ".\n",
            "\n",
            "Q: 20th century president who grew to manhood in southern Indiana.\n",
            "\n",
            "Q: 21st century president who grew to man\n",
            "======================================== SAMPLE 2 ========================================\n",
            ".\n",
            "\n",
            "Q: 20th century president who grew to manhood in southern Indiana.\n",
            "\n",
            "Q: 21st century president who grew to man\n",
            "======================================== SAMPLE 3 ========================================\n",
            ".\n",
            "\n",
            "Q: 20th century president who grew to manhood in southern Indiana.\n",
            "\n",
            "Q: 21st century president who grew to man\n",
            "================================================================================\n",
            "Model prompt >>> C: WORLD CITIES Q: The Universal Postal Union is headquartered in this Swiss capital\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What is the purpose of this organization? A: The Universal Postal Union is a global organization of postal workers, which is responsible for the delivery of\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What is the purpose of this organization? A: The Universal Postal Union is a global organization of postal workers, which is responsible for the delivery of\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What is the purpose of this organization? A: The Universal Postal Union is a global organization of postal workers, which is responsible for the delivery of\n",
            "================================================================================\n",
            "Model prompt >>> C: TIME'S MAN OF THE YEAR Q: This Soviet leader was Time's Man of the Year for 1987 & of the Decade in 1989\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: He was. Q: What was his name? A: He was called \"Time's Man of the Year.\" Q: What was\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: He was. Q: What was his name? A: He was called \"Time's Man of the Year.\" Q: What was\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: He was. Q: What was his name? A: He was called \"Time's Man of the Year.\" Q: What was\n",
            "================================================================================\n",
            "Model prompt >>> C: NATIONAL NAMES Q: Kampuchea\\n\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\\A: Kampuchea\\n\\B: Kampuchea\\n\\C: NATIONAL NAMES Q: Kampuchea\\n\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\\A: Kampuchea\\n\\B: Kampuchea\\n\\C: NATIONAL NAMES Q: Kampuchea\\n\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\\A: Kampuchea\\n\\B: Kampuchea\\n\\C: NATIONAL NAMES Q: Kampuchea\\n\n",
            "================================================================================\n",
            "Model prompt >>> C: NATIONAL NAMES Q: Kampuchea\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q\n",
            "======================================== SAMPLE 2 ========================================\n",
            ", Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q\n",
            "======================================== SAMPLE 3 ========================================\n",
            ", Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5061, in get_controller\n",
            "    yield default\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1573, in __exit__\n",
            "    exec_tb)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 123, in __exit__\n",
            "    if sys.exc_info()[1] is value:\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}