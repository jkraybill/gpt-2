{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Fine Tuning Notebook",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkraybill/gpt-2/blob/finetuning/GPT2-finetuning2-345M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHiwt3Ww7FF6",
        "colab_type": "text"
      },
      "source": [
        "To try out GPT-2, do this:\n",
        "\n",
        "- go to the \"Runtime\" menu and click \"Change runtime type\" and make sure this is a Python 3 notebook, running with GPU hardware acceleration.\n",
        "- use the \"Files\" section to the left to upload a text file called \"corpus.txt\" which contains all the text you want to train on.\n",
        "- run the steps below in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE_fFgQ8a_Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etM-i8RwbcTH",
        "colab_type": "code",
        "outputId": "5ca88241-36ba-4aaf-e7a5-7f3ae808efd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/jkraybill/gpt-2.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 175 (delta 1), reused 0 (delta 0), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (175/175), 4.38 MiB | 6.33 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVTyGrhsbdep",
        "colab_type": "code",
        "outputId": "c6f22267-1335-4d7a-cf10-5c3ddf779ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azu6KCOHbhIy",
        "colab_type": "code",
        "outputId": "12e6f0b9-3e3d-4a2f-bf61-e554393ccf8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.0.18 has requirement regex==2018.01.10, but you'll have regex 2017.4.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex\n",
            "  Found existing installation: regex 2018.1.10\n",
            "    Uninstalling regex-2018.1.10:\n",
            "      Successfully uninstalled regex-2018.1.10\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecmuIWxFgFBz",
        "colab_type": "code",
        "outputId": "7443dbaf-3378-4747-e7da-ffd5dee85900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!sh download_model.sh 345M"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching 345M/checkpoint\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    77  100    77    0     0    216      0 --:--:-- --:--:-- --:--:--   216\n",
            "Fetching 345M/encoder.json\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1017k  100 1017k    0     0  3011k      0 --:--:-- --:--:-- --:--:-- 3002k\n",
            "Fetching 345M/hparams.json\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    91  100    91    0     0    330      0 --:--:-- --:--:-- --:--:--   330\n",
            "Fetching 345M/model.ckpt.data-00000-of-00001\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1353M  100 1353M    0     0  64.3M      0  0:00:21  0:00:21 --:--:-- 69.0M\n",
            "Fetching 345M/model.ckpt.index\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10399  100 10399    0     0  36360      0 --:--:-- --:--:-- --:--:-- 36360\n",
            "Fetching 345M/model.ckpt.meta\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  904k  100  904k    0     0   931k      0 --:--:-- --:--:-- --:--:--  930k\n",
            "Fetching 345M/vocab.bpe\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  445k  100  445k    0     0  1896k      0 --:--:-- --:--:-- --:--:-- 1888k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OhJ6ka_DD94",
        "colab_type": "text"
      },
      "source": [
        "The below step encodes your corpus into \"NPZ\" tokenized format for GPT-2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngS1K3pTKWlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 755 encode345.py\n",
        "!chmod 755 train345.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHsNeMNenxR",
        "colab_type": "code",
        "outputId": "01932697-d083-4d9d-c4b8-ca00461023ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "!PYTHONPATH=src ./encode345.py --in-text ../corpus.txt --out-npz corpus.txt.npz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "\r  0% 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94oLzOq23474",
        "colab_type": "text"
      },
      "source": [
        "Training is below. I usually get usable results with \"stop_after\" anywhere from 800 to 3000, but you can try going even higher. 800 steps takes only a few minutes.\n",
        "\n",
        "\"sample_every\" controls how often you get sample output from the trained model.\n",
        "\n",
        "\"save_every\" controls how often the model is saved.\n",
        "\n",
        "\"learning_rate\" is the AI learning rate. 0.00005 is the rate I've gotten the best results with, but I think most people are running with significantly higher rates, so you could try adjusting it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbBJ6JoufBGQ",
        "colab_type": "code",
        "outputId": "a27105ae-3026-407c-aaeb-26cdcfcde713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17734
        }
      },
      "source": [
        "!PYTHONPATH=src ./train345.py --dataset corpus.txt.npz --sample_every=1000 --save_every=1000 --learning_rate=0.00005 --stop_after=60000\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-08 16:58:23.929276: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-08 16:58:23.929605: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1ef8520 executing computations on platform Host. Devices:\n",
            "2019-05-08 16:58:23.929641: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-08 16:58:24.203605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-08 16:58:24.204256: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1ef7e40 executing computations on platform CUDA. Devices:\n",
            "2019-05-08 16:58:24.204289: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-08 16:58:24.204641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-08 16:58:24.204663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-08 16:58:25.622703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-08 16:58:25.622781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-08 16:58:25.622793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-08 16:58:25.623168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "Reading corpus.txt.npz\n",
            "dataset has 1458292 tokens\n",
            "Training...\n",
            "2019-05-08 16:59:14.379200: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "[1 | 9.51] loss=3.31 avg=3.31\n",
            "[2 | 10.69] loss=3.34 avg=3.32\n",
            "[3 | 11.86] loss=3.18 avg=3.27\n",
            "[4 | 13.04] loss=3.00 avg=3.21\n",
            "[5 | 14.21] loss=3.07 avg=3.18\n",
            "[6 | 15.39] loss=2.97 avg=3.14\n",
            "[7 | 16.57] loss=3.00 avg=3.12\n",
            "[8 | 17.75] loss=2.94 avg=3.10\n",
            "[9 | 18.94] loss=2.96 avg=3.08\n",
            "[10 | 20.12] loss=2.88 avg=3.06\n",
            "[11 | 21.30] loss=2.95 avg=3.05\n",
            "[12 | 22.49] loss=2.85 avg=3.03\n",
            "[13 | 23.68] loss=2.88 avg=3.02\n",
            "[14 | 24.87] loss=2.81 avg=3.00\n",
            "[15 | 26.05] loss=2.75 avg=2.99\n",
            "[16 | 27.24] loss=2.85 avg=2.98\n",
            "[17 | 28.43] loss=2.75 avg=2.96\n",
            "[18 | 29.62] loss=2.70 avg=2.95\n",
            "[19 | 30.82] loss=2.81 avg=2.94\n",
            "[20 | 32.01] loss=2.80 avg=2.93\n",
            "[21 | 33.21] loss=2.89 avg=2.93\n",
            "[22 | 34.39] loss=2.73 avg=2.92\n",
            "[23 | 35.59] loss=2.75 avg=2.91\n",
            "[24 | 36.78] loss=2.82 avg=2.91\n",
            "[25 | 37.98] loss=2.73 avg=2.90\n",
            "[26 | 39.18] loss=2.79 avg=2.89\n",
            "[27 | 40.38] loss=2.91 avg=2.89\n",
            "[28 | 41.58] loss=2.90 avg=2.90\n",
            "[29 | 42.77] loss=2.72 avg=2.89\n",
            "[30 | 43.97] loss=2.90 avg=2.89\n",
            "[31 | 45.17] loss=2.69 avg=2.88\n",
            "[32 | 46.37] loss=2.78 avg=2.88\n",
            "[33 | 47.57] loss=3.04 avg=2.88\n",
            "[34 | 48.78] loss=2.85 avg=2.88\n",
            "[35 | 49.98] loss=2.54 avg=2.87\n",
            "[36 | 51.19] loss=2.88 avg=2.87\n",
            "[37 | 52.39] loss=2.71 avg=2.87\n",
            "[38 | 53.60] loss=2.74 avg=2.86\n",
            "[39 | 54.81] loss=2.83 avg=2.86\n",
            "[40 | 56.02] loss=2.78 avg=2.86\n",
            "[41 | 57.22] loss=2.51 avg=2.85\n",
            "[42 | 58.43] loss=2.81 avg=2.85\n",
            "[43 | 59.64] loss=2.57 avg=2.84\n",
            "[44 | 60.85] loss=2.69 avg=2.83\n",
            "[45 | 62.07] loss=2.65 avg=2.83\n",
            "[46 | 63.28] loss=2.79 avg=2.83\n",
            "[47 | 64.50] loss=2.69 avg=2.83\n",
            "[48 | 65.71] loss=2.58 avg=2.82\n",
            "[49 | 66.93] loss=2.81 avg=2.82\n",
            "[50 | 68.15] loss=2.45 avg=2.81\n",
            "[51 | 69.37] loss=2.66 avg=2.81\n",
            "[52 | 70.58] loss=2.74 avg=2.80\n",
            "[53 | 71.80] loss=2.74 avg=2.80\n",
            "[54 | 73.01] loss=2.70 avg=2.80\n",
            "[55 | 74.23] loss=2.66 avg=2.80\n",
            "[56 | 75.45] loss=2.73 avg=2.79\n",
            "[57 | 76.67] loss=2.73 avg=2.79\n",
            "[58 | 77.89] loss=2.69 avg=2.79\n",
            "[59 | 79.11] loss=2.61 avg=2.79\n",
            "[60 | 80.33] loss=2.70 avg=2.78\n",
            "[61 | 81.55] loss=2.53 avg=2.78\n",
            "[62 | 82.77] loss=2.72 avg=2.78\n",
            "[63 | 84.00] loss=2.55 avg=2.77\n",
            "[64 | 85.23] loss=2.69 avg=2.77\n",
            "[65 | 86.45] loss=2.78 avg=2.77\n",
            "[66 | 87.67] loss=2.59 avg=2.77\n",
            "[67 | 88.89] loss=2.79 avg=2.77\n",
            "[68 | 90.12] loss=2.64 avg=2.77\n",
            "[69 | 91.34] loss=2.97 avg=2.77\n",
            "[70 | 92.57] loss=2.71 avg=2.77\n",
            "[71 | 93.79] loss=2.67 avg=2.77\n",
            "[72 | 95.02] loss=2.25 avg=2.76\n",
            "[73 | 96.25] loss=2.59 avg=2.75\n",
            "[74 | 97.48] loss=2.67 avg=2.75\n",
            "[75 | 98.71] loss=2.63 avg=2.75\n",
            "[76 | 99.94] loss=2.59 avg=2.75\n",
            "[77 | 101.18] loss=2.80 avg=2.75\n",
            "[78 | 102.40] loss=2.80 avg=2.75\n",
            "[79 | 103.64] loss=3.07 avg=2.75\n",
            "[80 | 104.88] loss=2.72 avg=2.75\n",
            "[81 | 106.11] loss=2.60 avg=2.75\n",
            "[82 | 107.34] loss=2.57 avg=2.75\n",
            "[83 | 108.58] loss=2.50 avg=2.74\n",
            "[84 | 109.82] loss=2.78 avg=2.74\n",
            "[85 | 111.05] loss=2.66 avg=2.74\n",
            "[86 | 112.29] loss=2.66 avg=2.74\n",
            "[87 | 113.52] loss=2.75 avg=2.74\n",
            "[88 | 114.76] loss=2.68 avg=2.74\n",
            "[89 | 116.00] loss=2.70 avg=2.74\n",
            "[90 | 117.23] loss=2.74 avg=2.74\n",
            "[91 | 118.47] loss=2.78 avg=2.74\n",
            "[92 | 119.71] loss=2.57 avg=2.74\n",
            "[93 | 120.95] loss=2.70 avg=2.74\n",
            "[94 | 122.19] loss=2.64 avg=2.74\n",
            "[95 | 123.44] loss=2.69 avg=2.73\n",
            "[96 | 124.68] loss=2.77 avg=2.74\n",
            "[97 | 125.92] loss=2.64 avg=2.73\n",
            "[98 | 127.16] loss=2.25 avg=2.73\n",
            "[99 | 128.41] loss=2.64 avg=2.72\n",
            "[100 | 129.65] loss=2.67 avg=2.72\n",
            "[101 | 130.89] loss=2.46 avg=2.72\n",
            "[102 | 132.14] loss=2.77 avg=2.72\n",
            "[103 | 133.39] loss=2.67 avg=2.72\n",
            "[104 | 134.64] loss=2.56 avg=2.72\n",
            "[105 | 135.89] loss=2.63 avg=2.72\n",
            "[106 | 137.13] loss=2.59 avg=2.71\n",
            "[107 | 138.38] loss=2.67 avg=2.71\n",
            "[108 | 139.63] loss=2.64 avg=2.71\n",
            "[109 | 140.88] loss=2.44 avg=2.71\n",
            "[110 | 142.13] loss=2.58 avg=2.71\n",
            "[111 | 143.38] loss=2.50 avg=2.70\n",
            "[112 | 144.63] loss=2.56 avg=2.70\n",
            "[113 | 145.88] loss=2.59 avg=2.70\n",
            "[114 | 147.14] loss=2.62 avg=2.70\n",
            "[115 | 148.39] loss=2.71 avg=2.70\n",
            "[116 | 149.64] loss=2.70 avg=2.70\n",
            "[117 | 150.90] loss=2.62 avg=2.70\n",
            "[118 | 152.15] loss=2.19 avg=2.69\n",
            "[119 | 153.41] loss=2.64 avg=2.69\n",
            "[120 | 154.66] loss=2.67 avg=2.69\n",
            "[121 | 155.92] loss=2.56 avg=2.69\n",
            "[122 | 157.17] loss=2.68 avg=2.69\n",
            "[123 | 158.43] loss=2.38 avg=2.68\n",
            "[124 | 159.68] loss=2.64 avg=2.68\n",
            "[125 | 160.93] loss=2.63 avg=2.68\n",
            "[126 | 162.19] loss=2.71 avg=2.68\n",
            "[127 | 163.45] loss=2.58 avg=2.68\n",
            "[128 | 164.71] loss=2.62 avg=2.68\n",
            "[129 | 165.97] loss=2.67 avg=2.68\n",
            "[130 | 167.23] loss=2.69 avg=2.68\n",
            "[131 | 168.49] loss=2.39 avg=2.68\n",
            "[132 | 169.75] loss=2.53 avg=2.67\n",
            "[133 | 171.01] loss=2.68 avg=2.67\n",
            "[134 | 172.27] loss=2.74 avg=2.67\n",
            "[135 | 173.53] loss=2.61 avg=2.67\n",
            "[136 | 174.79] loss=2.67 avg=2.67\n",
            "[137 | 176.06] loss=2.82 avg=2.68\n",
            "[138 | 177.32] loss=2.25 avg=2.67\n",
            "[139 | 178.58] loss=2.46 avg=2.67\n",
            "[140 | 179.84] loss=2.74 avg=2.67\n",
            "[141 | 181.10] loss=2.47 avg=2.67\n",
            "[142 | 182.37] loss=2.59 avg=2.66\n",
            "[143 | 183.64] loss=2.28 avg=2.66\n",
            "[144 | 184.90] loss=2.64 avg=2.66\n",
            "[145 | 186.17] loss=2.60 avg=2.66\n",
            "[146 | 187.44] loss=2.57 avg=2.66\n",
            "[147 | 188.71] loss=2.56 avg=2.66\n",
            "[148 | 189.97] loss=2.53 avg=2.65\n",
            "[149 | 191.24] loss=2.57 avg=2.65\n",
            "[150 | 192.51] loss=2.68 avg=2.65\n",
            "[151 | 193.77] loss=2.68 avg=2.65\n",
            "[152 | 195.04] loss=2.51 avg=2.65\n",
            "[153 | 196.31] loss=2.71 avg=2.65\n",
            "[154 | 197.57] loss=2.57 avg=2.65\n",
            "[155 | 198.84] loss=2.57 avg=2.65\n",
            "[156 | 200.11] loss=2.54 avg=2.65\n",
            "[157 | 201.38] loss=2.63 avg=2.65\n",
            "[158 | 202.65] loss=2.61 avg=2.65\n",
            "[159 | 203.92] loss=2.30 avg=2.64\n",
            "[160 | 205.19] loss=2.67 avg=2.64\n",
            "[161 | 206.47] loss=2.58 avg=2.64\n",
            "[162 | 207.74] loss=2.59 avg=2.64\n",
            "[163 | 209.02] loss=2.52 avg=2.64\n",
            "[164 | 210.29] loss=2.50 avg=2.64\n",
            "[165 | 211.56] loss=2.68 avg=2.64\n",
            "[166 | 212.83] loss=2.64 avg=2.64\n",
            "[167 | 214.10] loss=2.54 avg=2.64\n",
            "[168 | 215.38] loss=2.42 avg=2.64\n",
            "[169 | 216.65] loss=2.62 avg=2.64\n",
            "[170 | 217.92] loss=2.60 avg=2.64\n",
            "[171 | 219.19] loss=2.63 avg=2.64\n",
            "[172 | 220.47] loss=2.80 avg=2.64\n",
            "[173 | 221.74] loss=2.65 avg=2.64\n",
            "[174 | 223.01] loss=2.76 avg=2.64\n",
            "[175 | 224.29] loss=2.64 avg=2.64\n",
            "[176 | 225.56] loss=2.20 avg=2.63\n",
            "[177 | 226.83] loss=2.55 avg=2.63\n",
            "[178 | 228.11] loss=2.56 avg=2.63\n",
            "[179 | 229.39] loss=2.47 avg=2.63\n",
            "[180 | 230.67] loss=2.58 avg=2.63\n",
            "[181 | 231.94] loss=2.73 avg=2.63\n",
            "[182 | 233.22] loss=2.54 avg=2.63\n",
            "[183 | 234.49] loss=2.68 avg=2.63\n",
            "[184 | 235.77] loss=2.59 avg=2.63\n",
            "[185 | 237.05] loss=2.38 avg=2.63\n",
            "[186 | 238.32] loss=2.58 avg=2.63\n",
            "[187 | 239.61] loss=2.48 avg=2.62\n",
            "[188 | 240.89] loss=2.46 avg=2.62\n",
            "[189 | 242.17] loss=2.62 avg=2.62\n",
            "[190 | 243.45] loss=2.75 avg=2.62\n",
            "[191 | 244.73] loss=2.54 avg=2.62\n",
            "[192 | 246.01] loss=2.29 avg=2.62\n",
            "[193 | 247.29] loss=2.62 avg=2.62\n",
            "[194 | 248.57] loss=2.59 avg=2.62\n",
            "[195 | 249.85] loss=2.45 avg=2.62\n",
            "[196 | 251.12] loss=2.35 avg=2.61\n",
            "[197 | 252.40] loss=2.69 avg=2.61\n",
            "[198 | 253.68] loss=2.53 avg=2.61\n",
            "[199 | 254.96] loss=2.45 avg=2.61\n",
            "[200 | 256.24] loss=2.46 avg=2.61\n",
            "[201 | 257.52] loss=2.47 avg=2.61\n",
            "[202 | 258.80] loss=2.49 avg=2.61\n",
            "[203 | 260.08] loss=2.74 avg=2.61\n",
            "[204 | 261.36] loss=2.61 avg=2.61\n",
            "[205 | 262.64] loss=2.00 avg=2.60\n",
            "[206 | 263.92] loss=2.53 avg=2.60\n",
            "[207 | 265.21] loss=2.66 avg=2.60\n",
            "[208 | 266.49] loss=2.57 avg=2.60\n",
            "[209 | 267.77] loss=2.62 avg=2.60\n",
            "[210 | 269.05] loss=2.66 avg=2.60\n",
            "[211 | 270.32] loss=2.36 avg=2.60\n",
            "[212 | 271.59] loss=2.76 avg=2.60\n",
            "[213 | 272.86] loss=2.65 avg=2.60\n",
            "[214 | 274.14] loss=2.55 avg=2.60\n",
            "[215 | 275.41] loss=2.45 avg=2.60\n",
            "[216 | 276.68] loss=2.59 avg=2.60\n",
            "[217 | 277.95] loss=2.60 avg=2.60\n",
            "[218 | 279.23] loss=2.81 avg=2.60\n",
            "[219 | 280.50] loss=2.31 avg=2.60\n",
            "[220 | 281.78] loss=2.72 avg=2.60\n",
            "[221 | 283.05] loss=2.58 avg=2.60\n",
            "[222 | 284.33] loss=2.59 avg=2.60\n",
            "[223 | 285.60] loss=2.35 avg=2.60\n",
            "[224 | 286.88] loss=2.47 avg=2.60\n",
            "[225 | 288.15] loss=2.56 avg=2.59\n",
            "[226 | 289.42] loss=2.54 avg=2.59\n",
            "[227 | 290.70] loss=2.65 avg=2.59\n",
            "[228 | 291.97] loss=2.64 avg=2.60\n",
            "[229 | 293.24] loss=2.33 avg=2.59\n",
            "[230 | 294.51] loss=2.48 avg=2.59\n",
            "[231 | 295.79] loss=2.73 avg=2.59\n",
            "[232 | 297.07] loss=2.13 avg=2.59\n",
            "[233 | 298.34] loss=2.52 avg=2.59\n",
            "[234 | 299.62] loss=2.43 avg=2.59\n",
            "[235 | 300.89] loss=2.72 avg=2.59\n",
            "[236 | 302.16] loss=2.57 avg=2.59\n",
            "[237 | 303.44] loss=2.61 avg=2.59\n",
            "[238 | 304.71] loss=2.60 avg=2.59\n",
            "[239 | 305.99] loss=2.51 avg=2.59\n",
            "[240 | 307.27] loss=2.55 avg=2.59\n",
            "[241 | 308.55] loss=2.58 avg=2.59\n",
            "[242 | 309.82] loss=2.16 avg=2.58\n",
            "[243 | 311.10] loss=2.54 avg=2.58\n",
            "[244 | 312.37] loss=2.49 avg=2.58\n",
            "[245 | 313.65] loss=2.57 avg=2.58\n",
            "[246 | 314.92] loss=2.63 avg=2.58\n",
            "[247 | 316.20] loss=2.72 avg=2.58\n",
            "[248 | 317.48] loss=2.62 avg=2.58\n",
            "[249 | 318.75] loss=2.30 avg=2.58\n",
            "[250 | 320.03] loss=2.57 avg=2.58\n",
            "[251 | 321.31] loss=2.50 avg=2.58\n",
            "[252 | 322.59] loss=2.41 avg=2.58\n",
            "[253 | 323.87] loss=2.49 avg=2.58\n",
            "[254 | 325.14] loss=2.50 avg=2.57\n",
            "[255 | 326.42] loss=2.74 avg=2.58\n",
            "[256 | 327.70] loss=2.56 avg=2.58\n",
            "[257 | 328.98] loss=2.41 avg=2.57\n",
            "[258 | 330.25] loss=2.51 avg=2.57\n",
            "[259 | 331.53] loss=2.54 avg=2.57\n",
            "[260 | 332.81] loss=2.26 avg=2.57\n",
            "[261 | 334.08] loss=2.60 avg=2.57\n",
            "[262 | 335.36] loss=2.42 avg=2.57\n",
            "[263 | 336.65] loss=2.38 avg=2.57\n",
            "[264 | 337.93] loss=2.41 avg=2.56\n",
            "[265 | 339.20] loss=2.12 avg=2.56\n",
            "[266 | 340.48] loss=2.28 avg=2.56\n",
            "[267 | 341.76] loss=2.40 avg=2.56\n",
            "[268 | 343.04] loss=2.53 avg=2.56\n",
            "[269 | 344.31] loss=2.55 avg=2.55\n",
            "[270 | 345.59] loss=2.61 avg=2.56\n",
            "[271 | 346.87] loss=2.72 avg=2.56\n",
            "[272 | 348.14] loss=2.22 avg=2.55\n",
            "[273 | 349.42] loss=2.27 avg=2.55\n",
            "[274 | 350.70] loss=2.52 avg=2.55\n",
            "[275 | 351.98] loss=2.60 avg=2.55\n",
            "[276 | 353.25] loss=2.29 avg=2.55\n",
            "[277 | 354.53] loss=2.45 avg=2.55\n",
            "[278 | 355.81] loss=2.18 avg=2.54\n",
            "[279 | 357.09] loss=2.43 avg=2.54\n",
            "[280 | 358.36] loss=2.68 avg=2.54\n",
            "[281 | 359.64] loss=2.67 avg=2.54\n",
            "[282 | 360.92] loss=2.55 avg=2.54\n",
            "[283 | 362.20] loss=2.41 avg=2.54\n",
            "[284 | 363.47] loss=2.59 avg=2.54\n",
            "[285 | 364.75] loss=2.66 avg=2.55\n",
            "[286 | 366.02] loss=2.54 avg=2.55\n",
            "[287 | 367.30] loss=2.39 avg=2.54\n",
            "[288 | 368.57] loss=2.57 avg=2.54\n",
            "[289 | 369.85] loss=2.53 avg=2.54\n",
            "[290 | 371.13] loss=2.62 avg=2.54\n",
            "[291 | 372.40] loss=2.72 avg=2.55\n",
            "[292 | 373.67] loss=2.47 avg=2.55\n",
            "[293 | 374.95] loss=2.66 avg=2.55\n",
            "[294 | 376.23] loss=2.64 avg=2.55\n",
            "[295 | 377.50] loss=2.61 avg=2.55\n",
            "[296 | 378.78] loss=2.61 avg=2.55\n",
            "[297 | 380.05] loss=2.67 avg=2.55\n",
            "[298 | 381.33] loss=2.56 avg=2.55\n",
            "[299 | 382.60] loss=1.97 avg=2.54\n",
            "[300 | 383.88] loss=2.55 avg=2.54\n",
            "[301 | 385.16] loss=2.24 avg=2.54\n",
            "[302 | 386.43] loss=2.49 avg=2.54\n",
            "[303 | 387.71] loss=2.72 avg=2.54\n",
            "[304 | 388.99] loss=2.22 avg=2.54\n",
            "[305 | 390.27] loss=2.69 avg=2.54\n",
            "[306 | 391.55] loss=2.63 avg=2.54\n",
            "[307 | 392.83] loss=2.41 avg=2.54\n",
            "[308 | 394.11] loss=2.61 avg=2.54\n",
            "[309 | 395.39] loss=2.65 avg=2.54\n",
            "[310 | 396.67] loss=1.96 avg=2.54\n",
            "[311 | 397.95] loss=2.59 avg=2.54\n",
            "[312 | 399.23] loss=2.65 avg=2.54\n",
            "[313 | 400.50] loss=2.69 avg=2.54\n",
            "[314 | 401.78] loss=1.98 avg=2.53\n",
            "[315 | 403.05] loss=2.58 avg=2.53\n",
            "[316 | 404.33] loss=2.47 avg=2.53\n",
            "[317 | 405.61] loss=2.41 avg=2.53\n",
            "[318 | 406.89] loss=2.50 avg=2.53\n",
            "[319 | 408.17] loss=2.65 avg=2.53\n",
            "[320 | 409.45] loss=2.57 avg=2.53\n",
            "[321 | 410.73] loss=2.40 avg=2.53\n",
            "[322 | 412.01] loss=2.64 avg=2.53\n",
            "[323 | 413.28] loss=2.44 avg=2.53\n",
            "[324 | 414.56] loss=2.47 avg=2.53\n",
            "[325 | 415.84] loss=2.22 avg=2.53\n",
            "[326 | 417.12] loss=2.45 avg=2.53\n",
            "[327 | 418.40] loss=2.19 avg=2.52\n",
            "[328 | 419.68] loss=2.40 avg=2.52\n",
            "[329 | 420.96] loss=2.44 avg=2.52\n",
            "[330 | 422.24] loss=2.51 avg=2.52\n",
            "[331 | 423.51] loss=2.64 avg=2.52\n",
            "[332 | 424.79] loss=2.24 avg=2.52\n",
            "[333 | 426.07] loss=2.59 avg=2.52\n",
            "[334 | 427.35] loss=2.52 avg=2.52\n",
            "[335 | 428.63] loss=2.39 avg=2.52\n",
            "[336 | 429.91] loss=2.51 avg=2.52\n",
            "[337 | 431.20] loss=2.66 avg=2.52\n",
            "[338 | 432.48] loss=2.59 avg=2.52\n",
            "[339 | 433.76] loss=2.55 avg=2.52\n",
            "[340 | 435.04] loss=2.32 avg=2.52\n",
            "[341 | 436.32] loss=2.60 avg=2.52\n",
            "[342 | 437.60] loss=2.72 avg=2.52\n",
            "[343 | 438.88] loss=2.36 avg=2.52\n",
            "[344 | 440.16] loss=2.14 avg=2.52\n",
            "[345 | 441.45] loss=2.54 avg=2.52\n",
            "[346 | 442.73] loss=2.41 avg=2.52\n",
            "[347 | 444.01] loss=2.49 avg=2.52\n",
            "[348 | 445.29] loss=2.69 avg=2.52\n",
            "[349 | 446.58] loss=2.57 avg=2.52\n",
            "[350 | 447.86] loss=2.58 avg=2.52\n",
            "[351 | 449.14] loss=2.64 avg=2.52\n",
            "[352 | 450.43] loss=2.57 avg=2.52\n",
            "[353 | 451.71] loss=2.44 avg=2.52\n",
            "[354 | 452.99] loss=2.51 avg=2.52\n",
            "[355 | 454.27] loss=2.27 avg=2.52\n",
            "[356 | 455.55] loss=2.67 avg=2.52\n",
            "[357 | 456.83] loss=2.66 avg=2.52\n",
            "[358 | 458.12] loss=2.56 avg=2.52\n",
            "[359 | 459.40] loss=2.63 avg=2.52\n",
            "[360 | 460.68] loss=2.46 avg=2.52\n",
            "[361 | 461.97] loss=2.39 avg=2.52\n",
            "[362 | 463.24] loss=2.32 avg=2.52\n",
            "[363 | 464.53] loss=2.48 avg=2.52\n",
            "[364 | 465.82] loss=2.61 avg=2.52\n",
            "[365 | 467.10] loss=2.41 avg=2.52\n",
            "[366 | 468.38] loss=2.62 avg=2.52\n",
            "[367 | 469.66] loss=2.58 avg=2.52\n",
            "[368 | 470.95] loss=2.62 avg=2.52\n",
            "[369 | 472.23] loss=2.53 avg=2.52\n",
            "[370 | 473.52] loss=2.11 avg=2.52\n",
            "[371 | 474.80] loss=2.38 avg=2.51\n",
            "[372 | 476.08] loss=2.51 avg=2.51\n",
            "[373 | 477.37] loss=2.60 avg=2.51\n",
            "[374 | 478.66] loss=2.36 avg=2.51\n",
            "[375 | 479.94] loss=2.53 avg=2.51\n",
            "[376 | 481.22] loss=2.66 avg=2.51\n",
            "[377 | 482.51] loss=2.57 avg=2.52\n",
            "[378 | 483.79] loss=2.63 avg=2.52\n",
            "[379 | 485.07] loss=2.46 avg=2.52\n",
            "[380 | 486.35] loss=2.52 avg=2.52\n",
            "[381 | 487.64] loss=2.61 avg=2.52\n",
            "[382 | 488.92] loss=2.19 avg=2.51\n",
            "[383 | 490.21] loss=2.54 avg=2.51\n",
            "[384 | 491.49] loss=2.59 avg=2.51\n",
            "[385 | 492.77] loss=2.66 avg=2.52\n",
            "[386 | 494.06] loss=2.64 avg=2.52\n",
            "[387 | 495.34] loss=2.64 avg=2.52\n",
            "[388 | 496.63] loss=2.41 avg=2.52\n",
            "[389 | 497.91] loss=2.24 avg=2.51\n",
            "[390 | 499.19] loss=2.38 avg=2.51\n",
            "[391 | 500.48] loss=1.92 avg=2.51\n",
            "[392 | 501.76] loss=2.56 avg=2.51\n",
            "[393 | 503.05] loss=2.02 avg=2.50\n",
            "[394 | 504.33] loss=2.16 avg=2.50\n",
            "[395 | 505.61] loss=2.71 avg=2.50\n",
            "[396 | 506.90] loss=2.63 avg=2.50\n",
            "[397 | 508.18] loss=2.57 avg=2.50\n",
            "[398 | 509.47] loss=2.08 avg=2.50\n",
            "[399 | 510.76] loss=2.45 avg=2.50\n",
            "[400 | 512.04] loss=2.48 avg=2.50\n",
            "[401 | 513.32] loss=2.35 avg=2.50\n",
            "[402 | 514.61] loss=2.30 avg=2.50\n",
            "[403 | 515.90] loss=2.47 avg=2.49\n",
            "[404 | 517.18] loss=1.93 avg=2.49\n",
            "[405 | 518.47] loss=2.20 avg=2.49\n",
            "[406 | 519.76] loss=2.66 avg=2.49\n",
            "[407 | 521.04] loss=2.33 avg=2.49\n",
            "[408 | 522.33] loss=2.39 avg=2.49\n",
            "[409 | 523.61] loss=2.51 avg=2.49\n",
            "[410 | 524.89] loss=2.08 avg=2.48\n",
            "[411 | 526.18] loss=2.43 avg=2.48\n",
            "[412 | 527.46] loss=2.35 avg=2.48\n",
            "[413 | 528.75] loss=2.03 avg=2.48\n",
            "[414 | 530.04] loss=2.47 avg=2.48\n",
            "[415 | 531.31] loss=2.66 avg=2.48\n",
            "[416 | 532.60] loss=2.64 avg=2.48\n",
            "[417 | 533.89] loss=2.69 avg=2.48\n",
            "[418 | 535.17] loss=2.65 avg=2.48\n",
            "[419 | 536.46] loss=2.47 avg=2.48\n",
            "[420 | 537.74] loss=2.37 avg=2.48\n",
            "[421 | 539.03] loss=2.42 avg=2.48\n",
            "[422 | 540.31] loss=2.40 avg=2.48\n",
            "[423 | 541.60] loss=2.57 avg=2.48\n",
            "[424 | 542.88] loss=2.49 avg=2.48\n",
            "[425 | 544.17] loss=2.61 avg=2.48\n",
            "[426 | 545.45] loss=2.62 avg=2.48\n",
            "[427 | 546.73] loss=2.10 avg=2.48\n",
            "[428 | 548.01] loss=2.51 avg=2.48\n",
            "[429 | 549.30] loss=1.87 avg=2.47\n",
            "[430 | 550.58] loss=2.49 avg=2.47\n",
            "[431 | 551.86] loss=2.28 avg=2.47\n",
            "[432 | 553.14] loss=2.61 avg=2.47\n",
            "[433 | 554.43] loss=2.04 avg=2.47\n",
            "[434 | 555.71] loss=2.53 avg=2.47\n",
            "[435 | 556.99] loss=2.20 avg=2.47\n",
            "[436 | 558.28] loss=2.11 avg=2.46\n",
            "[437 | 559.56] loss=2.58 avg=2.46\n",
            "[438 | 560.85] loss=2.03 avg=2.46\n",
            "[439 | 562.13] loss=2.48 avg=2.46\n",
            "[440 | 563.42] loss=2.37 avg=2.46\n",
            "[441 | 564.70] loss=2.56 avg=2.46\n",
            "[442 | 565.99] loss=2.61 avg=2.46\n",
            "[443 | 567.27] loss=2.50 avg=2.46\n",
            "[444 | 568.56] loss=2.64 avg=2.46\n",
            "[445 | 569.84] loss=2.57 avg=2.47\n",
            "[446 | 571.13] loss=2.27 avg=2.46\n",
            "[447 | 572.41] loss=2.44 avg=2.46\n",
            "[448 | 573.70] loss=2.53 avg=2.46\n",
            "[449 | 574.98] loss=2.70 avg=2.47\n",
            "[450 | 576.27] loss=2.56 avg=2.47\n",
            "[451 | 577.55] loss=2.62 avg=2.47\n",
            "[452 | 578.84] loss=2.36 avg=2.47\n",
            "[453 | 580.12] loss=2.36 avg=2.47\n",
            "[454 | 581.41] loss=2.57 avg=2.47\n",
            "[455 | 582.70] loss=2.16 avg=2.46\n",
            "[456 | 583.98] loss=2.54 avg=2.47\n",
            "[457 | 585.27] loss=2.14 avg=2.46\n",
            "[458 | 586.55] loss=2.52 avg=2.46\n",
            "[459 | 587.84] loss=2.36 avg=2.46\n",
            "[460 | 589.12] loss=2.76 avg=2.46\n",
            "[461 | 590.41] loss=2.50 avg=2.46\n",
            "[462 | 591.69] loss=2.29 avg=2.46\n",
            "[463 | 592.98] loss=2.07 avg=2.46\n",
            "[464 | 594.26] loss=2.53 avg=2.46\n",
            "[465 | 595.55] loss=2.65 avg=2.46\n",
            "[466 | 596.83] loss=2.32 avg=2.46\n",
            "[467 | 598.11] loss=2.63 avg=2.46\n",
            "[468 | 599.40] loss=2.42 avg=2.46\n",
            "[469 | 600.68] loss=2.45 avg=2.46\n",
            "[470 | 601.96] loss=2.63 avg=2.46\n",
            "[471 | 603.25] loss=2.64 avg=2.46\n",
            "[472 | 604.53] loss=2.33 avg=2.46\n",
            "[473 | 605.82] loss=2.40 avg=2.46\n",
            "[474 | 607.10] loss=2.10 avg=2.46\n",
            "[475 | 608.38] loss=2.48 avg=2.46\n",
            "[476 | 609.67] loss=2.48 avg=2.46\n",
            "[477 | 610.95] loss=2.44 avg=2.46\n",
            "[478 | 612.23] loss=2.41 avg=2.46\n",
            "[479 | 613.51] loss=2.56 avg=2.46\n",
            "[480 | 614.80] loss=2.48 avg=2.46\n",
            "[481 | 616.08] loss=2.60 avg=2.46\n",
            "[482 | 617.36] loss=2.56 avg=2.46\n",
            "[483 | 618.65] loss=2.55 avg=2.46\n",
            "[484 | 619.93] loss=2.50 avg=2.46\n",
            "[485 | 621.22] loss=2.60 avg=2.47\n",
            "[486 | 622.50] loss=2.49 avg=2.47\n",
            "[487 | 623.78] loss=2.47 avg=2.47\n",
            "[488 | 625.06] loss=2.12 avg=2.46\n",
            "[489 | 626.35] loss=1.88 avg=2.46\n",
            "[490 | 627.63] loss=2.33 avg=2.45\n",
            "[491 | 628.91] loss=1.78 avg=2.45\n",
            "[492 | 630.20] loss=2.35 avg=2.45\n",
            "[493 | 631.48] loss=2.12 avg=2.44\n",
            "[494 | 632.77] loss=2.47 avg=2.44\n",
            "[495 | 634.05] loss=2.50 avg=2.44\n",
            "[496 | 635.33] loss=2.47 avg=2.44\n",
            "[497 | 636.62] loss=2.39 avg=2.44\n",
            "[498 | 637.90] loss=2.29 avg=2.44\n",
            "[499 | 639.18] loss=2.22 avg=2.44\n",
            "[500 | 640.47] loss=2.31 avg=2.44\n",
            "[501 | 641.75] loss=2.53 avg=2.44\n",
            "[502 | 643.04] loss=2.52 avg=2.44\n",
            "[503 | 644.33] loss=2.36 avg=2.44\n",
            "[504 | 645.61] loss=2.65 avg=2.44\n",
            "[505 | 646.90] loss=1.97 avg=2.44\n",
            "[506 | 648.18] loss=2.57 avg=2.44\n",
            "[507 | 649.47] loss=2.56 avg=2.44\n",
            "[508 | 650.75] loss=2.00 avg=2.44\n",
            "[509 | 652.03] loss=2.59 avg=2.44\n",
            "[510 | 653.32] loss=2.53 avg=2.44\n",
            "[511 | 654.60] loss=2.18 avg=2.44\n",
            "[512 | 655.89] loss=2.45 avg=2.44\n",
            "[513 | 657.18] loss=2.38 avg=2.43\n",
            "[514 | 658.46] loss=2.59 avg=2.44\n",
            "[515 | 659.74] loss=2.56 avg=2.44\n",
            "[516 | 661.03] loss=2.43 avg=2.44\n",
            "[517 | 662.31] loss=2.43 avg=2.44\n",
            "[518 | 663.60] loss=2.38 avg=2.44\n",
            "[519 | 664.88] loss=2.37 avg=2.44\n",
            "[520 | 666.16] loss=2.39 avg=2.44\n",
            "[521 | 667.45] loss=2.63 avg=2.44\n",
            "[522 | 668.73] loss=2.61 avg=2.44\n",
            "[523 | 670.01] loss=2.57 avg=2.44\n",
            "[524 | 671.30] loss=2.42 avg=2.44\n",
            "[525 | 672.58] loss=2.26 avg=2.44\n",
            "[526 | 673.87] loss=2.33 avg=2.44\n",
            "[527 | 675.15] loss=2.46 avg=2.44\n",
            "[528 | 676.43] loss=2.38 avg=2.44\n",
            "[529 | 677.72] loss=2.65 avg=2.44\n",
            "[530 | 679.00] loss=2.55 avg=2.44\n",
            "[531 | 680.27] loss=2.53 avg=2.44\n",
            "[532 | 681.56] loss=2.37 avg=2.44\n",
            "[533 | 682.84] loss=2.53 avg=2.44\n",
            "[534 | 684.13] loss=2.60 avg=2.44\n",
            "[535 | 685.41] loss=2.57 avg=2.44\n",
            "[536 | 686.69] loss=2.03 avg=2.44\n",
            "[537 | 687.97] loss=2.50 avg=2.44\n",
            "[538 | 689.26] loss=2.46 avg=2.44\n",
            "[539 | 690.55] loss=2.45 avg=2.44\n",
            "[540 | 691.82] loss=2.37 avg=2.44\n",
            "[541 | 693.11] loss=2.53 avg=2.44\n",
            "[542 | 694.39] loss=2.47 avg=2.44\n",
            "[543 | 695.67] loss=2.13 avg=2.44\n",
            "[544 | 696.96] loss=2.02 avg=2.43\n",
            "[545 | 698.25] loss=2.52 avg=2.44\n",
            "[546 | 699.53] loss=2.57 avg=2.44\n",
            "[547 | 700.82] loss=2.49 avg=2.44\n",
            "[548 | 702.10] loss=2.58 avg=2.44\n",
            "[549 | 703.39] loss=2.57 avg=2.44\n",
            "[550 | 704.67] loss=2.35 avg=2.44\n",
            "[551 | 705.96] loss=2.33 avg=2.44\n",
            "[552 | 707.24] loss=2.55 avg=2.44\n",
            "[553 | 708.53] loss=1.97 avg=2.43\n",
            "[554 | 709.81] loss=2.41 avg=2.43\n",
            "[555 | 711.10] loss=2.62 avg=2.44\n",
            "[556 | 712.38] loss=2.18 avg=2.43\n",
            "[557 | 713.67] loss=2.18 avg=2.43\n",
            "[558 | 714.95] loss=2.12 avg=2.43\n",
            "[559 | 716.24] loss=2.35 avg=2.43\n",
            "[560 | 717.52] loss=2.56 avg=2.43\n",
            "[561 | 718.81] loss=2.55 avg=2.43\n",
            "[562 | 720.10] loss=2.56 avg=2.43\n",
            "[563 | 721.38] loss=2.56 avg=2.43\n",
            "[564 | 722.67] loss=2.54 avg=2.43\n",
            "[565 | 723.95] loss=2.50 avg=2.43\n",
            "[566 | 725.24] loss=2.05 avg=2.43\n",
            "[567 | 726.52] loss=2.50 avg=2.43\n",
            "[568 | 727.80] loss=2.23 avg=2.43\n",
            "[569 | 729.09] loss=2.17 avg=2.43\n",
            "[570 | 730.37] loss=2.47 avg=2.43\n",
            "[571 | 731.65] loss=2.57 avg=2.43\n",
            "[572 | 732.94] loss=2.54 avg=2.43\n",
            "[573 | 734.22] loss=2.31 avg=2.43\n",
            "[574 | 735.51] loss=2.59 avg=2.43\n",
            "[575 | 736.79] loss=2.27 avg=2.43\n",
            "[576 | 738.08] loss=2.08 avg=2.42\n",
            "[577 | 739.36] loss=2.21 avg=2.42\n",
            "[578 | 740.64] loss=2.15 avg=2.42\n",
            "[579 | 741.93] loss=2.60 avg=2.42\n",
            "[580 | 743.21] loss=2.25 avg=2.42\n",
            "[581 | 744.49] loss=2.44 avg=2.42\n",
            "[582 | 745.78] loss=2.15 avg=2.42\n",
            "[583 | 747.06] loss=2.58 avg=2.42\n",
            "[584 | 748.34] loss=2.20 avg=2.42\n",
            "[585 | 749.63] loss=2.24 avg=2.42\n",
            "[586 | 750.91] loss=2.03 avg=2.41\n",
            "[587 | 752.19] loss=2.60 avg=2.41\n",
            "[588 | 753.47] loss=2.35 avg=2.41\n",
            "[589 | 754.75] loss=2.44 avg=2.41\n",
            "[590 | 756.03] loss=1.91 avg=2.41\n",
            "[591 | 757.31] loss=2.48 avg=2.41\n",
            "[592 | 758.59] loss=2.69 avg=2.41\n",
            "[593 | 759.88] loss=2.53 avg=2.41\n",
            "[594 | 761.16] loss=2.50 avg=2.41\n",
            "[595 | 762.45] loss=2.73 avg=2.42\n",
            "[596 | 763.73] loss=2.40 avg=2.42\n",
            "[597 | 765.01] loss=2.45 avg=2.42\n",
            "[598 | 766.29] loss=2.12 avg=2.41\n",
            "[599 | 767.57] loss=2.55 avg=2.42\n",
            "[600 | 768.86] loss=2.65 avg=2.42\n",
            "[601 | 770.14] loss=2.37 avg=2.42\n",
            "[602 | 771.42] loss=2.30 avg=2.42\n",
            "[603 | 772.71] loss=2.46 avg=2.42\n",
            "[604 | 774.00] loss=2.47 avg=2.42\n",
            "[605 | 775.28] loss=2.25 avg=2.42\n",
            "[606 | 776.58] loss=2.00 avg=2.41\n",
            "[607 | 777.86] loss=2.17 avg=2.41\n",
            "[608 | 779.14] loss=2.25 avg=2.41\n",
            "[609 | 780.42] loss=2.52 avg=2.41\n",
            "[610 | 781.70] loss=2.37 avg=2.41\n",
            "[611 | 782.99] loss=2.44 avg=2.41\n",
            "[612 | 784.27] loss=2.45 avg=2.41\n",
            "[613 | 785.56] loss=2.61 avg=2.41\n",
            "[614 | 786.84] loss=2.54 avg=2.41\n",
            "[615 | 788.13] loss=2.51 avg=2.41\n",
            "[616 | 789.41] loss=2.45 avg=2.41\n",
            "[617 | 790.69] loss=2.62 avg=2.42\n",
            "[618 | 791.97] loss=2.23 avg=2.41\n",
            "[619 | 793.26] loss=1.91 avg=2.41\n",
            "[620 | 794.54] loss=2.49 avg=2.41\n",
            "[621 | 795.83] loss=2.52 avg=2.41\n",
            "[622 | 797.11] loss=2.37 avg=2.41\n",
            "[623 | 798.39] loss=2.41 avg=2.41\n",
            "[624 | 799.68] loss=2.56 avg=2.41\n",
            "[625 | 800.96] loss=2.39 avg=2.41\n",
            "[626 | 802.25] loss=2.57 avg=2.41\n",
            "[627 | 803.53] loss=2.58 avg=2.41\n",
            "[628 | 804.82] loss=2.14 avg=2.41\n",
            "[629 | 806.10] loss=1.85 avg=2.41\n",
            "[630 | 807.38] loss=2.57 avg=2.41\n",
            "[631 | 808.67] loss=2.44 avg=2.41\n",
            "[632 | 809.95] loss=2.49 avg=2.41\n",
            "[633 | 811.23] loss=2.47 avg=2.41\n",
            "[634 | 812.52] loss=1.74 avg=2.40\n",
            "[635 | 813.80] loss=2.26 avg=2.40\n",
            "[636 | 815.08] loss=2.48 avg=2.40\n",
            "[637 | 816.36] loss=1.82 avg=2.40\n",
            "[638 | 817.64] loss=2.27 avg=2.39\n",
            "[639 | 818.92] loss=2.06 avg=2.39\n",
            "[640 | 820.21] loss=2.56 avg=2.39\n",
            "[641 | 821.49] loss=1.91 avg=2.39\n",
            "[642 | 822.76] loss=1.95 avg=2.38\n",
            "[643 | 824.05] loss=2.52 avg=2.39\n",
            "[644 | 825.32] loss=2.20 avg=2.38\n",
            "[645 | 826.60] loss=2.42 avg=2.38\n",
            "[646 | 827.88] loss=2.60 avg=2.39\n",
            "[647 | 829.16] loss=2.72 avg=2.39\n",
            "[648 | 830.44] loss=2.43 avg=2.39\n",
            "[649 | 831.72] loss=2.54 avg=2.39\n",
            "[650 | 833.01] loss=1.90 avg=2.39\n",
            "[651 | 834.29] loss=2.56 avg=2.39\n",
            "[652 | 835.56] loss=2.51 avg=2.39\n",
            "[653 | 836.84] loss=2.44 avg=2.39\n",
            "[654 | 838.12] loss=2.45 avg=2.39\n",
            "[655 | 839.40] loss=2.25 avg=2.39\n",
            "[656 | 840.68] loss=1.98 avg=2.38\n",
            "[657 | 841.96] loss=2.48 avg=2.39\n",
            "[658 | 843.24] loss=2.06 avg=2.38\n",
            "[659 | 844.52] loss=1.97 avg=2.38\n",
            "[660 | 845.80] loss=2.48 avg=2.38\n",
            "[661 | 847.07] loss=2.47 avg=2.38\n",
            "[662 | 848.35] loss=2.19 avg=2.38\n",
            "[663 | 849.63] loss=2.01 avg=2.37\n",
            "[664 | 850.91] loss=2.31 avg=2.37\n",
            "[665 | 852.20] loss=2.25 avg=2.37\n",
            "[666 | 853.47] loss=2.15 avg=2.37\n",
            "[667 | 854.75] loss=2.50 avg=2.37\n",
            "[668 | 856.03] loss=2.28 avg=2.37\n",
            "[669 | 857.31] loss=2.73 avg=2.37\n",
            "[670 | 858.59] loss=2.02 avg=2.37\n",
            "[671 | 859.87] loss=2.44 avg=2.37\n",
            "[672 | 861.16] loss=2.49 avg=2.37\n",
            "[673 | 862.44] loss=2.09 avg=2.37\n",
            "[674 | 863.72] loss=1.74 avg=2.36\n",
            "[675 | 865.00] loss=2.46 avg=2.36\n",
            "[676 | 866.28] loss=2.68 avg=2.37\n",
            "[677 | 867.56] loss=2.31 avg=2.37\n",
            "[678 | 868.84] loss=2.44 avg=2.37\n",
            "[679 | 870.12] loss=2.67 avg=2.37\n",
            "[680 | 871.40] loss=2.31 avg=2.37\n",
            "[681 | 872.69] loss=2.55 avg=2.37\n",
            "[682 | 873.97] loss=2.31 avg=2.37\n",
            "[683 | 875.25] loss=2.44 avg=2.37\n",
            "[684 | 876.54] loss=2.51 avg=2.37\n",
            "[685 | 877.82] loss=2.40 avg=2.37\n",
            "[686 | 879.11] loss=2.35 avg=2.37\n",
            "[687 | 880.39] loss=2.08 avg=2.37\n",
            "[688 | 881.67] loss=2.42 avg=2.37\n",
            "[689 | 882.95] loss=2.46 avg=2.37\n",
            "[690 | 884.24] loss=2.13 avg=2.37\n",
            "[691 | 885.52] loss=2.52 avg=2.37\n",
            "[692 | 886.80] loss=2.38 avg=2.37\n",
            "[693 | 888.07] loss=2.28 avg=2.37\n",
            "[694 | 889.36] loss=2.36 avg=2.37\n",
            "[695 | 890.64] loss=2.56 avg=2.37\n",
            "[696 | 891.92] loss=2.27 avg=2.37\n",
            "[697 | 893.20] loss=2.41 avg=2.37\n",
            "[698 | 894.49] loss=2.27 avg=2.37\n",
            "[699 | 895.76] loss=2.23 avg=2.37\n",
            "[700 | 897.04] loss=2.54 avg=2.37\n",
            "[701 | 898.33] loss=2.38 avg=2.37\n",
            "[702 | 899.61] loss=2.07 avg=2.37\n",
            "[703 | 900.89] loss=1.96 avg=2.36\n",
            "[704 | 902.17] loss=2.28 avg=2.36\n",
            "[705 | 903.46] loss=2.42 avg=2.36\n",
            "[706 | 904.74] loss=2.50 avg=2.37\n",
            "[707 | 906.02] loss=2.32 avg=2.36\n",
            "[708 | 907.31] loss=2.68 avg=2.37\n",
            "[709 | 908.59] loss=2.61 avg=2.37\n",
            "[710 | 909.87] loss=2.62 avg=2.37\n",
            "[711 | 911.15] loss=2.36 avg=2.37\n",
            "[712 | 912.43] loss=2.05 avg=2.37\n",
            "[713 | 913.71] loss=2.05 avg=2.37\n",
            "[714 | 915.00] loss=2.56 avg=2.37\n",
            "[715 | 916.28] loss=2.55 avg=2.37\n",
            "[716 | 917.56] loss=2.07 avg=2.37\n",
            "[717 | 918.84] loss=2.53 avg=2.37\n",
            "[718 | 920.12] loss=2.23 avg=2.37\n",
            "[719 | 921.41] loss=2.25 avg=2.37\n",
            "[720 | 922.69] loss=2.50 avg=2.37\n",
            "[721 | 923.97] loss=2.36 avg=2.37\n",
            "[722 | 925.25] loss=1.93 avg=2.36\n",
            "[723 | 926.54] loss=2.31 avg=2.36\n",
            "[724 | 927.82] loss=2.24 avg=2.36\n",
            "[725 | 929.11] loss=2.15 avg=2.36\n",
            "[726 | 930.39] loss=2.18 avg=2.36\n",
            "[727 | 931.67] loss=2.49 avg=2.36\n",
            "[728 | 932.96] loss=2.19 avg=2.36\n",
            "[729 | 934.25] loss=2.34 avg=2.36\n",
            "[730 | 935.53] loss=2.33 avg=2.36\n",
            "[731 | 936.81] loss=2.30 avg=2.36\n",
            "[732 | 938.09] loss=2.45 avg=2.36\n",
            "[733 | 939.37] loss=2.26 avg=2.36\n",
            "[734 | 940.65] loss=1.93 avg=2.35\n",
            "[735 | 941.94] loss=2.60 avg=2.35\n",
            "[736 | 943.22] loss=2.17 avg=2.35\n",
            "[737 | 944.50] loss=2.24 avg=2.35\n",
            "[738 | 945.78] loss=2.58 avg=2.35\n",
            "[739 | 947.06] loss=1.97 avg=2.35\n",
            "[740 | 948.35] loss=2.02 avg=2.35\n",
            "[741 | 949.63] loss=2.11 avg=2.34\n",
            "[742 | 950.91] loss=2.46 avg=2.35\n",
            "[743 | 952.20] loss=2.49 avg=2.35\n",
            "[744 | 953.48] loss=2.44 avg=2.35\n",
            "[745 | 954.77] loss=2.20 avg=2.35\n",
            "[746 | 956.05] loss=2.48 avg=2.35\n",
            "[747 | 957.33] loss=1.94 avg=2.34\n",
            "[748 | 958.62] loss=2.65 avg=2.35\n",
            "[749 | 959.90] loss=2.39 avg=2.35\n",
            "[750 | 961.19] loss=2.50 avg=2.35\n",
            "[751 | 962.47] loss=2.62 avg=2.35\n",
            "[752 | 963.76] loss=2.57 avg=2.35\n",
            "[753 | 965.04] loss=1.73 avg=2.35\n",
            "[754 | 966.32] loss=2.27 avg=2.35\n",
            "[755 | 967.60] loss=2.65 avg=2.35\n",
            "[756 | 968.89] loss=2.52 avg=2.35\n",
            "[757 | 970.17] loss=2.13 avg=2.35\n",
            "[758 | 971.45] loss=2.12 avg=2.35\n",
            "[759 | 972.74] loss=2.69 avg=2.35\n",
            "[760 | 974.03] loss=2.52 avg=2.35\n",
            "[761 | 975.31] loss=2.09 avg=2.35\n",
            "[762 | 976.60] loss=2.50 avg=2.35\n",
            "[763 | 977.88] loss=2.14 avg=2.35\n",
            "[764 | 979.17] loss=2.06 avg=2.35\n",
            "[765 | 980.45] loss=2.55 avg=2.35\n",
            "[766 | 981.74] loss=2.51 avg=2.35\n",
            "[767 | 983.02] loss=2.51 avg=2.35\n",
            "[768 | 984.31] loss=2.52 avg=2.35\n",
            "[769 | 985.59] loss=2.35 avg=2.35\n",
            "[770 | 986.88] loss=2.59 avg=2.36\n",
            "[771 | 988.17] loss=1.73 avg=2.35\n",
            "[772 | 989.45] loss=2.62 avg=2.35\n",
            "[773 | 990.73] loss=2.20 avg=2.35\n",
            "[774 | 992.02] loss=2.38 avg=2.35\n",
            "[775 | 993.30] loss=2.21 avg=2.35\n",
            "[776 | 994.58] loss=2.52 avg=2.35\n",
            "[777 | 995.86] loss=1.91 avg=2.35\n",
            "[778 | 997.15] loss=2.42 avg=2.35\n",
            "[779 | 998.43] loss=2.30 avg=2.35\n",
            "[780 | 999.71] loss=2.35 avg=2.35\n",
            "[781 | 1001.00] loss=2.01 avg=2.34\n",
            "[782 | 1002.28] loss=2.30 avg=2.34\n",
            "[783 | 1003.57] loss=1.73 avg=2.34\n",
            "[784 | 1004.85] loss=1.78 avg=2.33\n",
            "[785 | 1006.13] loss=2.10 avg=2.33\n",
            "[786 | 1007.41] loss=2.69 avg=2.33\n",
            "[787 | 1008.70] loss=2.61 avg=2.34\n",
            "[788 | 1009.98] loss=2.68 avg=2.34\n",
            "[789 | 1011.26] loss=2.30 avg=2.34\n",
            "[790 | 1012.55] loss=2.28 avg=2.34\n",
            "[791 | 1013.83] loss=2.05 avg=2.33\n",
            "[792 | 1015.12] loss=2.52 avg=2.34\n",
            "[793 | 1016.40] loss=1.93 avg=2.33\n",
            "[794 | 1017.68] loss=2.46 avg=2.33\n",
            "[795 | 1018.96] loss=2.60 avg=2.34\n",
            "[796 | 1020.25] loss=2.45 avg=2.34\n",
            "[797 | 1021.53] loss=2.35 avg=2.34\n",
            "[798 | 1022.82] loss=2.19 avg=2.34\n",
            "[799 | 1024.10] loss=2.09 avg=2.33\n",
            "[800 | 1025.39] loss=2.40 avg=2.33\n",
            "[801 | 1026.67] loss=2.12 avg=2.33\n",
            "[802 | 1027.95] loss=2.56 avg=2.33\n",
            "[803 | 1029.24] loss=2.46 avg=2.34\n",
            "[804 | 1030.52] loss=2.46 avg=2.34\n",
            "[805 | 1031.81] loss=2.41 avg=2.34\n",
            "[806 | 1033.09] loss=2.71 avg=2.34\n",
            "[807 | 1034.37] loss=2.34 avg=2.34\n",
            "[808 | 1035.66] loss=2.21 avg=2.34\n",
            "[809 | 1036.94] loss=1.82 avg=2.33\n",
            "[810 | 1038.23] loss=2.13 avg=2.33\n",
            "[811 | 1039.50] loss=2.59 avg=2.34\n",
            "[812 | 1040.79] loss=1.83 avg=2.33\n",
            "[813 | 1042.07] loss=2.32 avg=2.33\n",
            "[814 | 1043.35] loss=1.75 avg=2.32\n",
            "[815 | 1044.63] loss=1.93 avg=2.32\n",
            "[816 | 1045.91] loss=2.22 avg=2.32\n",
            "[817 | 1047.19] loss=2.32 avg=2.32\n",
            "[818 | 1048.48] loss=2.17 avg=2.32\n",
            "[819 | 1049.77] loss=2.13 avg=2.32\n",
            "[820 | 1051.05] loss=1.98 avg=2.31\n",
            "[821 | 1052.34] loss=2.56 avg=2.32\n",
            "[822 | 1053.62] loss=2.22 avg=2.31\n",
            "[823 | 1054.90] loss=2.75 avg=2.32\n",
            "[824 | 1056.18] loss=2.27 avg=2.32\n",
            "[825 | 1057.47] loss=2.54 avg=2.32\n",
            "[826 | 1058.75] loss=2.39 avg=2.32\n",
            "[827 | 1060.03] loss=2.33 avg=2.32\n",
            "[828 | 1061.31] loss=2.09 avg=2.32\n",
            "[829 | 1062.60] loss=1.99 avg=2.32\n",
            "[830 | 1063.89] loss=2.40 avg=2.32\n",
            "[831 | 1065.18] loss=2.37 avg=2.32\n",
            "[832 | 1066.46] loss=2.24 avg=2.32\n",
            "[833 | 1067.74] loss=2.09 avg=2.31\n",
            "[834 | 1069.03] loss=2.27 avg=2.31\n",
            "[835 | 1070.31] loss=2.50 avg=2.32\n",
            "[836 | 1071.60] loss=2.14 avg=2.31\n",
            "[837 | 1072.88] loss=1.91 avg=2.31\n",
            "[838 | 1074.16] loss=2.51 avg=2.31\n",
            "[839 | 1075.45] loss=2.48 avg=2.31\n",
            "[840 | 1076.74] loss=2.37 avg=2.31\n",
            "[841 | 1078.02] loss=2.66 avg=2.32\n",
            "[842 | 1079.30] loss=2.12 avg=2.32\n",
            "[843 | 1080.59] loss=2.02 avg=2.31\n",
            "[844 | 1081.87] loss=1.73 avg=2.31\n",
            "[845 | 1083.15] loss=2.32 avg=2.31\n",
            "[846 | 1084.44] loss=2.18 avg=2.31\n",
            "[847 | 1085.72] loss=2.47 avg=2.31\n",
            "[848 | 1087.00] loss=2.28 avg=2.31\n",
            "[849 | 1088.29] loss=2.28 avg=2.31\n",
            "[850 | 1089.57] loss=2.23 avg=2.31\n",
            "[851 | 1090.85] loss=2.35 avg=2.31\n",
            "[852 | 1092.13] loss=2.05 avg=2.30\n",
            "[853 | 1093.42] loss=2.44 avg=2.31\n",
            "[854 | 1094.70] loss=2.06 avg=2.30\n",
            "[855 | 1095.99] loss=2.58 avg=2.31\n",
            "[856 | 1097.27] loss=2.34 avg=2.31\n",
            "[857 | 1098.55] loss=2.61 avg=2.31\n",
            "[858 | 1099.84] loss=2.21 avg=2.31\n",
            "[859 | 1101.12] loss=2.42 avg=2.31\n",
            "[860 | 1102.40] loss=2.39 avg=2.31\n",
            "[861 | 1103.69] loss=1.81 avg=2.30\n",
            "[862 | 1104.98] loss=1.67 avg=2.30\n",
            "[863 | 1106.26] loss=2.11 avg=2.30\n",
            "[864 | 1107.54] loss=2.55 avg=2.30\n",
            "[865 | 1108.82] loss=2.43 avg=2.30\n",
            "[866 | 1110.11] loss=2.48 avg=2.30\n",
            "[867 | 1111.39] loss=2.02 avg=2.30\n",
            "[868 | 1112.68] loss=1.89 avg=2.30\n",
            "[869 | 1113.96] loss=2.35 avg=2.30\n",
            "[870 | 1115.25] loss=2.08 avg=2.29\n",
            "[871 | 1116.53] loss=2.49 avg=2.30\n",
            "[872 | 1117.81] loss=2.15 avg=2.29\n",
            "[873 | 1119.10] loss=2.52 avg=2.30\n",
            "[874 | 1120.38] loss=2.60 avg=2.30\n",
            "[875 | 1121.66] loss=2.53 avg=2.30\n",
            "[876 | 1122.95] loss=2.11 avg=2.30\n",
            "[877 | 1124.24] loss=2.52 avg=2.30\n",
            "[878 | 1125.52] loss=2.22 avg=2.30\n",
            "[879 | 1126.81] loss=2.29 avg=2.30\n",
            "[880 | 1128.09] loss=1.77 avg=2.30\n",
            "[881 | 1129.38] loss=2.31 avg=2.30\n",
            "[882 | 1130.66] loss=1.88 avg=2.29\n",
            "[883 | 1131.94] loss=2.11 avg=2.29\n",
            "[884 | 1133.23] loss=2.42 avg=2.29\n",
            "[885 | 1134.52] loss=1.95 avg=2.29\n",
            "[886 | 1135.80] loss=1.78 avg=2.28\n",
            "[887 | 1137.09] loss=2.35 avg=2.28\n",
            "[888 | 1138.37] loss=2.04 avg=2.28\n",
            "[889 | 1139.65] loss=2.44 avg=2.28\n",
            "[890 | 1140.94] loss=2.56 avg=2.29\n",
            "[891 | 1142.22] loss=2.27 avg=2.29\n",
            "[892 | 1143.50] loss=2.41 avg=2.29\n",
            "[893 | 1144.79] loss=2.68 avg=2.29\n",
            "[894 | 1146.07] loss=2.47 avg=2.29\n",
            "[895 | 1147.36] loss=2.17 avg=2.29\n",
            "[896 | 1148.64] loss=2.45 avg=2.29\n",
            "[897 | 1149.93] loss=2.23 avg=2.29\n",
            "[898 | 1151.22] loss=2.16 avg=2.29\n",
            "[899 | 1152.50] loss=1.62 avg=2.28\n",
            "[900 | 1153.78] loss=2.56 avg=2.29\n",
            "[901 | 1155.06] loss=2.29 avg=2.29\n",
            "[902 | 1156.34] loss=2.43 avg=2.29\n",
            "[903 | 1157.63] loss=1.98 avg=2.29\n",
            "[904 | 1158.91] loss=2.02 avg=2.28\n",
            "[905 | 1160.19] loss=2.11 avg=2.28\n",
            "[906 | 1161.48] loss=2.11 avg=2.28\n",
            "[907 | 1162.76] loss=1.80 avg=2.27\n",
            "[908 | 1164.05] loss=1.86 avg=2.27\n",
            "[909 | 1165.33] loss=2.07 avg=2.27\n",
            "[910 | 1166.62] loss=2.12 avg=2.27\n",
            "[911 | 1167.90] loss=2.18 avg=2.27\n",
            "[912 | 1169.18] loss=1.92 avg=2.26\n",
            "[913 | 1170.47] loss=2.40 avg=2.26\n",
            "[914 | 1171.75] loss=2.50 avg=2.27\n",
            "[915 | 1173.03] loss=2.21 avg=2.27\n",
            "[916 | 1174.31] loss=2.42 avg=2.27\n",
            "[917 | 1175.60] loss=2.26 avg=2.27\n",
            "[918 | 1176.88] loss=2.27 avg=2.27\n",
            "[919 | 1178.17] loss=1.54 avg=2.26\n",
            "[920 | 1179.45] loss=2.60 avg=2.26\n",
            "[921 | 1180.74] loss=2.21 avg=2.26\n",
            "[922 | 1182.02] loss=2.43 avg=2.26\n",
            "[923 | 1183.30] loss=2.37 avg=2.27\n",
            "[924 | 1184.59] loss=2.22 avg=2.26\n",
            "[925 | 1185.87] loss=1.94 avg=2.26\n",
            "[926 | 1187.16] loss=2.44 avg=2.26\n",
            "[927 | 1188.44] loss=1.89 avg=2.26\n",
            "[928 | 1189.72] loss=2.26 avg=2.26\n",
            "[929 | 1191.01] loss=2.23 avg=2.26\n",
            "[930 | 1192.30] loss=1.74 avg=2.25\n",
            "[931 | 1193.58] loss=1.68 avg=2.25\n",
            "[932 | 1194.87] loss=2.07 avg=2.25\n",
            "[933 | 1196.15] loss=1.64 avg=2.24\n",
            "[934 | 1197.44] loss=2.46 avg=2.24\n",
            "[935 | 1198.72] loss=2.56 avg=2.25\n",
            "[936 | 1200.00] loss=2.18 avg=2.25\n",
            "[937 | 1201.28] loss=2.62 avg=2.25\n",
            "[938 | 1202.57] loss=2.74 avg=2.25\n",
            "[939 | 1203.86] loss=2.56 avg=2.26\n",
            "[940 | 1205.14] loss=2.70 avg=2.26\n",
            "[941 | 1206.42] loss=2.40 avg=2.26\n",
            "[942 | 1207.70] loss=2.19 avg=2.26\n",
            "[943 | 1208.99] loss=2.18 avg=2.26\n",
            "[944 | 1210.27] loss=2.19 avg=2.26\n",
            "[945 | 1211.56] loss=1.75 avg=2.26\n",
            "[946 | 1212.84] loss=2.01 avg=2.25\n",
            "[947 | 1214.12] loss=2.13 avg=2.25\n",
            "[948 | 1215.41] loss=1.86 avg=2.25\n",
            "[949 | 1216.69] loss=2.12 avg=2.25\n",
            "[950 | 1217.97] loss=1.97 avg=2.24\n",
            "[951 | 1219.26] loss=2.41 avg=2.25\n",
            "[952 | 1220.54] loss=2.30 avg=2.25\n",
            "[953 | 1221.83] loss=2.53 avg=2.25\n",
            "[954 | 1223.11] loss=2.28 avg=2.25\n",
            "[955 | 1224.39] loss=2.05 avg=2.25\n",
            "[956 | 1225.68] loss=2.49 avg=2.25\n",
            "[957 | 1226.96] loss=2.55 avg=2.25\n",
            "[958 | 1228.24] loss=2.42 avg=2.25\n",
            "[959 | 1229.52] loss=1.73 avg=2.25\n",
            "[960 | 1230.81] loss=2.32 avg=2.25\n",
            "[961 | 1232.09] loss=2.34 avg=2.25\n",
            "[962 | 1233.37] loss=2.20 avg=2.25\n",
            "[963 | 1234.65] loss=1.63 avg=2.24\n",
            "[964 | 1235.94] loss=2.04 avg=2.24\n",
            "[965 | 1237.22] loss=2.41 avg=2.24\n",
            "[966 | 1238.51] loss=2.55 avg=2.25\n",
            "[967 | 1239.79] loss=2.10 avg=2.25\n",
            "[968 | 1241.08] loss=2.26 avg=2.25\n",
            "[969 | 1242.36] loss=2.52 avg=2.25\n",
            "[970 | 1243.65] loss=1.50 avg=2.24\n",
            "[971 | 1244.93] loss=2.56 avg=2.24\n",
            "[972 | 1246.22] loss=2.49 avg=2.25\n",
            "[973 | 1247.50] loss=2.31 avg=2.25\n",
            "[974 | 1248.78] loss=2.41 avg=2.25\n",
            "[975 | 1250.07] loss=2.49 avg=2.25\n",
            "[976 | 1251.35] loss=2.42 avg=2.25\n",
            "[977 | 1252.64] loss=2.30 avg=2.25\n",
            "[978 | 1253.93] loss=2.13 avg=2.25\n",
            "[979 | 1255.21] loss=2.06 avg=2.25\n",
            "[980 | 1256.49] loss=2.46 avg=2.25\n",
            "[981 | 1257.77] loss=2.21 avg=2.25\n",
            "[982 | 1259.06] loss=2.43 avg=2.25\n",
            "[983 | 1260.34] loss=2.37 avg=2.25\n",
            "[984 | 1261.63] loss=2.46 avg=2.26\n",
            "[985 | 1262.91] loss=2.56 avg=2.26\n",
            "[986 | 1264.19] loss=2.45 avg=2.26\n",
            "[987 | 1265.47] loss=2.59 avg=2.27\n",
            "[988 | 1266.75] loss=2.24 avg=2.26\n",
            "[989 | 1268.04] loss=2.13 avg=2.26\n",
            "[990 | 1269.32] loss=2.55 avg=2.27\n",
            "[991 | 1270.61] loss=2.66 avg=2.27\n",
            "[992 | 1271.89] loss=2.30 avg=2.27\n",
            "[993 | 1273.17] loss=2.58 avg=2.27\n",
            "[994 | 1274.46] loss=2.53 avg=2.28\n",
            "[995 | 1275.74] loss=2.35 avg=2.28\n",
            "[996 | 1277.02] loss=2.01 avg=2.27\n",
            "[997 | 1278.30] loss=2.63 avg=2.28\n",
            "[998 | 1279.59] loss=1.71 avg=2.27\n",
            "[999 | 1280.88] loss=2.43 avg=2.27\n",
            "Saving checkpoint/run1/model-1000\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUa2LujW82cy",
        "colab_type": "text"
      },
      "source": [
        "The step below simply copies your trained model to the model directory, so the output will use your training. If you don't do this, you will be running against the trained GPT-2 model without your finetuning training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNXhOM22fNHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvtv6NUj5eXU",
        "colab_type": "text"
      },
      "source": [
        "Run the below step to generate unconditional samples (i.e. \"dream mode\").\n",
        "\n",
        "\"top_k\" controls how many options to consider per word (the larger, the more \"diverse\" the output - anything from 1 to about 50 usually works, I think values around 10 are pretty good).\n",
        "\n",
        "\"temperature\" controls the sampling of the words, from 0 to 1 where 1 is the most \"random\".\n",
        "\n",
        "\"length\" controls the number of words in each sample output.\n",
        "\n",
        "This command will run continuously until you turn it off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2csc2bZHfrXd",
        "colab_type": "code",
        "outputId": "dbc68283-3d94-479a-d65e-2b80e65bf962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        }
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --top_k 10 --temperature 1 --length=300 --model_name=345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"src/generate_unconditional_samples.py\", line 7, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 52, in <module>\n",
            "    from tensorflow.core.framework.graph_pb2 import *\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n",
            "    from google.protobuf import descriptor as _descriptor\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/__init__.py\", line 37, in <module>\n",
            "    __import__('pkg_resources').declare_namespace(__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3241, in <module>\n",
            "    @_call_aside\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3225, in _call_aside\n",
            "    f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3277, in _initialize_master_working_set\n",
            "    list(map(working_set.add_entry, sys.path))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 623, in add_entry\n",
            "    for dist in find_distributions(entry, True):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2052, in find_on_path\n",
            "    path_item_entries = _by_version_descending(filtered)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2025, in _by_version_descending\n",
            "    return sorted(names, key=_by_version, reverse=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2021, in _by_version\n",
            "    name, ext = os.path.splitext(name)\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 129, in splitext\n",
            "    return genericpath._splitext(p, sep, None, extsep)\n",
            "  File \"/usr/lib/python3.6/genericpath.py\", line 124, in _splitext\n",
            "    sepIndex = p.rfind(sep)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwZl7JAn6d5y",
        "colab_type": "text"
      },
      "source": [
        "Run the command below to run in interactive / \"completion\" mode. You will get a prompt; just type in whatever prompt text you want, and the model will attempt to complete it \"nsamples\" times.\n",
        "\n",
        "\"top_k\", \"length\", and \"temperature\" work as specified above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugmKkFv__NI_",
        "colab_type": "code",
        "outputId": "6bfd860c-3f9e-43e4-ef85-33248ff6198f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7823
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --top_k 1 --length=30 --temperature 0.1 --nsamples 3 --model_name=345M"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-08 17:53:50.739097: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-08 17:53:50.739382: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1f36520 executing computations on platform Host. Devices:\n",
            "2019-05-08 17:53:50.739454: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-08 17:53:50.972419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-08 17:53:50.972910: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1f35e40 executing computations on platform CUDA. Devices:\n",
            "2019-05-08 17:53:50.972939: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-08 17:53:50.973272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-08 17:53:50.973294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-08 17:53:52.350214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-08 17:53:52.350270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-08 17:53:52.350282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-08 17:53:52.350546: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-08 17:53:52.350660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> \"C: YOU'RE AN ANIMAL! Q: The giant moray variety of this critter is seen here\"\n",
            "2019-05-08 17:54:06.713321: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "C: YOU'RE AN ANIMAL! Q: The giant moray variety of this critter is seen here\" C: YOU\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "\n",
            "C: YOU'RE AN ANIMAL! Q: The giant moray variety of this critter is seen here\" C: YOU\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "\n",
            "C: YOU'RE AN ANIMAL! Q: The giant moray variety of this critter is seen here\" C: YOU\n",
            "================================================================================\n",
            "Model prompt >>> C: YOU'RE AN ANIMAL! Q: The giant moray variety of this critter is seen here\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: I'm not sure what you mean. Q: The giant moray variety of this critter is seen here. A: I'm\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: I'm not sure what you mean. Q: The giant moray variety of this critter is seen here. A: I'm\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: I'm not sure what you mean. Q: The giant moray variety of this critter is seen here. A: I'm\n",
            "================================================================================\n",
            "Model prompt >>> C: THE FICTIONAL LAND OF OZ Q: The \"\"Wizard\"\" of Oz is from Omaha, Nebraska, not far from this state where Dorothy lives\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: I don't know if he's from Omaha or not. Q: What is the name of the town where Oz lives? A:\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: I don't know if he's from Omaha or not. Q: What is the name of the town where Oz lives? A:\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: I don't know if he's from Omaha or not. Q: What is the name of the town where Oz lives? A:\n",
            "================================================================================\n",
            "Model prompt >>> C: EYE ON AFRICA Q: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "================================================================================\n",
            "Model prompt >>> C: EYE ON AFRICA\\nQ: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\\n\n",
            "======================================== SAMPLE 1 ========================================\n",
            "Q: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\\nQ: On landing\n",
            "======================================== SAMPLE 2 ========================================\n",
            "Q: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\\nQ: On landing\n",
            "======================================== SAMPLE 3 ========================================\n",
            "Q: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\\nQ: On landing\n",
            "================================================================================\n",
            "Model prompt >>> C: EYE ON AFRICA Q: On landing in Accra, the very first Peace Corps volunteers sang this country's national anthem on the tarmac\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". Q: What was the song? A: \"The Star-Spangled Banner.\" Q: What was the song? A: \"The Star\n",
            "================================================================================\n",
            "Model prompt >>> C: RELIGION Q: In 1978, this pope reigned for only 34 days\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What was the reaction of the Catholic Church? A: The reaction was very positive. The pope was very popular. He was very popular with the\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What was the reaction of the Catholic Church? A: The reaction was very positive. The pope was very popular. He was very popular with the\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What was the reaction of the Catholic Church? A: The reaction was very positive. The pope was very popular. He was very popular with the\n",
            "================================================================================\n",
            "Model prompt >>> C: THE TRUTH IS OUT THERE Q: In \"Paradise Regained\", this British epic poet opined, \"Hard are the ways of truth, & rough to walk\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " (p. 5). A: I think that's a very good statement. Q: In \"Paradise Regained\", this British epic poet\n",
            "======================================== SAMPLE 2 ========================================\n",
            " (p. 5). A: I think that's a very good statement. Q: In \"Paradise Regained\", this British epic poet\n",
            "======================================== SAMPLE 3 ========================================\n",
            " (p. 5). A: I think that's a very good statement. Q: In \"Paradise Regained\", this British epic poet\n",
            "================================================================================\n",
            "Model prompt >>> C: THE VIOLIN Q: The finest violins ever made, they bear the Latinized form of their maker's name\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". Q: The finest violins ever made, they bear the Latinized form of their maker's name. Q: The finest violins ever made\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". Q: The finest violins ever made, they bear the Latinized form of their maker's name. Q: The finest violins ever made\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". Q: The finest violins ever made, they bear the Latinized form of their maker's name. Q: The finest violins ever made\n",
            "================================================================================\n",
            "Model prompt >>> C: FOLLOW THE MONEY Q: Nepal, Bhutan, India & this island nation off India all use the rupee as their currency\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What is the difference between the rupee and the US dollar? A: The rupee is a unit of exchange. It is used to buy\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What is the difference between the rupee and the US dollar? A: The rupee is a unit of exchange. It is used to buy\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What is the difference between the rupee and the US dollar? A: The rupee is a unit of exchange. It is used to buy\n",
            "================================================================================\n",
            "Model prompt >>> C: YOUNG NOBEL PRIZE WINNERS Q: This 31-year-old won for the creation of quantum mechanics & not for his uncertainty principle\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: I'm sorry, but I'm not sure that's true. Q: This 31-year-old won for the creation of quantum\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: I'm sorry, but I'm not sure that's true. Q: This 31-year-old won for the creation of quantum\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: I'm sorry, but I'm not sure that's true. Q: This 31-year-old won for the creation of quantum\n",
            "================================================================================\n",
            "Model prompt >>> C: FADS Q: The trademarked version of \"skinny\" these are \"classic denim but with a high nylon/elastane content\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Q: The trademarked version of \"skinny\" these are \"classic denim but with a high nylon/elastane content\" Q: The\n",
            "======================================== SAMPLE 2 ========================================\n",
            " Q: The trademarked version of \"skinny\" these are \"classic denim but with a high nylon/elastane content\" Q: The\n",
            "======================================== SAMPLE 3 ========================================\n",
            " Q: The trademarked version of \"skinny\" these are \"classic denim but with a high nylon/elastane content\" Q: The\n",
            "================================================================================\n",
            "Model prompt >>> C: THE AMERICAN FLAG Q: The number of stars on the American flag\n",
            "======================================== SAMPLE 1 ========================================\n",
            " is a symbol of the nation's commitment to freedom and democracy. A: The American flag is a symbol of freedom and democracy. Q: The flag\n",
            "======================================== SAMPLE 2 ========================================\n",
            " is a symbol of the nation's commitment to freedom and democracy. A: The American flag is a symbol of freedom and democracy. Q: The flag\n",
            "======================================== SAMPLE 3 ========================================\n",
            " is a symbol of the nation's commitment to freedom and democracy. A: The American flag is a symbol of freedom and democracy. Q: The flag\n",
            "================================================================================\n",
            "Model prompt >>> C: AFRICAN CUISINE Q: Doro Wat, stewed chicken over injera bread, is a national dish of this East African country\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: It's a dish that's been around for a long time. It's a dish that's been around for a long time. Q\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: It's a dish that's been around for a long time. It's a dish that's been around for a long time. Q\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: It's a dish that's been around for a long time. It's a dish that's been around for a long time. Q\n",
            "================================================================================\n",
            "Model prompt >>> C: POETS OF SONG Q: This Dylan song asked, \"How many times must the cannonballs fly, before they're forever banned\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " A: \"How many times must the cannonballs fly, before they're forever banned\" Q: This Dylan song asked, \"How many times must\n",
            "======================================== SAMPLE 2 ========================================\n",
            " A: \"How many times must the cannonballs fly, before they're forever banned\" Q: This Dylan song asked, \"How many times must\n",
            "======================================== SAMPLE 3 ========================================\n",
            " A: \"How many times must the cannonballs fly, before they're forever banned\" Q: This Dylan song asked, \"How many times must\n",
            "================================================================================\n",
            "Model prompt >>> C: ISLANDS Q: Almost all the inhabitants of this Chilean island live in the village of Hanga Roa on the west coast\n",
            "======================================== SAMPLE 1 ========================================\n",
            " of the island. They are called the Hanga Roa people. They are a very small people, about the size of a small dog. They\n",
            "======================================== SAMPLE 2 ========================================\n",
            " of the island. They are called the Hanga Roa people. They are a very small people, about the size of a small dog. They\n",
            "======================================== SAMPLE 3 ========================================\n",
            " of the island. They are called the Hanga Roa people. They are a very small people, about the size of a small dog. They\n",
            "================================================================================\n",
            "Model prompt >>> C: WORLD HISTORY Q: In 1920 the League of Nations gave this neighboring country a mandate over the territory of Namibia\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What was the result? A: The Namibian government was overthrown by the British. Q: What was the result of the British invasion\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What was the result? A: The Namibian government was overthrown by the British. Q: What was the result of the British invasion\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What was the result? A: The Namibian government was overthrown by the British. Q: What was the result of the British invasion\n",
            "================================================================================\n",
            "Model prompt >>> C: MY SPACE Q: One measure of this constant in a vacuum is 299,792,458 meters per second\n",
            "======================================== SAMPLE 1 ========================================\n",
            ".\n",
            "\n",
            "Q: What is the speed of light?\n",
            "\n",
            "A: The speed of light is 299,792,458 meters per second.\n",
            "======================================== SAMPLE 2 ========================================\n",
            ".\n",
            "\n",
            "Q: What is the speed of light?\n",
            "\n",
            "A: The speed of light is 299,792,458 meters per second.\n",
            "======================================== SAMPLE 3 ========================================\n",
            ".\n",
            "\n",
            "Q: What is the speed of light?\n",
            "\n",
            "A: The speed of light is 299,792,458 meters per second.\n",
            "================================================================================\n",
            "Model prompt >>> C: MEDICINE Q: Cystitis, an inflammation of this urinary organ, is usually caused by bacteria\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: Cystitis is a condition of the urinary tract. It is caused by a bacterial infection. It is not a disease. Q:\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: Cystitis is a condition of the urinary tract. It is caused by a bacterial infection. It is not a disease. Q:\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: Cystitis is a condition of the urinary tract. It is caused by a bacterial infection. It is not a disease. Q:\n",
            "================================================================================\n",
            "Model prompt >>> C: LEFTIES Q: Born a lefty, this president governed from the right from 1981 to 1989\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". He was a Republican, but he was a Republican from 1981 to 1989. He was a Republican from 1981 to 1989. He was a Republican from\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". He was a Republican, but he was a Republican from 1981 to 1989. He was a Republican from 1981 to 1989. He was a Republican from\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". He was a Republican, but he was a Republican from 1981 to 1989. He was a Republican from 1981 to 1989. He was a Republican from\n",
            "================================================================================\n",
            "Model prompt >>> C: LETTER, PLEASE Q: Females have 2 copies of this chromosome\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "======================================== SAMPLE 2 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "======================================== SAMPLE 3 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "================================================================================\n",
            "Model prompt >>> C: LETTER, PLEASE Q: Females have 2 copies of this chromosome\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "======================================== SAMPLE 2 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "======================================== SAMPLE 3 ========================================\n",
            ", one from each parent. A: YES, YES, YES Q: Females have 2 copies of this chromosome, one from each parent. A:\n",
            "================================================================================\n",
            "Model prompt >>> C: SINGERS Q: \"Everybody Loves Somebody\" & Las Vegas loved this man, dimming the Strip's lights in his honor December 28, 1995\n",
            "======================================== SAMPLE 1 ========================================\n",
            ".\n",
            "\n",
            "A: \"I'm not going to lie, I was a little bit nervous. I was like, 'Oh my God, I\n",
            "======================================== SAMPLE 2 ========================================\n",
            ".\n",
            "\n",
            "A: \"I'm not going to lie, I was a little bit nervous. I was like, 'Oh my God, I\n",
            "======================================== SAMPLE 3 ========================================\n",
            ".\n",
            "\n",
            "A: \"I'm not going to lie, I was a little bit nervous. I was like, 'Oh my God, I\n",
            "================================================================================\n",
            "Model prompt >>> C: CLUB MED Q: This river flows north for more than 4,000 miles before emptying into the Mediterranean through a delta\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Rome to Venice. Q: What's the best way to get there\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Rome to Venice. Q: What's the best way to get there\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Rome to Venice. Q: What's the best way to get there\n",
            "================================================================================\n",
            "Model prompt >>> C: LANGUAGE Q: It is the 2nd most spoken language in the world\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: It is the 2nd most spoken language in the world. Q: What is the most spoken language in the world? A: It\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: It is the 2nd most spoken language in the world. Q: What is the most spoken language in the world? A: It\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: It is the 2nd most spoken language in the world. Q: What is the most spoken language in the world? A: It\n",
            "================================================================================\n",
            "Model prompt >>> C: TROPICANA Q: This tropical island has roads linking Port Antonio, Montego Bay & Kingston\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Port Antonio to Montego Bay. It's about 2 hours. Q\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Port Antonio to Montego Bay. It's about 2 hours. Q\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What's the best way to get there? A: Take the ferry from Port Antonio to Montego Bay. It's about 2 hours. Q\n",
            "================================================================================\n",
            "Model prompt >>> C: OSCAR Q: Prior to winning his Best Director Oscar for \"Platoon\", he won for Best Adapted Screenplay for \"Midnight Express\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " and \"The Theory of Everything\". What was it like to work with him?\n",
            "\n",
            "JH: It was a pleasure. I've known him\n",
            "======================================== SAMPLE 2 ========================================\n",
            " and \"The Theory of Everything\". What was it like to work with him?\n",
            "\n",
            "JH: It was a pleasure. I've known him\n",
            "======================================== SAMPLE 3 ========================================\n",
            " and \"The Theory of Everything\". What was it like to work with him?\n",
            "\n",
            "JH: It was a pleasure. I've known him\n",
            "================================================================================\n",
            "Model prompt >>> C: INCREDIBLE EDIBLES Q: These \"soft-shell\" animals are sometimes prepared & eaten without removing their eyes\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: Yes, they are. Q: What is the difference between a soft-shell and a hard-shell? A: A soft-\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: Yes, they are. Q: What is the difference between a soft-shell and a hard-shell? A: A soft-\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: Yes, they are. Q: What is the difference between a soft-shell and a hard-shell? A: A soft-\n",
            "================================================================================\n",
            "Model prompt >>> C: \"PRINCE\" Q: Satan, Dracula & Ozzy Osbourne go by this nickname\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What's your favorite name? A: \"PRINCE\" Q: What's your favorite song? A: \"I'm Not a Human\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What's your favorite name? A: \"PRINCE\" Q: What's your favorite song? A: \"I'm Not a Human\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What's your favorite name? A: \"PRINCE\" Q: What's your favorite song? A: \"I'm Not a Human\n",
            "================================================================================\n",
            "Model prompt >>> C: THE 50 STATES Q: Its southeast corner touches Arizona, Colorado & New Mexico\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: The southern corner of the state is the southernmost part of the state. Q: What is the southernmost part of the state?\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: The southern corner of the state is the southernmost part of the state. Q: What is the southernmost part of the state?\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: The southern corner of the state is the southernmost part of the state. Q: What is the southernmost part of the state?\n",
            "================================================================================\n",
            "Model prompt >>> C: PROFESSIONS IN SONG Q: Peter Schilling's \"Major Tom\" continues the story of the astronaut in this singer's hit \"Space Oddity\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " (1962). Q: The \"Space Oddity\" song is a classic of the genre. It's a great song. It's a great\n",
            "======================================== SAMPLE 2 ========================================\n",
            " (1962). Q: The \"Space Oddity\" song is a classic of the genre. It's a great song. It's a great\n",
            "======================================== SAMPLE 3 ========================================\n",
            " (1962). Q: The \"Space Oddity\" song is a classic of the genre. It's a great song. It's a great\n",
            "================================================================================\n",
            "Model prompt >>> C: MULTIPLE-WORD CAPITALS Q: This capital of the Dominican Republic is said to contain the remains of Christopher Columbus\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What is the origin of this capital? A: The capital of the Dominican Republic is the capital of the Dominican Republic. Q: What is the\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What is the origin of this capital? A: The capital of the Dominican Republic is the capital of the Dominican Republic. Q: What is the\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What is the origin of this capital? A: The capital of the Dominican Republic is the capital of the Dominican Republic. Q: What is the\n",
            "================================================================================\n",
            "Model prompt >>> C: BEGINS & ENDS WITH \"T\" Q: A male feline\n",
            "======================================== SAMPLE 1 ========================================\n",
            " is a male cat. A female feline is a female cat. A female cat is a female cat. A female cat is a female cat.\n",
            "======================================== SAMPLE 2 ========================================\n",
            " is a male cat. A female feline is a female cat. A female cat is a female cat. A female cat is a female cat.\n",
            "======================================== SAMPLE 3 ========================================\n",
            " is a male cat. A female feline is a female cat. A female cat is a female cat. A female cat is a female cat.\n",
            "================================================================================\n",
            "Model prompt >>> C: 1985 Q: This soft drink co. introduced its new formula drink & then brought back the classic version\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What was the reaction? A: The reaction was very positive. People were very excited about the new formula. Q: What was the reaction?\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What was the reaction? A: The reaction was very positive. People were very excited about the new formula. Q: What was the reaction?\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What was the reaction? A: The reaction was very positive. People were very excited about the new formula. Q: What was the reaction?\n",
            "================================================================================\n",
            "Model prompt >>> C: FOOD & DRINK Q: With the hull left on it's called brown; with it removed it's white & called \"polished\"\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Q: What is the difference between the two? A: Polished is a more polished finish, and brown is a more polished finish. Q:\n",
            "======================================== SAMPLE 2 ========================================\n",
            " Q: What is the difference between the two? A: Polished is a more polished finish, and brown is a more polished finish. Q:\n",
            "======================================== SAMPLE 3 ========================================\n",
            " Q: What is the difference between the two? A: Polished is a more polished finish, and brown is a more polished finish. Q:\n",
            "================================================================================\n",
            "Model prompt >>> C: COMMUNICATIONS Q: It's waved to indicate surrender or truce\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: It's waved to indicate surrender or truce. Q: It's waved to indicate surrender or truce. A: It's waved to indicate\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: It's waved to indicate surrender or truce. Q: It's waved to indicate surrender or truce. A: It's waved to indicate\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: It's waved to indicate surrender or truce. Q: It's waved to indicate surrender or truce. A: It's waved to indicate\n",
            "================================================================================\n",
            "Model prompt >>> C: GEOLOGY Q: An earthquake with a 7.0 magnitude on this scale releases about 32 times as much energy as a 6.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            " magnitude earthquake. A 7.0 magnitude earthquake releases about 1,000 times as much energy as a 6.0 magnitude earthquake. A 7.0\n",
            "======================================== SAMPLE 2 ========================================\n",
            " magnitude earthquake. A 7.0 magnitude earthquake releases about 1,000 times as much energy as a 6.0 magnitude earthquake. A 7.0\n",
            "======================================== SAMPLE 3 ========================================\n",
            " magnitude earthquake. A 7.0 magnitude earthquake releases about 1,000 times as much energy as a 6.0 magnitude earthquake. A 7.0\n",
            "================================================================================\n",
            "Model prompt >>> C: FRANCE Q: A part-iridium cylinder in France is the standard for this metric unit of mass (about 2.2 pounds)\n",
            "======================================== SAMPLE 1 ========================================\n",
            " and is used in the United States. A part-iridium cylinder is also used in Europe. A part-iridium cylinder is a cylinder of\n",
            "======================================== SAMPLE 2 ========================================\n",
            " and is used in the United States. A part-iridium cylinder is also used in Europe. A part-iridium cylinder is a cylinder of\n",
            "======================================== SAMPLE 3 ========================================\n",
            " and is used in the United States. A part-iridium cylinder is also used in Europe. A part-iridium cylinder is a cylinder of\n",
            "================================================================================\n",
            "Model prompt >>> C: CALL OUT THE BOB SQUAD Q: He was only 15 when he became an international chess grandmaster in 1958\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: He was 15 when he became an international chess grandmaster in 1958. Q: He was only 15 when he became an international chess grand\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: He was 15 when he became an international chess grandmaster in 1958. Q: He was only 15 when he became an international chess grand\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: He was 15 when he became an international chess grandmaster in 1958. Q: He was only 15 when he became an international chess grand\n",
            "================================================================================\n",
            "Model prompt >>> C: AROUND THE WORLD Q: The Amundsen-Scott research base is located at this famous point on Antarctica\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What is the significance of this location? A: The Amundsen-Scott research base is located at this famous point on Antarctica. The base\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What is the significance of this location? A: The Amundsen-Scott research base is located at this famous point on Antarctica. The base\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What is the significance of this location? A: The Amundsen-Scott research base is located at this famous point on Antarctica. The base\n",
            "================================================================================\n",
            "Model prompt >>> C: INDIANA Q: 19th century president who grew to manhood in southern Indiana\n",
            "======================================== SAMPLE 1 ========================================\n",
            ".\n",
            "\n",
            "Q: 20th century president who grew to manhood in southern Indiana.\n",
            "\n",
            "Q: 21st century president who grew to man\n",
            "======================================== SAMPLE 2 ========================================\n",
            ".\n",
            "\n",
            "Q: 20th century president who grew to manhood in southern Indiana.\n",
            "\n",
            "Q: 21st century president who grew to man\n",
            "======================================== SAMPLE 3 ========================================\n",
            ".\n",
            "\n",
            "Q: 20th century president who grew to manhood in southern Indiana.\n",
            "\n",
            "Q: 21st century president who grew to man\n",
            "================================================================================\n",
            "Model prompt >>> C: WORLD CITIES Q: The Universal Postal Union is headquartered in this Swiss capital\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". What is the purpose of this organization? A: The Universal Postal Union is a global organization of postal workers, which is responsible for the delivery of\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". What is the purpose of this organization? A: The Universal Postal Union is a global organization of postal workers, which is responsible for the delivery of\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". What is the purpose of this organization? A: The Universal Postal Union is a global organization of postal workers, which is responsible for the delivery of\n",
            "================================================================================\n",
            "Model prompt >>> C: TIME'S MAN OF THE YEAR Q: This Soviet leader was Time's Man of the Year for 1987 & of the Decade in 1989\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". A: He was. Q: What was his name? A: He was called \"Time's Man of the Year.\" Q: What was\n",
            "======================================== SAMPLE 2 ========================================\n",
            ". A: He was. Q: What was his name? A: He was called \"Time's Man of the Year.\" Q: What was\n",
            "======================================== SAMPLE 3 ========================================\n",
            ". A: He was. Q: What was his name? A: He was called \"Time's Man of the Year.\" Q: What was\n",
            "================================================================================\n",
            "Model prompt >>> C: NATIONAL NAMES Q: Kampuchea\\n\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\\A: Kampuchea\\n\\B: Kampuchea\\n\\C: NATIONAL NAMES Q: Kampuchea\\n\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\\A: Kampuchea\\n\\B: Kampuchea\\n\\C: NATIONAL NAMES Q: Kampuchea\\n\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\\A: Kampuchea\\n\\B: Kampuchea\\n\\C: NATIONAL NAMES Q: Kampuchea\\n\n",
            "================================================================================\n",
            "Model prompt >>> C: NATIONAL NAMES Q: Kampuchea\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q\n",
            "======================================== SAMPLE 2 ========================================\n",
            ", Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q\n",
            "======================================== SAMPLE 3 ========================================\n",
            ", Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q: Liberia Q\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5061, in get_controller\n",
            "    yield default\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1573, in __exit__\n",
            "    exec_tb)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 123, in __exit__\n",
            "    if sys.exc_info()[1] is value:\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}